<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>tglcourse - Lesson 12 - Introduction to Diffusion Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="tglcourse - Lesson 12 - Introduction to Diffusion Models">
<meta property="og:description" content="Diffusion models are a little different to the other types of generative models you may have encountered.">
<meta property="og:site-name" content="tglcourse">
<meta name="twitter:title" content="tglcourse - Lesson 12 - Introduction to Diffusion Models">
<meta name="twitter:description" content="Diffusion models are a little different to the other types of generative models you may have encountered.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">tglcourse</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/johnowhitaker/tglcourse"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://discord.gg/vSjhr8xb4g"><i class="bi bi-discord" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.patreon.com/johnowhitaker"><i class="bi bi-currency-dollar" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Lesson 12 - Introduction to Diffusion Models</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">The Generative Landscape</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./getting_started.html" class="sidebar-item-text sidebar-link">Getting Started</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Lessons</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pytorch_basics.html" class="sidebar-item-text sidebar-link">Lesson 1: PyTorch Basics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link">Lesson 2: Gradient Descent and Optimization</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./building_nns.html" class="sidebar-item-text sidebar-link">Lesson 3: Neural Networks and Loss Functions</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./representations.html" class="sidebar-item-text sidebar-link">Lesson 4: Learning Representations + Style Transfer</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clip.html" class="sidebar-item-text sidebar-link">Lesson 5: Exploring Multiple Modalities with CLIP</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generative_1.html" class="sidebar-item-text sidebar-link">Lesson 6 - Generative Modelling Intro (AutoEncoders)</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gan1.html" class="sidebar-item-text sidebar-link">Lesson 7: GANs Part 1 - GAN Training</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gan2.html" class="sidebar-item-text sidebar-link">Lesson 8: Conditional GANs, Training Tricks and GANs+CLIP</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sequence_1.html" class="sidebar-item-text sidebar-link">Lesson 9: Introduction to Sequence Modelling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformers.html" class="sidebar-item-text sidebar-link">Lesson 10 - Transformers</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sequence_3.html" class="sidebar-item-text sidebar-link">Lesson 11: Everything Is A Sequence</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dm1.html" class="sidebar-item-text sidebar-link active">Lesson 12 - Introduction to Diffusion Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dm2.html" class="sidebar-item-text sidebar-link">Lesson 13: Going Deeper with Diffusion Models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dm3.html" class="sidebar-item-text sidebar-link">Lesson 14 - Stable Diffusion Deep Dive</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dm4.html" class="sidebar-item-text sidebar-link">Lesson 15: Diffusion for Audio</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./wrapup.html" class="sidebar-item-text sidebar-link">Lesson 16: The Next Generation</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Bonus Material</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bonus_material_intro.html" class="sidebar-item-text sidebar-link">Bonus Material</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./datasets.html" class="sidebar-item-text sidebar-link">Datasets and General Data Utilities</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scripts.html" class="sidebar-item-text sidebar-link">Creating Scripts</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generators_and_losses.html" class="sidebar-item-text sidebar-link">Fun with Generators and Losses</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interfaces_with_gradio.html" class="sidebar-item-text sidebar-link">Creating Quick Interfaces with Gradio</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./finetuning_pretrained_models.html" class="sidebar-item-text sidebar-link">Fine-Tuning Pretrained Networks for Image Classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">Ethics in Generative Modelling</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussions.html" class="sidebar-item-text sidebar-link">Discussions</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./projects.html" class="sidebar-item-text sidebar-link">Projects</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./library.html" class="sidebar-item-text sidebar-link">The Library</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-toy-example" id="toc-a-toy-example" class="nav-link active" data-scroll-target="#a-toy-example">A Toy Example</a>
  <ul class="collapse">
  <li><a href="#the-corruption-process" id="toc-the-corruption-process" class="nav-link" data-scroll-target="#the-corruption-process">The Corruption Process</a></li>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The Model</a></li>
  </ul></li>
  <li><a href="#training-the-network" id="toc-training-the-network" class="nav-link" data-scroll-target="#training-the-network">Training the network</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a>
  <ul class="collapse">
  <li><a href="#making-the-models-job-easier-noise-conditioning" id="toc-making-the-models-job-easier-noise-conditioning" class="nav-link" data-scroll-target="#making-the-models-job-easier-noise-conditioning">Making the model’s job easier: Noise Conditioning</a></li>
  <li><a href="#class-conditioning" id="toc-class-conditioning" class="nav-link" data-scroll-target="#class-conditioning">Class Conditioning</a>
  <ul class="collapse">
  <li><a href="#what-have-we-learnt-and-how-much-was-a-lie" id="toc-what-have-we-learnt-and-how-much-was-a-lie" class="nav-link" data-scroll-target="#what-have-we-learnt-and-how-much-was-a-lie">What have we learnt, and how much was a lie?</a></li>
  <li><a href="#clarification-1-whats-in-an-objective" id="toc-clarification-1-whats-in-an-objective" class="nav-link" data-scroll-target="#clarification-1-whats-in-an-objective">Clarification 1: What’s in an Objective?</a></li>
  <li><a href="#clarification-2-variance-preserving-vp-vs-variance-exploding-ve" id="toc-clarification-2-variance-preserving-vp-vs-variance-exploding-ve" class="nav-link" data-scroll-target="#clarification-2-variance-preserving-vp-vs-variance-exploding-ve">Clarification 2: Variance Preserving (VP) vs Variance Exploding (VE)</a></li>
  <li><a href="#clarification-3-continuous-vs-discrete-time" id="toc-clarification-3-continuous-vs-discrete-time" class="nav-link" data-scroll-target="#clarification-3-continuous-vs-discrete-time">Clarification 3: Continuous vs discrete time</a></li>
  <li><a href="#clarification-4-differential-equations" id="toc-clarification-4-differential-equations" class="nav-link" data-scroll-target="#clarification-4-differential-equations">Clarification 4: Differential Equations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#ddpm-with-the-diffusers-library" id="toc-ddpm-with-the-diffusers-library" class="nav-link" data-scroll-target="#ddpm-with-the-diffusers-library">DDPM with the Diffusers library</a>
  <ul class="collapse">
  <li><a href="#the-data" id="toc-the-data" class="nav-link" data-scroll-target="#the-data">The Data</a></li>
  <li><a href="#the-scheduler" id="toc-the-scheduler" class="nav-link" data-scroll-target="#the-scheduler">The Scheduler</a></li>
  <li><a href="#the-model-1" id="toc-the-model-1" class="nav-link" data-scroll-target="#the-model-1">The model</a></li>
  <li><a href="#the-training-loop" id="toc-the-training-loop" class="nav-link" data-scroll-target="#the-training-loop">The training loop</a></li>
  <li><a href="#sampling-1" id="toc-sampling-1" class="nav-link" data-scroll-target="#sampling-1">Sampling</a></li>
  <li><a href="#our-toy-example-to-the-max" id="toc-our-toy-example-to-the-max" class="nav-link" data-scroll-target="#our-toy-example-to-the-max">Our ‘Toy’ Example TO THE MAX</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/johnowhitaker/tglcourse/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Lesson 12 - Introduction to Diffusion Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>Diffusion models are a little different to the other types of generative models you may have encountered. During training, input images (or any other kind of data) are corrupted, and the model attempts to ‘undo’ this corruption. Once trained, we can begin from a random set of inputs that resembles data that has been so completely degraded as to be unrecognizeable and then gradually ‘uncorrupt’ this, typically over a large number of steps.</p>
<p>TODO diagram</p>
<p>In most (but not all!) current approaches, this ‘corruption’ takes the place of adding noise, hence ‘denoising diffusion models’. The exact method of adding noise and the details of how the problem is formulated vary across the literature, which can lead to confusion.</p>
<p>The goal of this lesson is to introduce the key concepts without worrying too much about specific implementations. The plan:</p>
<ul>
<li>Start with a simplistic toy example</li>
<li>Discuss some issues with it, and try a few improvements</li>
<li>Try an existing implementation, exploring the differences between this and our toy version</li>
<li>Along the way we’ll define some key terms and highlight some additional questions that will be addressed in later lessons. Ready? Let’s go!</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
</div>
<section id="a-toy-example" class="level2">
<h2 class="anchored" data-anchor-id="a-toy-example">A Toy Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://images.pexels.com/photos/12211/pexels-photo-12211.jpeg?cs=srgb&amp;dl=pexels-tetyana-kovyrina-12211.jpg&amp;fm=jpg&amp;w=640&amp;h=427" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">toy photo</figcaption><p></p>
</figure>
</div>
<p>[video: intro idea]</p>
<p>Load the dataset (replace MNIST with FashionMNIST for a variant):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!output: false</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>mnist_dataset <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">"mnist/"</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>torchvision.transforms.ToTensor())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fd360ed4b8b54b659f2fb012a365deb0","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5fb5b33c8a2649ed97276a7ed030c2e0","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"23b9c42d131a4fd189eb5070b75e78aa","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"2202d005ab3c4475ba329756aa5a5a15","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw
</code></pre>
</div>
</div>
<p>Viewing the first example image:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View an example</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>im, label <span class="op">=</span> mnist_dataset[<span class="dv">0</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image shape:'</span>, im.shape)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image min and max:'</span>, im.<span class="bu">min</span>(), im.<span class="bu">max</span>())</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'label:'</span>, label)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(im[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape: torch.Size([1, 28, 28])
Image min and max: tensor(0.) tensor(1.)
label: 5</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See how we can make a dataloader to serve the data in batches for training</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(mnist_dataset, batch_size<span class="op">=</span><span class="dv">8</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>x.shape, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([8, 1, 28, 28]), tensor([3, 7, 0, 2, 5, 3, 9, 9]))</code></pre>
</div>
</div>
<section id="the-corruption-process" class="level3">
<h3 class="anchored" data-anchor-id="the-corruption-process">The Corruption Process</h3>
<p>Pretend you haven’t read any diffusion model papers, but you know the process involves adding noise. How would you do it?</p>
<p>We probably want an easy way to control the amount of corruption. So what if we take in a parameter for the <code>amount</code> of noise to add, and then we do:</p>
<p><code>noise = torch.rand_like(x)</code></p>
<p><code>noisy_x =  (1-amount)*x + amount*noise</code></p>
<p>If amount = 0, we get back the input without any changes. If amount gets up to 1, we get back noise with no trace of the input x. By mixing the input with noise this way, we keep the output in the same range (0 to 1).</p>
<p>We can implement this fairly easily (just watch the shapes so you don’t get burnt by broadcasting rules):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> corrupt(x, amount):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Corrupt the input `x` by mixing it with noise according to `amount`"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> torch.rand_like(x)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    amount <span class="op">=</span> amount.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Sort shape so broadcasting works</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>amount) <span class="op">+</span> noise<span class="op">*</span>amount</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And looking at the results visually to see that it works as expected:</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>With this in place, let’s move on to the next piece of this process - the model.</p>
</section>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The Model</h3>
<p>Sticking with our goal of simplicity, we’ll specify the model here as a neural network that produces an output the same shape as it’s input. I’ve made three variants you can try out:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicConvNet(nn.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A stack of conv layers with padding to keep the output the same size as </span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    the input. Hidden channel numbers fixed at: [16, 32, 64, 64, 16].</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args: in_channels, out_channels,kernel_size=5."""</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        padding <span class="op">=</span> kernel_size <span class="op">//</span> <span class="dv">2</span> <span class="co"># So we keep output size the same</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, <span class="dv">16</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">16</span>, kernel_size,  padding<span class="op">=</span>padding),</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">16</span>, out_channels, kernel_size, padding<span class="op">=</span>padding),</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> einops.layers.torch <span class="im">import</span> Rearrange, Reduce</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PreNormResidual(nn.Module):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, fn):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fn <span class="op">=</span> fn</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fn(<span class="va">self</span>.norm(x)) <span class="op">+</span> x</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> FeedForward(dim, expansion_factor <span class="op">=</span> <span class="dv">4</span>, dropout <span class="op">=</span> <span class="fl">0.</span>, dense <span class="op">=</span> nn.Linear):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    inner_dim <span class="op">=</span> <span class="bu">int</span>(dim <span class="op">*</span> expansion_factor)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        dense(dim, inner_dim),</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        nn.GELU(),</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(dropout),</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        dense(inner_dim, dim),</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        nn.Dropout(dropout)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> MLPMixer(<span class="op">*</span>, image_size, channels, patch_size, dim, depth, expansion_factor <span class="op">=</span> <span class="dv">4</span>, expansion_factor_token <span class="op">=</span> <span class="fl">0.5</span>, dropout <span class="op">=</span> <span class="fl">0.</span>):</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A minimal MLP Mixer stolen from lucidrain's implementation."""</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get image width and height (same if image_size isn't a tuple):</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    pair <span class="op">=</span> <span class="kw">lambda</span> x: x <span class="cf">if</span> <span class="bu">isinstance</span>(x, <span class="bu">tuple</span>) <span class="cf">else</span> (x, x)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    image_h, image_w <span class="op">=</span> pair(image_size)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check they divide neatly by patch_size</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> (image_h <span class="op">%</span> patch_size) <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> (image_w <span class="op">%</span> patch_size) <span class="op">==</span> <span class="dv">0</span>, <span class="st">'image must be divisible by patch size'</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    num_patches <span class="op">=</span> (image_h <span class="op">//</span> patch_size) <span class="op">*</span> (image_w <span class="op">//</span> patch_size)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prep the two layers</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    chan_first, chan_last <span class="op">=</span> partial(nn.Conv1d, kernel_size <span class="op">=</span> <span class="dv">1</span>), nn.Linear</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the model (a stack of [FeedForward(chan_first), FeedForward(chan_last)] pairs</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># with layer norm on the inputs and a skip connection thanks to PreNormResidual)</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        Rearrange(<span class="st">'b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)'</span>, p1 <span class="op">=</span> patch_size, p2 <span class="op">=</span> patch_size),</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        nn.Linear((patch_size <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> channels, dim),</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>[nn.Sequential(</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>            PreNormResidual(dim, FeedForward(num_patches, expansion_factor, dropout, chan_first)),</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>            PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, chan_last))</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        ) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth)],</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>        Rearrange(<span class="st">'b (h w) (p1 p2 c) -&gt; b c (h p1) (w p2)'</span>, h <span class="op">=</span> <span class="bu">int</span>(image_h<span class="op">/</span>patch_size),  w <span class="op">=</span> <span class="bu">int</span>(image_w<span class="op">/</span>patch_size), p1 <span class="op">=</span> patch_size, p2 <span class="op">=</span> patch_size),</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(dim<span class="op">//</span>(patch_size<span class="op">**</span><span class="dv">2</span>), channels, kernel_size<span class="op">=</span><span class="dv">1</span>) <span class="co"># Back to right number of channels</span></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicUNet(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A minimal UNet implementation."""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels):</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_layers <span class="op">=</span> torch.nn.ModuleList([ </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_layers <span class="op">=</span> torch.nn.ModuleList([</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, out_channels, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>), </span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> nn.SiLU()</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downscale <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.upscale <span class="op">=</span> nn.Upsample(scale_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> []</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.down_layers):</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.act(l(x))</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            h.append(x)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">2</span>: x <span class="op">=</span> <span class="va">self</span>.downscale(x)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.up_layers):</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span>: x <span class="op">=</span> <span class="va">self</span>.upscale(x)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>            x <span class="op">+=</span> h.pop()</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.act(l(x))</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We’ll stick with the basic UNet for this demo but you can swap in the others if you’d like.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>unet <span class="op">=</span> BasicUNet(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>unet(torch.rand(<span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([8, 2, 28, 28])</code></pre>
</div>
</div>
<p>You can see how many parameters this has:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>([p.numel() <span class="cf">for</span> p <span class="kw">in</span> unet.parameters()]) <span class="co"># Try halving the number of channels in each layer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>309858</code></pre>
</div>
</div>
<p>We can create one and feed our demo batch of data through to check that it works and that the output shape is the same as the input as we expect:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the network (for single channel input &amp; output images)</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> BasicUNet(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed some data through:</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>x.shape, net(x).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([8, 1, 28, 28]), torch.Size([8, 1, 28, 28]))</code></pre>
</div>
</div>
</section>
</section>
<section id="training-the-network" class="level1">
<h1>Training the network</h1>
<p>So what should the model do, exactly? Again, there are various takes on this but for this demo let’s pick a simple framing: given a corrupted input noisy_x the model should output its best guess for what the original x looks like. We will compare this to the actual value via the mean squared error</p>
<p>We can now have a go at training the network. - Get a batch of data - Corrupt it by random amounts - Feed it through the model - Compare the model predictions with the clean images to calculate our loss - Update the model’s parameters accordingly.</p>
<p>Feel free to experiment with all of the parameters here - for this example I chose most fairly arbitrarily!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataloader (you can mess with batch size)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(mnist_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># How many runs through the data should we do?</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the network (feel free to explore)</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># net = BasicConvNet(1, 1)</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># net = MLPMixer(image_size=28, channels=1, patch_size=4, dim=64, depth=4)</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> BasicUNet(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>net.to(device) <span class="co"># We want to train on the GPU if that is available</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Our loss finction</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co"># The optimizer - explore different learning rates or try</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># a different optimizer instead</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(net.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>) </span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Keeping a record of the losses for later viewing</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="co"># And a record of smoothed loss values after each epoch</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>smoothed_losses_basic <span class="op">=</span> []</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="co"># The training loop</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> train_dataloader:</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get some data and prepare the corrupted version</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device) <span class="co"># Data on the GPU</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        noise_amount <span class="op">=</span> torch.rand(x.shape[<span class="dv">0</span>]).to(device) <span class="co"># Pick random noise amounts</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        noisy_x <span class="op">=</span> corrupt(x, noise_amount) <span class="co"># Create our noisy x</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the model prediction</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(noisy_x)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the loss</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, x) <span class="co"># How close is the output to the true 'clean' x?</span></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop and update the params:</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the loss for later</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># break</span></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print our the average of the last 100 loss values to get an idea of progress:</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> <span class="bu">sum</span>(losses[<span class="op">-</span><span class="dv">100</span>:])<span class="op">/</span><span class="dv">100</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>    smoothed_losses_basic.append(avg_loss)</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Finished epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">. Average of the last 100 loss values: </span><span class="sc">{</span>avg_loss<span class="sc">:05f}</span><span class="ss">'</span>)</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a><span class="co"># View the loss curve</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Finished epoch 0. Average of the last 100 loss values: 0.022459
Finished epoch 1. Average of the last 100 loss values: 0.019406
Finished epoch 2. Average of the last 100 loss values: 0.018515
Finished epoch 3. Average of the last 100 loss values: 0.017215
Finished epoch 4. Average of the last 100 loss values: 0.016586
Finished epoch 5. Average of the last 100 loss values: 0.016600
Finished epoch 6. Average of the last 100 loss values: 0.016814
Finished epoch 7. Average of the last 100 loss values: 0.016558
Finished epoch 8. Average of the last 100 loss values: 0.016264
Finished epoch 9. Average of the last 100 loss values: 0.016051</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We can try to see what the model predictions look like by grabbing a batch of data, corrupting it my different amounts and then seeing the models predictions:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fetch some data</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x[:<span class="dv">8</span>] <span class="co"># Only using the first 8 for easy plotting</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Corrupt with a range of amounts</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>amount <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, x.shape[<span class="dv">0</span>]) <span class="co"># Left to right -&gt; more corruption</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>noised_x <span class="op">=</span> corrupt(x, amount)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the model predictions</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> net(noised_x.to(device)).detach().cpu()</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">7</span>))</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">'Input data'</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].imshow(torchvision.utils.make_grid(x)[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="st">'Corrupted data'</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].imshow(torchvision.utils.make_grid(noised_x)[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].set_title(<span class="st">'Network Predictions'</span>)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">2</span>].imshow(torchvision.utils.make_grid(preds)[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>&lt;matplotlib.image.AxesImage&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-20-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>You can see that for the lower amounts the predictions are pretty good! But as the level gets very high there is less for the model to work with, and by the time we get to amount=1 it outputs something close to the mean of the dataset to try and hedge its bets on what the output might look like.</p>
<p><strong>Think</strong>: Does that final sentence make sense?</p>
</section>
<section id="sampling" class="level1">
<h1>Sampling</h1>
<p>As we just saw, feeding pure noise into the model doesn’t give a prediction that looks much like a digit! We need a sampling strategy. Again, pretend we’re trying this without peeking at any existing papers and that the words ‘differential equation’ are an alien language.</p>
<p>When the model sees a very corrupted input it doesn’t have much information to go on, but perhaps there’s a darker region that might indicate some ink near the top - perhaps a 7 or a 5. For an image with less noise, perhaps more structure becomes clear - a 5 or perhaps an 8? Based on this, perhaps we can gradually approach the goal? Let’s think about it through an analogy:</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>[placeholder image]</p>
<p>There’s a cool volcano explosion happening somewhere, and you want to visit it and take photos. You look around, and the western horizon seems darker. You head that way, and even though you didn’t quite guess the exact direction right, when you next park the car and look around you can see some hazy hills by a lake in the distance. It looks like one of them is the source of the smoke. As you pull up near the water, it is clear which hill is covered in lava, and how far away it is. Out comes the camera and you get a journalism prize.</p>
<p>Far fetched perhaps, but let’s translate this back to the problem at hand and see if we get anywhere:</p>
<ul>
<li>The model predictions for high noise amounts aren’t great, but might at least partly show us a fruitful direction to explore.</li>
<li>If we just move a little in that direction, we should then be able to make a better prediction and move a bit closer…</li>
<li>Repeat this enough and hopefully we’ll get a good result!</li>
</ul>
<p>Putting it in pseudo-code:</p>
<pre><code>steps = 10
x = random noise to start
for i in range(steps):
  pred = model(x)
  x = a mix of x and pred</code></pre>
<p>And in code, visualizing the steps and model outputs along the way:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take one: just break the process into 10 steps and move 1/10'th of the way there each time:</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).to(device) <span class="co"># Start from random</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>step_history <span class="op">=</span> []</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>pred_output_history <span class="op">=</span> []</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): <span class="co"># No need to track gradients during inference</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(x) <span class="co"># Predict the denoised x0</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    pred_output_history.append(pred.detach().cpu()) <span class="co"># Store model output for plotting</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    mix_factor <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(n_steps <span class="op">-</span> i) <span class="co"># How much we move towards the prediction</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>mix_factor) <span class="op">+</span> pred<span class="op">*</span>mix_factor <span class="co"># Move part of the way there</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    step_history.append(x.detach().cpu()) <span class="co"># Store step for plotting</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(n_steps, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    axs[i, <span class="dv">0</span>].imshow(torchvision.utils.make_grid(step_history[i])[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>),</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    axs[i, <span class="dv">1</span>].imshow(torchvision.utils.make_grid(pred_output_history[i])[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The left-hand column shows the steps after each update, while the right shows the model’s predictions for the fully denoised image given the noisy input. You can see the general structure appears quite soon in the process.</p>
<section id="making-the-models-job-easier-noise-conditioning" class="level3">
<h3 class="anchored" data-anchor-id="making-the-models-job-easier-noise-conditioning">Making the model’s job easier: Noise Conditioning</h3>
<p>At the moment the model gets a noisy_x and tries to predict the original just based on that. It seems intuitive that if we can find a way to slip the model extra info in the form of a hint, it should be able to do a better job. So, to start with, let’s make a version that can receive info about how much noise has been added:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a network</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> NoiseConditionedUNet(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we need both x and noise_amount to make predictions:</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>noise_amount <span class="op">=</span> torch.rand([bs])</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>x.shape, net(x, noise_amount).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([64, 1, 28, 28]), torch.Size([64, 1, 28, 28]))</code></pre>
</div>
</div>
<p>All we do here in add a second channel to the input image with the noise amount as the value for all ‘pixels’. The training looks identical to before except we use our new network and must pass the noise amount as a second argument to the forward pass with <code>pred = net(noisy_x, noise_amount)</code>:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataloader (you can mess with batch size)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(mnist_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># How many runs through the data should we do?</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Our new network type: </span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> NoiseConditionedUNet(<span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># &lt;&lt;&lt; Using our new noise conditioned net</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>net.to(device) <span class="co"># We want to train on the GPU if that is available</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Our loss finction</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The optimizer - explore different learning rates or try</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># a different optimizer instead</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(net.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>) </span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Keeping a record of the losses for later viewing</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co"># And a record of smoothed loss values after each epoch</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>smoothed_losses_noise_cond <span class="op">=</span> []</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="co"># The training loop</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> train_dataloader:</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get some data and prepare the corrupted version</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device) <span class="co"># Data on the GPU</span></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>        noise_amount <span class="op">=</span> torch.rand(x.shape[<span class="dv">0</span>]).to(device) <span class="co"># Pick random noise amounts</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        noisy_x <span class="op">=</span> corrupt(x, noise_amount) <span class="co"># Create our noisy x</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the model prediction</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(noisy_x, noise_amount) <span class="co"># &lt;&lt;&lt;&lt;&lt;&lt;</span></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the loss</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, x) <span class="co"># How close is the output to the true 'clean' x?</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop and update the params:</span></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the loss for later</span></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print our the average of the last 100 loss values to get an idea of progress:</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> <span class="bu">sum</span>(losses[<span class="op">-</span><span class="dv">100</span>:])<span class="op">/</span><span class="dv">100</span></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>    smoothed_losses_noise_cond.append(avg_loss)</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Finished epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">. Average of the last 100 loss values: </span><span class="sc">{</span>avg_loss<span class="sc">:05f}</span><span class="ss">'</span>)</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a><span class="co"># View the loss curve</span></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Finished epoch 0. Average of the last 100 loss values: 0.021278
Finished epoch 1. Average of the last 100 loss values: 0.018984
Finished epoch 2. Average of the last 100 loss values: 0.018017
Finished epoch 3. Average of the last 100 loss values: 0.017176
Finished epoch 4. Average of the last 100 loss values: 0.016522
Finished epoch 5. Average of the last 100 loss values: 0.015958
Finished epoch 6. Average of the last 100 loss values: 0.016164
Finished epoch 7. Average of the last 100 loss values: 0.015976
Finished epoch 8. Average of the last 100 loss values: 0.016329
Finished epoch 9. Average of the last 100 loss values: 0.015129</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-25-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The loss is marginally lower (and only for some runs with this network - you may see a larger effect with the other architectures). There’s something I want to note here: A decent network should be trivially capable of seeing whether a given input is super noisy or not. And so this noise conditioning might not be necessary - in fact in tests I’ve found that it doesn’t seem to make any difference! And yet this is used everywhere, largely as an artifact of the historical framing of diffusion models or the way the training objective is set up.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparing the training curves of the two vairants</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_losses_basic, label<span class="op">=</span><span class="st">'Smoothed loss - basic (no conditioning)'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_losses_noise_cond, label<span class="op">=</span><span class="st">'Smoothed loss - noise conditioned'</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Anyway, our sampling loop looks roughly the same as before except now we need to pass a noise_amount to the model at each step. Here I assume it goes linearly from 1 to 0 over the course of sampling but THIS IS NOT NECESSARILY RIGHT. Sampling schemes usually explicity account for the timestep in the update, as we’ll see when we look at the DDPM version.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sampling as before, but conditioning on an estimated noise amount:</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).to(device) <span class="co"># Start from random</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>step_history <span class="op">=</span> []</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>pred_output_history <span class="op">=</span> []</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    noise_amount <span class="op">=</span> torch.ones((x.shape[<span class="dv">0</span>], )).to(device) <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>(i<span class="op">/</span>n_steps))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): <span class="co"># No need to track gradients during inference</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(x, noise_amount) <span class="co"># Predict the denoised x0</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    pred_output_history.append(pred.detach().cpu()) <span class="co"># Store for plotting</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    mix_factor <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(n_steps <span class="op">-</span> i) <span class="co"># How much we move towards the prediction</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>mix_factor) <span class="op">+</span> pred<span class="op">*</span>mix_factor <span class="co"># Move part of the way there # Should it be (pred-x) instead of pred?</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    step_history.append(x.detach().cpu()) <span class="co"># Store for plotting</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(n_steps, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    axs[i, <span class="dv">0</span>].imshow(torchvision.utils.make_grid(step_history[i])[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>),</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    axs[i, <span class="dv">1</span>].imshow(torchvision.utils.make_grid(pred_output_history[i])[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s make a whle grid of predictions, sampling with more steps:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try some at higher steps</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">20</span> <span class="co"># Try 2, 5, 10, 50 - how does it affact things?</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">64</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).to(device)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> [x.detach().cpu()]</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    noise_amount <span class="op">=</span> torch.ones((x.shape[<span class="dv">0</span>], )).to(device) <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>(i<span class="op">/</span>n_steps)) <span class="co"># Starting high going low</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(x, noise_amount)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    mix_factor <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(n_steps <span class="op">-</span> i) <span class="co"># Explain how we're moving linearly towards the solution</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>mix_factor) <span class="op">+</span> pred<span class="op">*</span>mix_factor</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>ax.imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow<span class="op">=</span><span class="dv">8</span>)[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>&lt;matplotlib.image.AxesImage&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-28-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="class-conditioning" class="level2">
<h2 class="anchored" data-anchor-id="class-conditioning">Class Conditioning</h2>
<p>Let’s give the model even more information to work with:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a network</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> NoiseAndClassConditionedUNet(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we need x, noise_amount AND y to make predictions:</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>noise_amount <span class="op">=</span> torch.rand([bs])</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>x.shape, net(x, noise_amount, y).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([64, 1, 28, 28]), torch.Size([64, 1, 28, 28]))</code></pre>
</div>
</div>
<p>And training as before, but now with <code>y</code> (the class labels for the batch) as additional conditioning: <code>pred = net(noisy_x, noise_amount, y.to(device))</code></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dataloader (you can mess with batch size)</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(mnist_dataset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># How many runs through the data should we do?</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Our new network type: </span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> NoiseAndClassConditionedUNet(<span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># &lt;&lt;&lt; Using our new noise and class conditioned net</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>net.to(device) <span class="co"># We want to train on the GPU if that is available</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Our loss finction</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The optimizer - explore different learning rates or try</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># a different optimizer instead</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(net.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>) </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Keeping a record of the losses for later viewing</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a><span class="co"># And a record of smoothed loss values after each epoch</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>smoothed_losses_class_cond <span class="op">=</span> []</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a><span class="co"># The training loop</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> train_dataloader:</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get some data and prepare the corrupted version</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device) <span class="co"># Data on the GPU</span></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>        noise_amount <span class="op">=</span> torch.rand(x.shape[<span class="dv">0</span>]).to(device) <span class="co"># Pick random noise amounts</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>        noisy_x <span class="op">=</span> corrupt(x, noise_amount) <span class="co"># Create our noisy x</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the model prediction</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(noisy_x, noise_amount, y.to(device)) <span class="co"># &lt;&lt;&lt;&lt;&lt;&lt;</span></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the loss</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, x) <span class="co"># How close is the output to the true 'clean' x?</span></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop and update the params:</span></span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the loss for later</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print our the average of the last 100 loss values to get an idea of progress:</span></span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a>    avg_loss <span class="op">=</span> <span class="bu">sum</span>(losses[<span class="op">-</span><span class="dv">100</span>:])<span class="op">/</span><span class="dv">100</span></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a>    smoothed_losses_class_cond.append(avg_loss)</span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Finished epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">. Average of the last 100 loss values: </span><span class="sc">{</span>avg_loss<span class="sc">:05f}</span><span class="ss">'</span>)</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a><span class="co"># View the loss curve</span></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Finished epoch 0. Average of the last 100 loss values: 0.021159
Finished epoch 1. Average of the last 100 loss values: 0.017454
Finished epoch 2. Average of the last 100 loss values: 0.016664
Finished epoch 3. Average of the last 100 loss values: 0.015876
Finished epoch 4. Average of the last 100 loss values: 0.015266
Finished epoch 5. Average of the last 100 loss values: 0.015012
Finished epoch 6. Average of the last 100 loss values: 0.014697
Finished epoch 7. Average of the last 100 loss values: 0.014331
Finished epoch 8. Average of the last 100 loss values: 0.013996
Finished epoch 9. Average of the last 100 loss values: 0.013917</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The reason this is useful is that we can now feed in a set of labels as our conditioning during sampling, and hopefully see those reflected in the outputs:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">80</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).to(device)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([[i]<span class="op">*</span><span class="dv">8</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]).flatten().to(device)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> [x.detach().cpu()]</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    noise_amount <span class="op">=</span> torch.ones((x.shape[<span class="dv">0</span>], )).to(device) <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>(i<span class="op">/</span>n_steps))</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> net(x, noise_amount, y)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    mix_factor <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(n_steps <span class="op">-</span> i)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>mix_factor) <span class="op">+</span> pred<span class="op">*</span>mix_factor</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(<span class="dv">0</span>, <span class="dv">1</span>), nrow<span class="op">=</span><span class="dv">8</span>)[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'Greys'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>&lt;matplotlib.image.AxesImage&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-32-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Knowing what digit it is working on DOES give the model more of a hint, and we see a lower loss than the unconditional version during training:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_losses_basic, label<span class="op">=</span><span class="st">'Smoothed loss - basic (no conditioning)'</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_losses_noise_cond, label<span class="op">=</span><span class="st">'Smoothed loss - noise conditioned'</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_losses_class_cond, label<span class="op">=</span><span class="st">'Smoothed loss - noise and class conditioned'</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="what-have-we-learnt-and-how-much-was-a-lie" class="level3">
<h3 class="anchored" data-anchor-id="what-have-we-learnt-and-how-much-was-a-lie">What have we learnt, and how much was a lie?</h3>
<p>This exercise has hopefully given at least a conceptual understanding of roughly what is going on here. None of the components are optimal, and there is some extra complexity we’ll need to address eventually, but it’s not a bad place to start. We’ve identified the key ingredients for training a diffusion model, namely: - A method for gradually corrupting the data - A model of some sort that takes in this corrupted data as inputs - A plan for how much noise to add, and how to reverse the process during sampling.</p>
<p>Turns out nobody does any of these quite like how we did it. When you see a paper or read an explainer notebook on a new diffusion model variant try to see how they do each bit, and dig into why they made those choices.</p>
<p>Questions we haven’t (yet) answered: - Where does the idea of timesteps come in? What does it mean when people talk about discrete vs continuous time formulations? - What training objectives are used? - OK but someone mentioned differential equations? - Something something variance preserving (VP) or variance exploding (VE)??? - What are better ways to sample with these models? - How do I control this with text? - How do ‘real’ implementations feed in the conditioning for noise level/timestep and for things like text? - Why is this better than a one-shot approach like a GAN?</p>
<p>We’ll cover some of these in future lessons or as we go through the DDPM example in the second half of the notebook, but first let’s quickly clarify a few things:</p>
</section>
<section id="clarification-1-whats-in-an-objective" class="level3">
<h3 class="anchored" data-anchor-id="clarification-1-whats-in-an-objective">Clarification 1: What’s in an Objective?</h3>
<p>You may think that predicting the noise (from which we can derive what the denoised image looks like) is equivalent to just predicting the denoised image directly. So why favour one over the other - is it just for mathematical convenience?</p>
<p>It turns out there’s another subtlety here. We compute the loss across different (randomly chosen) timesteps during training. These different objectives will lead to different ‘implicit weighting’ of these losses, where predicting the noise puts more weight on lower noise levels. You can pick more complex objectives to change this ‘implicit loss weighting’. Or perhaps you choose a noise schedule that will result in more examples at a higher noise level. Perhaps you have the model predict a ‘velocity’ v which we define as being a combination of both the image and the noise dependant on the noise level (see ‘PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS’). Perhaps you have the model predict the noise but then scale the loss by some factor dependant on the amount of noise based on a bit of theory (see ‘Perception Prioritized Training of Diffusion Models’) or based on experiments trying to see what noise levels are most informative to the model (see ‘Elucidating the Design Space of Diffusion-Based Generative Models’). TL;DR: choosing the objective has an effect on model performance, and research in ongoing into what the ‘best’ option is.</p>
<p>At the moment, predicting the noise (epsilon or eps you’ll see in some places) is the favoured approach, but work on distillation and other applications suggest that something like the v-objective might lead to more stable training, and as this course is being put together work is ongoing to add support for these different objectives to the diffusers library (see [todo link PR]).</p>
</section>
<section id="clarification-2-variance-preserving-vp-vs-variance-exploding-ve" class="level3">
<h3 class="anchored" data-anchor-id="clarification-2-variance-preserving-vp-vs-variance-exploding-ve">Clarification 2: Variance Preserving (VP) vs Variance Exploding (VE)</h3>
<p>In some formulations, the corruption process looks something like ours: combining x with noise, scaling both so that the result is still roughly in the same range. This is the variance preserving case, and we’ll see a more typical example in the DDPM-style noising process later.</p>
<p>An alternative is to do the ‘corruption’ as follows: <code>x_noisy = x + noise*sigma</code>, where noise is gaussian noise (variance one) and sigma can be high - 20, or 80 for example. This too results in a noisy x that is part data and part noise, but the variance of the noisy x can be much higher than the original variance of the data - hence ‘variance exploding’. In these cases the noisy x is typically scaled according to sigma or otherwise normalized before being fed into the model.</p>
</section>
<section id="clarification-3-continuous-vs-discrete-time" class="level3">
<h3 class="anchored" data-anchor-id="clarification-3-continuous-vs-discrete-time">Clarification 3: Continuous vs discrete time</h3>
<p>Many papers formulate the noising process as a finite sequence of discrete steps, each adding some small amount of noise. During training, a large number of timesteps are used (1000, or 10000). Some prefer to remove this discretization and treat the process as continuous, with T running from 0 to 1 (just like our ‘amount’ here can be any value between 0 and 1). This divide means you’ll hear some people rant about the ‘timesteps’ terminology, and others grumble about having to generalize to continuous formalizations. I suspect we’ll see a trend towards working with <code>sigma</code> or some similar measure of the noise level rather than the timestep version, but for now you’ll need to keep track of which approach a given implementation favours.</p>
</section>
<section id="clarification-4-differential-equations" class="level3">
<h3 class="anchored" data-anchor-id="clarification-4-differential-equations">Clarification 4: Differential Equations</h3>
<p>The derivative of a function describes how it changes. Differential equations describe how functions and their derivatives relate. <br></p>
<p>The corruption process is an example: given a noisy x, it specifies a <strong>change in x</strong> (a derivative) at a given time. Because there is randomness involved, we call this process a ‘stochastic differential equation’. <br></p>
<p>A model the estimates the <strong>noise</strong> in a noisy x can be though of as predicting the gradient. And ‘undoing’ the corruption process can be thought of as solving an (ordinary) differential equation. We usually can’t solve ODEs in one step, but we can iteratively approximate a solution. And this is one way people view sampling methods for diffusion models: as custom ODE solvers. Indeed, you can use off-the-shelf DE solving tools to sample diffusion models if you formulate things the right way. <br></p>
<p>That said, you can also go a long way towards understanding diffusion models without worrying too much about the mathy details around differential equations. If you want to dive deeper, <a href="https://www.youtube.com/watch?v=mYpjmM7O-30">this video</a> introduces some of these ideas alongisde the notation you’ll need to understand this a little better. But if you’d prefer to save yourself the trouble, forget DEs for now and let’s instead continue to explore diffusion models together.<br></p>
</section>
</section>
</section>
<section id="ddpm-with-the-diffusers-library" class="level1">
<h1>DDPM with the Diffusers library</h1>
<p>With the first example held in mind, let’s look at an implementation of a diffusion model based on [the DDPM paper] and see how it compares to our toy version above:</p>
<section id="the-data" class="level3">
<h3 class="anchored" data-anchor-id="the-data">The Data</h3>
<p>Explain - TODO dataset lib maybe?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"huggan/smithsonian_butterflies_subset"</span>, split<span class="op">=</span><span class="st">"train"</span>) <span class="co"># A smaller dataset featuring butterflies</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>preprocess <span class="op">=</span> transforms.Compose(</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        transforms.Resize((image_size, image_size)),</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        transforms.RandomHorizontalFlip(),</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize([<span class="fl">0.5</span>], [<span class="fl">0.5</span>]),</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transform(examples):</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> [preprocess(image.convert(<span class="st">"RGB"</span>)) <span class="cf">for</span> image <span class="kw">in</span> examples[<span class="st">"image"</span>]]</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"images"</span>: images}</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>dataset.set_transform(transform)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> torch.utils.data.DataLoader(dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-scheduler" class="level3">
<h3 class="anchored" data-anchor-id="the-scheduler">The Scheduler</h3>
<p>The scheduler handles adding noise, sampling and managing any pre or post-processing of the model predictions.</p>
<p>We can set up a scheduler based on the DDPM paper like so:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> DDPMScheduler</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>noise_scheduler <span class="op">=</span> DDPMScheduler(num_train_timesteps<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Noise a batch of images to view the effect</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">6</span>))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>xb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))[<span class="st">'images'</span>].to(device)[:<span class="dv">8</span>]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'X shape'</span>, xb.shape)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].imshow(torchvision.utils.make_grid(xb[:<span class="dv">8</span>]).detach().cpu().permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.5</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>timesteps <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">999</span>, <span class="dv">8</span>).<span class="bu">long</span>().to(device)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.randn_like(xb)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>noisy_xb <span class="op">=</span> noise_scheduler.add_noise(xb, noise, timesteps)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Noisy X shape'</span>, noisy_xb.shape)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].imshow(torchvision.utils.make_grid(noisy_xb[:<span class="dv">8</span>]).clip(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>).detach().cpu().permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X shape torch.Size([8, 3, 32, 32])
Noisy X shape torch.Size([8, 3, 32, 32])</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;matplotlib.image.AxesImage&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-38-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ??noise_scheduler.add_noise</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The key line of code here is:</p>
<p><code>noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise</code></p>
<p>We can plot these two scaling factors over the training timesteps:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>plt.plot(noise_scheduler.alphas_cumprod.cpu()<span class="op">**</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'sqrt_alpha_prod'</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>plt.plot((<span class="dv">1</span> <span class="op">-</span> noise_scheduler.alphas_cumprod.cpu()) <span class="op">**</span> <span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'sqrt_one_minus_alpha_prod'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-40-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="the-model-1" class="level2">
<h2 class="anchored" data-anchor-id="the-model-1">The model</h2>
<p>The Unet used here is a little fancier than our mini one above.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> UNet2DModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> UNet2DModel(</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>image_size,  <span class="co"># the target image resolution</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    in_channels<span class="op">=</span><span class="dv">3</span>,  <span class="co"># the number of input channels, 3 for RGB images</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    out_channels<span class="op">=</span><span class="dv">3</span>,  <span class="co"># the number of output channels</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    layers_per_block<span class="op">=</span><span class="dv">2</span>,  <span class="co"># how many ResNet layers to use per UNet block</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    block_out_channels<span class="op">=</span>(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>),  <span class="co"># &lt;&lt;&lt; smaller than their eg</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    down_block_types<span class="op">=</span>( </span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"DownBlock2D"</span>,  <span class="co"># a regular ResNet downsampling block</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"AttnDownBlock2D"</span>,  <span class="co"># a ResNet downsampling block with spatial self-attention</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"AttnDownBlock2D"</span>,</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    ), </span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    up_block_types<span class="op">=</span>(</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"AttnUpBlock2D"</span>, </span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"AttnUpBlock2D"</span>,  <span class="co"># a ResNet upsampling block with spatial self-attention</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"UpBlock2D"</span>,   <span class="co"># a regular ResNet upsampling block</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>      ),</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>model.to(device)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model # Uncomment to view the model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>TODO: talk about resnets in some bonus notebook</p>
<p>TODO talk about attention referencing transformers lesson</p>
<p>TODO look at the code to see timestep embedding</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure the shapes work by pushing our noisy batch through</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>model_output <span class="op">=</span> model(noisy_xb, timestep<span class="op">=</span>timesteps).sample</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Output shape:'</span>, model_output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output shape: torch.Size([8, 3, 32, 32])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the outputs as they currently stand:</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">3</span>))</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>axs.imshow(torchvision.utils.make_grid(model_output[:<span class="dv">8</span>]).clip(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>).detach().cpu().permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)<span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-45-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>([p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters()]) <span class="co"># That's a lot of parameters!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>16056451</code></pre>
</div>
</div>
</section>
<section id="the-training-loop" class="level2">
<h2 class="anchored" data-anchor-id="the-training-loop">The training loop</h2>
<p>As before we will: - Grab some data - Noise by random amounts (using <code>noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)</code> where timesteps are random ints between 0 and <code>noise_scheduler.num_train_timesteps</code>) - Have the model predict the noise residual (rather than the denoised image) based on the noisy inputs and the timesteps (the latter as conditioning). - Cmpute a loss (MSE), backward, update with an optimizer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        clean_images <span class="op">=</span> batch[<span class="st">'images'</span>].to(device)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample noise to add to the images</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn(clean_images.shape).to(clean_images.device)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>        bs <span class="op">=</span> clean_images.shape[<span class="dv">0</span>]</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample a random timestep for each image</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>        timesteps <span class="op">=</span> torch.randint(<span class="dv">0</span>, noise_scheduler.num_train_timesteps, (bs,), device<span class="op">=</span>clean_images.device).<span class="bu">long</span>()</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add noise to the clean images according to the noise magnitude at each timestep</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        noisy_images <span class="op">=</span> noise_scheduler.add_noise(clean_images, noise, timesteps)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get the model prediction</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>        noise_pred <span class="op">=</span> model(noisy_images, timesteps, return_dict<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the loss</span></span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.mse_loss(noise_pred, noise)</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>        loss.backward(loss)</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the model parameters with the optimizer</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(epoch, <span class="bu">sum</span>(losses[<span class="op">-</span><span class="bu">len</span>(train_dataloader):])<span class="op">/</span><span class="bu">len</span>(train_dataloader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4 0.08375396009068936
9 0.060123563394881785
14 0.054015768808312714
19 0.0513919978402555
24 0.039610146603081375</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="sampling-1" class="level2">
<h2 class="anchored" data-anchor-id="sampling-1">Sampling</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> DDIMScheduler</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Doesn't have to be the same as scheduler we trained with</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>sampling_scheduler <span class="op">=</span> DDIMScheduler(num_train_timesteps<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>sampling_scheduler.set_timesteps(<span class="dv">40</span>)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample with this new scheduler</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>).to(device)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(sampling_scheduler.timesteps):</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get model pred</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        residual <span class="op">=</span> model(sample, t).sample</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sample with step</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> sampling_scheduler.step(residual, t, sample).prev_sample</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. optionally look at image</span></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Soon we can do:</span></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pred_original_sample = scheduler.step(residual, t, sample).pred_original_sample </span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># But for now we steal code from step:</span></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look at predicted output image if this is the noise (see sampler code for formula)</span></span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>        alpha_prod_t <span class="op">=</span> sampling_scheduler.alphas_cumprod[t]</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>        beta_prod_t <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> alpha_prod_t</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>        pred_original_sample <span class="op">=</span> (sample <span class="op">-</span> beta_prod_t <span class="op">**</span> (<span class="fl">0.5</span>) <span class="op">*</span> residual) <span class="op">/</span> alpha_prod_t <span class="op">**</span> (<span class="fl">0.5</span>)</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>        preview_im <span class="op">=</span> torchvision.utils.make_grid(torch.cat([sample, pred_original_sample], dim<span class="op">=</span><span class="dv">0</span>)).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a>        display_sample(preview_im, <span class="ss">f'prev_sample and pred_original_sample at step: </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (timestep </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">)'</span>, size<span class="op">=</span>(<span class="dv">128</span>, <span class="dv">64</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'prev_sample and pred_original_sample at step: 9 (timestep 750)'</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-51-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<pre><code>'prev_sample and pred_original_sample at step: 19 (timestep 500)'</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-51-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<pre><code>'prev_sample and pred_original_sample at step: 29 (timestep 250)'</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-51-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<pre><code>'prev_sample and pred_original_sample at step: 39 (timestep 0)'</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-51-output-8.png" class="img-fluid"></p>
</div>
</div>
<p>Viewing a grid of more samples:</p>
<div class="cell">
<div class="cell-output cell-output-display">
<p><img src="12_DM1_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As you can see, this model isn’t perfect… <br></p>
<p>You can try changing the number of layers and the number of channels per layer, as well as training for much longer. You can also try different datasets, image sizes etc. I’ve made a script (scripts/train_unconditional_diffusion_model.py) to make experimenting easier, and you can see some different runs <a href="https://wandb.ai/tglcourse/lesson12_diffusers_training">in thie W&amp;B project</a>. The project is open - you can edit the script or try it with different command-line options to log your own runs for us to compare. Collectively, we should be able to figure out some sensible defaults for unet size, training parameters etc! I’ll write up anything fun we discover in <a href="https://wandb.ai/tglcourse/lesson12_diffusers_training/reports/Exploring-Diffusion-Model-Training--VmlldzoyODg4ODY2">this report</a> (which also has some extra info and tips on where to start).</p>
<p>Training from scratch (especially at higher resolution) can be very time-consuming, and so in the next lesson we’ll show how you can start from an existing diffusion model and re-train it on new data, cutting down the time required for a good model. And then we’ll see what we can do with these unconditional models to steer them as we wish.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co">## PS: If you run the script with --save_model and specify --output_dir it will save a pipeline</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="co">## which you can load like so:</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="co"># from diffusers import DDPMPipeline</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pipeline = DDPMPipeline.from_pretrained('my_output_dir')</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a><span class="co"># pipeline().images[0] # Sample a single image (DDPM sampler)</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="co">## We'll cover saving models to the hub in the next lesson.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="our-toy-example-to-the-max" class="level2">
<h2 class="anchored" data-anchor-id="our-toy-example-to-the-max">Our ‘Toy’ Example TO THE MAX</h2>
<p>You may be wondering: that toy example, were all those choices terrible? What would this ‘naive’ corruption method and sampling approach look like if you trained a decent unet model to ‘decorrupt’ the image? Turns out you can get that approach working fairly well with a few tweaks. The UNet TODO write up experiments here.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>We’ve met diffusion models and tried a few toy demos. Hopefully you’ve got a handle on the key pieces and what they do. In the next lesson we’ll cover a number of improvements.</p>
<p>Page stats: Total Hits: <a href="http://hits.dwyl.com/johnowhitaker/tglcourse"><img src="https://hits.dwyl.com/johnowhitaker/tglcourse.svg?style=flat-square&amp;show=unique" class="img-fluid" alt="HitCount"></a> Page visitors: <img src="https://page-views.glitch.me/badge?page_id=tglcourse.l12" class="img-fluid" alt="visitor badge"></p>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>