[
  {
    "objectID": "pytorch_basics.html",
    "href": "pytorch_basics.html",
    "title": "Lesson 1: PyTorch Basics",
    "section": "",
    "text": "PyTorch is primarily a deep learning framework. It has been designed to make creating and working with deep neural networks as easy, fast and flexible as possible. Today we’ll look at one of the core components that makes this possible: tensors. We’ll start by looking at how to contruct and manipulate tensors, and then apply some of these ideas by representing images as tensors and seeing what we can do with that.\nRemember: this isn’t an exhaustive reference! The goal here is just to begin building a bit of familiarity with some tensor operations - you can always come back later or dig into the excellent PyTorch Documentation for a more complete explanation of what all of these functions can do!\nAs mentioned in the intro video above, you’re welcome to reach out via Discord if you have questions. There is a long-form notebook run-through video here where I go through all of the code and explain things in a bit more detail."
  },
  {
    "objectID": "pytorch_basics.html#creating-tensors",
    "href": "pytorch_basics.html#creating-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Creating Tensors",
    "text": "Creating Tensors\nWe can construct a tensor directly from some common python iterables, such as list and tuple. Nested iterables can also be handled as long as the dimensions make sense.\n\n# tensor from a list\na = torch.tensor([0, 1, 2])\nprint(f\"Tensor a: {a}\")\n\n#tensor from a tuple of tuples\nb = ((1.0, 1.1), (1.2, 1.3))\nb = torch.tensor(b)\nprint(f\"Tensor b: {b}\")\n\n# tensor from a numpy array\nc = np.ones([2, 3])\nc = torch.tensor(c)\nprint(f\"Tensor c: {c}\")\n\nTensor a: tensor([0, 1, 2])\nTensor b: tensor([[1.0000, 1.1000],\n        [1.2000, 1.3000]])\nTensor c: tensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\n\n\nIt’s easy to get confused as to what ‘shape’ a tensor has. So debugging tip #1: when in doubt, print out the shape!\n\nprint(b.shape)\n\ntorch.Size([2, 2])\n\n\nThere are also various constructor methods you can use. The arguments determine the size - explore changing these are see what happens to the output:\n\nx = torch.ones(5, 3)\ny = torch.zeros(2)\nz = torch.empty(1, 1, 5)\nprint(f\"Tensor x: {x}\")\nprint(f\"Tensor y: {y}\")\nprint(f\"Tensor z: {z}\")\n\nTensor x: tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\nTensor y: tensor([0., 0.])\nTensor z: tensor([[[-1.6948e+14,  4.5755e-41, -6.6485e-26, -1.6412e+22, -1.7098e+14]]])\n\n\nNotice that .empty() does not return zeros, but seemingly random small numbers. Unlike .zeros(), which initialises the elements of the tensor with zeros, .empty() just allocates the memory. It is hence a little bit faster if you are looking to just create a tensor.\nThere are also constructors for random numbers:\n\n# uniform distribution\na = torch.rand(1, 3)\n\n# normal distribution\nb = torch.randn(3, 4)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\n\nTensor a: tensor([[0.9829, 0.4177, 0.3677]])\nTensor b: tensor([[ 1.0115,  0.2457, -0.6459, -1.9705],\n        [-0.1782,  0.4208, -0.1519,  0.2699],\n        [-0.2137, -2.2777, -0.5582, -2.0515]])\n\n\nTHINK: What’s the difference? If you’re curious, use plt.hist(torch.randn(100)) to view the distribution.\nThere are also constructors that allow us to construct a tensor according to the above constructors, but with dimensions equal to another tensor:\n\nc = torch.zeros_like(a)\nd = torch.rand_like(c)\nprint(f\"Tensor c: {c}\")\nprint(f\"Tensor d: {d}\")\n\nTensor c: tensor([[0., 0., 0.]])\nTensor d: tensor([[0.6201, 0.9000, 0.1410]])\n\n\nFinally, .arange() and .linspace() behave how you would expect them to if you are familar with numpy.\n\na = torch.arange(0, 10, step=1) # Equivalent to np.arange(0, 10, step=1)\nb = torch.linspace(0, 5, steps=11) # np.linspace(0, 5, num=11)\n\nprint(f\"Tensor a: {a}\\n\")\nprint(f\"Tensor b: {b}\\n\")\n\nTensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nTensor b: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n        4.5000, 5.0000])\n\n\n\nAgain, play with the code above to get familiar with the different arguments and how they affect the output. If you’re not sure of what arguments a function takes, you can query it from Jupyter. Remove the # then run this cell to see the docstring.\n\n#?torch.linspace"
  },
  {
    "objectID": "pytorch_basics.html#tensor-operations",
    "href": "pytorch_basics.html#tensor-operations",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Tensor Operations",
    "text": "Tensor Operations\nWe can perform operations on tensors using methods under torch.. However, in PyTorch most common Python operators are overridden, so we can use those instead. The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations.\n\nx = torch.tensor([1, 2, 4, 8])\ny = torch.tensor([1, 2, 3, 4])\nprint('Addition via torch.add:', torch.add(x, y))\nprint('Addition using \"+\":', x+y) # The same\nprint('Some other operations:')\nx + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation\n\nAddition via torch.add: tensor([ 2,  4,  7, 12])\nAddition using \"+\": tensor([ 2,  4,  7, 12])\nSome other operations:\n\n\n(tensor([ 2,  4,  7, 12]),\n tensor([0, 0, 1, 4]),\n tensor([ 1,  4, 12, 32]),\n tensor([1.0000, 1.0000, 1.3333, 2.0000]),\n tensor([   1,    4,   64, 4096]))\n\n\nTHINK: What does ‘element-wise’ mean? Inspect the outputs above.\nTensors also have many built-in methods such as .mean() or .sum() (see the full list here: https://pytorch.org/docs/stable/tensors.html). Whenever you’re working with a multi-dimensional tensor, pay attention to the dimensions and think about what result you’re aiming to achieve.\n\nx = torch.rand(3, 3)\nprint('x:\\n', x)\nprint(f\"Sum of every element of x: {x.sum()}\")\nprint(f\"Sum of the columns of x: {x.sum(axis=0)}\")\nprint(f\"Sum of the rows of x: {x.sum(axis=1)}\")\n\nx:\n tensor([[0.4153, 0.6779, 0.5828],\n        [0.5472, 0.5127, 0.6336],\n        [0.3987, 0.8190, 0.7021]])\nSum of every element of x: 5.2892351150512695\nSum of the columns of x: tensor([1.3612, 2.0096, 1.9185])\nSum of the rows of x: tensor([1.6760, 1.6934, 1.9198])\n\n\nRemember we said most operations default to ‘element-wise’? What if we want the matrix operation? Torch has you covered there as well. torch.matmul() or the @ symbol let you do matrix multiplication. For dot multiplication, you can use torch.dot().\nTransposes of 2D tensors are obtained using torch.t() or Tensor.T. Note the lack of brackets for Tensor.T - it is an attribute, not a method.\n\na = torch.rand(2, 3)\nb = a.T\nprint('a.shape:', a.shape, 'b.shape:', b.shape)\nprint(a@b) # Matrix multiplication of a 2x3 with a 3x2 matrix gives a 2x2 result.\n\na.shape: torch.Size([2, 3]) b.shape: torch.Size([3, 2])\ntensor([[0.5995, 0.9596],\n        [0.9596, 1.9312]])"
  },
  {
    "objectID": "pytorch_basics.html#manipulating-tensors",
    "href": "pytorch_basics.html#manipulating-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Manipulating Tensors",
    "text": "Manipulating Tensors\nBeyond mathematical operations, we often want to access specific items or sets if items in a tensor, or perform operations like changing the shape of a tensor. Here are a few examples of some common tasks. These may feel simple if you’re used to something like numpy, but it’s worth making sure you know how to do these basic operations (or at least, you know where to find these examples again to refer to them!) since we’ll use these a lot in the coming lessons.\n\n# Indexing tensors\nx = torch.arange(0, 10)\nprint(x)\nprint(x[-1])\nprint(x[1:3]) # From index 1 up to but NOT INCLUDING index 3\nprint(x[:-2])\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ntensor(9)\ntensor([1, 2])\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n\n\nReshaping works as long as the shapes make sense. (3, 4) -> (4, 3) is fine, but (3, 4) -> (8, 2) won’t work since there aren’t enough elements!\n\nprint('Starting tensor:')\nz = torch.arange(1, 13)\nprint('z.shape:', z.shape)\nprint(f'z: {z}\\n')\n\nprint('Reshaping to (3, 4):')\nz = z.reshape(3, 4)\nprint('z.shape:', z.shape)\nprint(f'z:{z}\\n')\n\nprint('Flattening:')\nz = z.flatten()\nprint('z.shape:', z.shape)\nprint(f'z: {z}')\n\nStarting tensor:\nz.shape: torch.Size([12])\nz: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n\nReshaping to (3, 4):\nz.shape: torch.Size([3, 4])\nz:tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n\nFlattening:\nz.shape: torch.Size([12])\nz: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n\n\nConcatenating tensors is done with torch.cat - take a look at this examples and take note of how the dimension specified affects the output:\n\n# Create two tensors of the same shape\nx = torch.arange(12, dtype=torch.float32).reshape((3, 4))\ny = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n\n\n#concatenate them along rows\ncat_rows = torch.cat((x, y), dim=0)\n\n# concatenate along columns\ncat_cols = torch.cat((x, y), dim=1)\n\n# printing outputs\nprint('Concatenated by rows: shape{} \\n {}'.format(list(cat_rows.shape), cat_rows))\nprint('\\n Concatenated by colums: shape{}  \\n {}'.format(list(cat_cols.shape), cat_cols))\n\nConcatenated by rows: shape[6, 4] \n tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [ 2.,  1.,  4.,  3.],\n        [ 1.,  2.,  3.,  4.],\n        [ 4.,  3.,  2.,  1.]])\n\n Concatenated by colums: shape[3, 8]  \n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
  },
  {
    "objectID": "pytorch_basics.html#squeezing-tensors",
    "href": "pytorch_basics.html#squeezing-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Squeezing Tensors",
    "text": "Squeezing Tensors\nWhen processing batches of data, you will quite often be left with singleton dimensions. e.g. [1,10] or [256, 1, 3]. This dimension can quite easily mess up your matrix operations if you don’t plan on it being there…\nIn order to compress tensors along their singleton dimensions we can use the .squeeze() method. We can use the .unsqueeze() method to do the opposite.\n\nx = torch.randn(1, 10)\nprint(x.shape)\nprint(f\"x[0]: {x[0]}\") # printing the zeroth element of the tensor will not give us the first number!\n\ntorch.Size([1, 10])\nx[0]: tensor([-0.0246, -0.1939, -1.8423, -1.0410, -0.7128,  1.0612,  0.3478,  0.7456,\n        -0.7619, -1.0841])\n\n\n\n# lets get rid of that singleton dimension and see what happens now\nx = x.squeeze(0)\nprint(x.shape)\nprint(f\"x[0]: {x[0]}\")\n\ntorch.Size([10])\nx[0]: -0.024552516639232635\n\n\n\ny = torch.randn(5, 5)\nprint(f\"shape of y: {y.shape}\")\n\n# lets insert a singleton dimension\ny = y.unsqueeze(1) # Note the argument here is 1 - try 0 and 2 and make sure you get a feel for what unsqueeze does. \nprint(f\"shape of y: {y.shape}\")\n\nshape of y: torch.Size([5, 5])\nshape of y: torch.Size([5, 1, 5])"
  },
  {
    "objectID": "pytorch_basics.html#images-as-tensors",
    "href": "pytorch_basics.html#images-as-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Images as Tensors",
    "text": "Images as Tensors\n\n\n\n        \n        \n\n\nNow that we know how to work with tensors, let’s take a look at how we might represent and manipulate images as tensors.\n\nImages as arrays of numbers\n\nim = Image.open('images/frog.png').convert('RGB').resize((128, 128))\nim\n\n\n\n\n\nnp.array(im).shape # Remove .convert('RGB') in the cell above and see how this changes\n\n(128, 128, 3)\n\n\nWe’ll want to do this a bunch during the course, so let’s create a function to load an image using PIL and export it so that we can later access it in other notebooks:\n\nsource\n\n\nload_image_pil\n\n load_image_pil (fn, size=None)\n\nAnd to test that it works:\n\nload_image_pil('images/frog.png', size=(32, 32))\n\n\n\n\nIt will probably come in handy to be able to load images from a url, so let’s export a function for that too:\n\nsource\n\n\npil_from_url\n\n pil_from_url (url, size=None)\n\n\npil_from_url(\"https://images.pexels.com/photos/156934/pexels-photo-156934.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\", size=(250, 200))\n\n\n\n\n\n\nConverting to Tensors\nWe can get a numpy array from a PIL image like so:\n\nim = load_image_pil('images/frog.png', size=(32, 32))\nimage_array = np.array(im)\n\n\nimage_array.shape\n\n(32, 32, 3)\n\n\nThis array has shape (32, 32, 3). The (3) is the channels (Red, Green, Blue). The values are integers between 0 and 255 (8-bit numbers). For compatability with various tools in the PyTorch world, we’re going to shuffle things around so that the channels dimension comes first, and we’ll represent images as floats between 0 and 1:\n\ntensor_im = torch.tensor(np.array(im)).permute(2, 0, 1)/255.0\ntensor_im.shape\n\ntorch.Size([3, 32, 32])\n\n\n\ntensor_im.min(), tensor_im.max()\n\n(tensor(0.), tensor(0.7804))\n\n\nThe torchvision library does this for us too:\n\nto_tensor_transform = torchvision.transforms.ToTensor()\nto_tensor_transform(im).shape\n\ntorch.Size([3, 32, 32])\n\n\nThis channels-first representation is popular but not universal - keep an eye out for variants if you’re using other people’s code :) \nThe final step here is that pytorch also typically works on batches of data, and so for images we usually have a tensor of shape (batch_size, channels, width, height). \nSince this back-and-forth translation is going to be something we do continuously during this course, let’s write functions to do this for us and export them so that we can use these in all the other notebooks:\n\nsource\n\n\ntensor_to_pil\n\n tensor_to_pil (tensor_im)\n\n\nsource\n\n\npil_to_tensor\n\n pil_to_tensor (im)\n\nLet’s see this in action:\n\nim = load_image_pil('images/frog.png', size=(128, 128)) # Load an image\ntensor_im = pil_to_tensor(im) # To tensor with our function\nprint(tensor_im.shape)\nim_out = tensor_to_pil(tensor_im) # And back to a PIL imge\nprint(type(im_out))\nim_out\n\ntorch.Size([1, 3, 128, 128])\n<class 'PIL.Image.Image'>\n\n\n\n\n\nAnother thing we’ll do is visualize images or specific image channels using matplotlib. It is fine with images being floats between 0 and 1, but expects the channels dimension first (for RGB images) or no channel dimension (for single-channel images). To demo this here’s how we might plot the three separate color channels in an image:\n\n# Plotting the original image and the three color channels\nfig, axs = plt.subplots(1, 4, figsize=(16, 5))\naxs[0].imshow(tensor_im[0].permute(1, 2, 0)) # Note: we need to rearrange the color channels\naxs[0].set_title('Original image')\ncolors = ['red', 'green', 'blue']\nfor i in range(1, 4):\n    axs[i].imshow(tensor_im[0][i-1], cmap='gray')\n    axs[i].set_title(f'{colors[i-1]} channel')\n\n\n\n\nWe can index into the tensor image to access specific slices or parts of the image. For the plotting above, we accessed each channel independantly. We can also select all channels but only a subset of pixels:\n\ncropped = tensor_im[0, :, 50:100, 50:120] # First image in batch, all channels, 50px high and 70 wide starting from (50, 50) (top left is 0, 0)\ntensor_to_pil(cropped)\n\n\n\n\n\n# Exercise: Can you make a greyscale image by taking the mean of the three color channels? \n# You may need to look at the pytorch docs for 'mean', as well as 'expand' or 'repeat' or 'cat' to turn the single-channel \n# result back into a three-channel image.\n\n\n\nTransforms\nWe’ll often want to perform some kind of image transform, such as zooming, rotating, warping or cropping. You can do this with the Python Imaging Library (PIL) but for performance reasons it is also nice to be able to do these operations on tensors, which is where the torchvision library comes in. Torchvision provides a number fo transforms which we can use on images, including random transforms which are useful for data augmentation. Uncomment the cell below to see a list of some of the available options, and try a few out for yourself:\n\n# print(dir(torchvision.transforms)[:-16]) # Uncomment to see some of the available options\n\n\ntransform = torchvision.transforms.RandomAffine(30)\ntensor_to_pil(transform(tensor_im))\n\n\n\n\n\n\nExercise: Least Average Image\nLet’s end this lesson with a bit of an exercise. Given the tensor operations we’ve looked at, can you find an interesting way to combine a number of images into a single output? For example, given a set of images, for each pixel location find the furthest value from the mean. Inspiration: https://www.andreweckel.com/LeastAverageImage/  Experiment with variations on this idea. \nAn example with stills from a video I made:\n\n\n\n\n\nAnd one made in a slightly different way with 5 random images from this course:\n\n\n\n\n\n\n# Your code here..."
  },
  {
    "objectID": "pytorch_basics.html#a-dose-of-ethics-cameras-and-race",
    "href": "pytorch_basics.html#a-dose-of-ethics-cameras-and-race",
    "title": "Lesson 1: PyTorch Basics",
    "section": "A Dose of Ethics: Cameras and Race",
    "text": "A Dose of Ethics: Cameras and Race\nCameras often have a limit on their ‘dynamic range’ - how much difference there can be between the lightest and darkest region of an image. Some sensors can capture more information in the RAW file format, but then processing needs to be done to compress that range down to be represented as the 8-bit jpegs that the rest of us see. A lot of time the camera settings and processing pipeline are automated or use a set of defaults tuned to be convenient for the “average user”.\nWhat does that have to do with race or ethics? Anyone with dark skin will be able to tell you: the defaults often suck for anyone with a different skin tone! Slow progress is being made on adapting photography tools to better handle skin tone variation, but for many years the defaults have been tied to a specific subset of humanity, which in turn has had knock-on effects on who gets represented in image data, which in turn means that things like facial recognition algorithms perform measurably worse on some populations.\nAs you move through this course, keep this in mind: even seeimgly arbitrary technical choices (such as default exposure settings) can have consequences for people down the line. Let’s work towards designing inclusive systems that can benefit as many people as possible.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "gan2.html",
    "href": "gan2.html",
    "title": "Lesson 8: Conditional GANs, Training Tricks and GANs+CLIP",
    "section": "",
    "text": "Introduce the idea of conditioning\nPlay with BigGAN\nMaybe key ideas of stylegan…\nSHow GauGAN as amazing conditioning\nOptimize a latent with CLIP\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm1.html",
    "href": "dm1.html",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "",
    "text": "Diffusion models are a little different to the other types of generative models you may have encountered. During training, input images (or any other kind of data) are corrupted, and the model attempts to ‘undo’ this corruption. Once trained, we can begin from a random set of inputs that resembles data that has been so completely degraded as to be unrecognizeable and then gradually ‘uncorrupt’ this, typically over a large number of steps.\nTODO diagram\nIn most (but not all!) current approaches, this ‘corruption’ takes the place of adding noise, hence ‘denoising diffusion models’. The exact method of adding noise and the details of how the problem is formulated vary across the literature, which can lead to confusion.\nThe goal of this lesson is to introduce the key concepts without worrying too much about specific implementations. The plan:"
  },
  {
    "objectID": "dm1.html#a-toy-example",
    "href": "dm1.html#a-toy-example",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "A Toy Example",
    "text": "A Toy Example\n\n\n\ntoy photo\n\n\n[video: intro idea]\nLoad the dataset (replace MNIST with FashionMNIST for a variant):\n\n#!output: false\nmnist_dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n\n\n\nViewing the first example image:\n\n# View an example\nim, label = mnist_dataset[0]\nprint('Image shape:', im.shape)\nprint('Image min and max:', im.min(), im.max())\nprint('label:', label)\nplt.imshow(im[0], cmap='Greys');\n\nImage shape: torch.Size([1, 28, 28])\nImage min and max: tensor(0.) tensor(1.)\nlabel: 5\n\n\n\n\n\n\n# See how we can make a dataloader to serve the data in batches for training\ntrain_dataloader = DataLoader(mnist_dataset, batch_size=8, shuffle=True)\nx, y = next(iter(train_dataloader))\nx.shape, y\n\n(torch.Size([8, 1, 28, 28]), tensor([3, 7, 0, 2, 5, 3, 9, 9]))\n\n\n\nThe Corruption Process\nPretend you haven’t read any diffusion model papers, but you know the process involves adding noise. How would you do it?\nWe probably want an easy way to control the amount of corruption. So what if we take in a parameter for the amount of noise to add, and then we do:\nnoise = torch.rand_like(x)\nnoisy_x =  (1-amount)*x + amount*noise\nIf amount = 0, we get back the input without any changes. If amount gets up to 1, we get back noise with no trace of the input x. By mixing the input with noise this way, we keep the output in the same range (0 to 1).\nWe can implement this fairly easily (just watch the shapes so you don’t get burnt by broadcasting rules):\n\ndef corrupt(x, amount):\n    \"\"\"Corrupt the input `x` by mixing it with noise according to `amount`\"\"\"\n    noise = torch.rand_like(x)\n    amount = amount.view(-1, 1, 1, 1) # Sort shape so broadcasting works\n    return x*(1-amount) + noise*amount\n\nAnd looking at the results visually to see that it works as expected:\n\n\n\n\n\nWith this in place, let’s move on to the next piece of this process - the model.\n\n\nThe Model\nSticking with our goal of simplicity, we’ll specify the model here as a neural network that produces an output the same shape as it’s input. I’ve made three variants you can try out:\n\n\nCode\nclass BasicConvNet(nn.Module):\n    \"\"\"A stack of conv layers with padding to keep the output the same size as \n    the input. Hidden channel numbers fixed at: [16, 32, 64, 64, 16].\n    Args: in_channels, out_channels,kernel_size=5.\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=5):\n        super().__init__()\n        padding = kernel_size // 2 # So we keep output size the same\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(64, 16, kernel_size,  padding=padding),\n            nn.ReLU(),\n            nn.Conv2d(16, out_channels, kernel_size, padding=padding),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\n\n\nCode\nfrom torch import nn\nfrom functools import partial\nfrom einops.layers.torch import Rearrange, Reduce\n\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\n\ndef FeedForward(dim, expansion_factor = 4, dropout = 0., dense = nn.Linear):\n    inner_dim = int(dim * expansion_factor)\n    return nn.Sequential(\n        dense(dim, inner_dim),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        dense(inner_dim, dim),\n        nn.Dropout(dropout)\n    )\n\ndef MLPMixer(*, image_size, channels, patch_size, dim, depth, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):\n    \"\"\"A minimal MLP Mixer stolen from lucidrain's implementation.\"\"\"\n    # Get image width and height (same if image_size isn't a tuple):\n    pair = lambda x: x if isinstance(x, tuple) else (x, x)\n    image_h, image_w = pair(image_size)\n    # Check they divide neatly by patch_size\n    assert (image_h % patch_size) == 0 and (image_w % patch_size) == 0, 'image must be divisible by patch size'\n    num_patches = (image_h // patch_size) * (image_w // patch_size)\n    # Prep the two layers\n    chan_first, chan_last = partial(nn.Conv1d, kernel_size = 1), nn.Linear\n    # Return the model (a stack of [FeedForward(chan_first), FeedForward(chan_last)] pairs\n    # with layer norm on the inputs and a skip connection thanks to PreNormResidual)\n    return nn.Sequential(\n        Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n        nn.Linear((patch_size ** 2) * channels, dim),\n        *[nn.Sequential(\n            PreNormResidual(dim, FeedForward(num_patches, expansion_factor, dropout, chan_first)),\n            PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, chan_last))\n        ) for _ in range(depth)],\n        Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h = int(image_h/patch_size),  w = int(image_w/patch_size), p1 = patch_size, p2 = patch_size),\n        nn.Conv2d(dim//(patch_size**2), channels, kernel_size=1) # Back to right number of channels\n    )\n\n\n\n\nCode\nclass BasicUNet(nn.Module):\n    \"\"\"A minimal UNet implementation.\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.down_layers = torch.nn.ModuleList([ \n            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n        ])\n        self.up_layers = torch.nn.ModuleList([\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n            nn.Conv2d(32, out_channels, kernel_size=5, padding=2), \n        ])\n        self.act = nn.SiLU()\n        self.downscale = nn.MaxPool2d(2)\n        self.upscale = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        h = []\n        for i, l in enumerate(self.down_layers):\n            x = self.act(l(x))\n            h.append(x)\n            if i < 2: x = self.downscale(x)\n        for i, l in enumerate(self.up_layers):\n            if i > 0: x = self.upscale(x)\n            x += h.pop()\n            x = self.act(l(x))\n        return x\n\n\nWe’ll stick with the basic UNet for this demo but you can swap in the others if you’d like.\n\nunet = BasicUNet(1, 2)\nunet(torch.rand(8, 1, 28, 28)).shape\n\ntorch.Size([8, 2, 28, 28])\n\n\nYou can see how many parameters this has:\n\nsum([p.numel() for p in unet.parameters()]) # Try halving the number of channels in each layer\n\n309858\n\n\nWe can create one and feed our demo batch of data through to check that it works and that the output shape is the same as the input as we expect:\n\n# Create the network (for single channel input & output images)\nnet = BasicUNet(1, 1)\n\n# Feed some data through:\nx.shape, net(x).shape\n\n(torch.Size([8, 1, 28, 28]), torch.Size([8, 1, 28, 28]))"
  },
  {
    "objectID": "dm1.html#class-conditioning",
    "href": "dm1.html#class-conditioning",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "Class Conditioning",
    "text": "Class Conditioning\nLet’s give the model even more information to work with:\n\n# Creating a network\nnet = NoiseAndClassConditionedUNet(1, 1)\n\n# Now we need x, noise_amount AND y to make predictions:\nx, y = next(iter(train_dataloader))\nbs = x.shape[0]\nnoise_amount = torch.rand([bs])\nx.shape, net(x, noise_amount, y).shape\n\n(torch.Size([64, 1, 28, 28]), torch.Size([64, 1, 28, 28]))\n\n\nAnd training as before, but now with y (the class labels for the batch) as additional conditioning: pred = net(noisy_x, noise_amount, y.to(device))\n\n\nCode\n# Dataloader (you can mess with batch size)\ntrain_dataloader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n\n# How many runs through the data should we do?\nn_epochs = 10\n\n# Our new network type: \nnet = NoiseAndClassConditionedUNet(1, 1) # <<< Using our new noise and class conditioned net\nnet.to(device) # We want to train on the GPU if that is available\n\n# Our loss finction\nloss_fn = nn.MSELoss()\n\n# The optimizer - explore different learning rates or try\n# a different optimizer instead\nopt = torch.optim.Adam(net.parameters(), lr=3e-4) \n\n# Keeping a record of the losses for later viewing\nlosses = []\n\n# And a record of smoothed loss values after each epoch\nsmoothed_losses_class_cond = []\n\n# The training loop\nfor epoch in range(n_epochs):\n    for x, y in train_dataloader:\n        \n        # Get some data and prepare the corrupted version\n        x = x.to(device) # Data on the GPU\n        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts\n        noisy_x = corrupt(x, noise_amount) # Create our noisy x\n\n        # Get the model prediction\n        pred = net(noisy_x, noise_amount, y.to(device)) # <<<<<<\n\n        # Calculate the loss\n        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n\n        # Backprop and update the params:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        # Store the loss for later\n        losses.append(loss.item())\n\n    # Print our the average of the last 100 loss values to get an idea of progress:\n    avg_loss = sum(losses[-100:])/100\n    smoothed_losses_class_cond.append(avg_loss)\n    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n\n# View the loss curve\nplt.plot(losses)\n\n\nFinished epoch 0. Average of the last 100 loss values: 0.021159\nFinished epoch 1. Average of the last 100 loss values: 0.017454\nFinished epoch 2. Average of the last 100 loss values: 0.016664\nFinished epoch 3. Average of the last 100 loss values: 0.015876\nFinished epoch 4. Average of the last 100 loss values: 0.015266\nFinished epoch 5. Average of the last 100 loss values: 0.015012\nFinished epoch 6. Average of the last 100 loss values: 0.014697\nFinished epoch 7. Average of the last 100 loss values: 0.014331\nFinished epoch 8. Average of the last 100 loss values: 0.013996\nFinished epoch 9. Average of the last 100 loss values: 0.013917\n\n\n\n\n\nThe reason this is useful is that we can now feed in a set of labels as our conditioning during sampling, and hopefully see those reflected in the outputs:\n\n\nCode\nn_steps = 20\nx = torch.rand(80, 1, 28, 28).to(device)\ny = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)\nhistory = [x.detach().cpu()]\n\nfor i in range(n_steps):\n    noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps))\n    with torch.no_grad():\n        pred = net(x, noise_amount, y)\n    mix_factor = 1/(n_steps - i)\n    x = x*(1-mix_factor) + pred*mix_factor\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 12))\nax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(0, 1), nrow=8)[0], cmap='Greys')\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\nKnowing what digit it is working on DOES give the model more of a hint, and we see a lower loss than the unconditional version during training:\n\n\nCode\nplt.plot(smoothed_losses_basic, label='Smoothed loss - basic (no conditioning)')\nplt.plot(smoothed_losses_noise_cond, label='Smoothed loss - noise conditioned')\nplt.plot(smoothed_losses_class_cond, label='Smoothed loss - noise and class conditioned')\nplt.legend();\n\n\n\n\n\n\nWhat have we learnt, and how much was a lie?\nThis exercise has hopefully given at least a conceptual understanding of roughly what is going on here. None of the components are optimal, and there is some extra complexity we’ll need to address eventually, but it’s not a bad place to start. We’ve identified the key ingredients for training a diffusion model, namely: - A method for gradually corrupting the data - A model of some sort that takes in this corrupted data as inputs - A plan for how much noise to add, and how to reverse the process during sampling.\nTurns out nobody does any of these quite like how we did it. When you see a paper or read an explainer notebook on a new diffusion model variant try to see how they do each bit, and dig into why they made those choices.\nQuestions we haven’t (yet) answered: - Where does the idea of timesteps come in? What does it mean when people talk about discrete vs continuous time formulations? - What training objectives are used? - OK but someone mentioned differential equations? - Something something variance preserving (VP) or variance exploding (VE)??? - What are better ways to sample with these models? - How do I control this with text? - How do ‘real’ implementations feed in the conditioning for noise level/timestep and for things like text? - Why is this better than a one-shot approach like a GAN?\nWe’ll cover some of these in future lessons or as we go through the DDPM example in the second half of the notebook, but first let’s quickly clarify a few things:\n\n\nClarification 1: What’s in an Objective?\nYou may think that predicting the noise (from which we can derive what the denoised image looks like) is equivalent to just predicting the denoised image directly. So why favour one over the other - is it just for mathematical convenience?\nIt turns out there’s another subtlety here. We compute the loss across different (randomly chosen) timesteps during training. These different objectives will lead to different ‘implicit weighting’ of these losses, where predicting the noise puts more weight on lower noise levels. You can pick more complex objectives to change this ‘implicit loss weighting’. Or perhaps you choose a noise schedule that will result in more examples at a higher noise level. Perhaps you have the model predict a ‘velocity’ v which we define as being a combination of both the image and the noise dependant on the noise level (see ‘PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS’). Perhaps you have the model predict the noise but then scale the loss by some factor dependant on the amount of noise based on a bit of theory (see ‘Perception Prioritized Training of Diffusion Models’) or based on experiments trying to see what noise levels are most informative to the model (see ‘Elucidating the Design Space of Diffusion-Based Generative Models’). TL;DR: choosing the objective has an effect on model performance, and research in ongoing into what the ‘best’ option is.\nAt the moment, predicting the noise (epsilon or eps you’ll see in some places) is the favoured approach, but work on distillation and other applications suggest that something like the v-objective might lead to more stable training, and as this course is being put together work is ongoing to add support for these different objectives to the diffusers library (see [todo link PR]).\n\n\nClarification 2: Variance Preserving (VP) vs Variance Exploding (VE)\nIn some formulations, the corruption process looks something like ours: combining x with noise, scaling both so that the result is still roughly in the same range. This is the variance preserving case, and we’ll see a more typical example in the DDPM-style noising process later.\nAn alternative is to do the ‘corruption’ as follows: x_noisy = x + noise*sigma, where noise is gaussian noise (variance one) and sigma can be high - 20, or 80 for example. This too results in a noisy x that is part data and part noise, but the variance of the noisy x can be much higher than the original variance of the data - hence ‘variance exploding’. In these cases the noisy x is typically scaled according to sigma or otherwise normalized before being fed into the model.\n\n\nClarification 3: Continuous vs discrete time\nMany papers formulate the noising process as a finite sequence of discrete steps, each adding some small amount of noise. During training, a large number of timesteps are used (1000, or 10000). Some prefer to remove this discretization and treat the process as continuous, with T running from 0 to 1 (just like our ‘amount’ here can be any value between 0 and 1). This divide means you’ll hear some people rant about the ‘timesteps’ terminology, and others grumble about having to generalize to continuous formalizations. I suspect we’ll see a trend towards working with sigma or some similar measure of the noise level rather than the timestep version, but for now you’ll need to keep track of which approach a given implementation favours.\n\n\nClarification 4: Differential Equations\nThe derivative of a function describes how it changes. Differential equations describe how functions and their derivatives relate. \nThe corruption process is an example: given a noisy x, it specifies a change in x (a derivative) at a given time. Because there is randomness involved, we call this process a ‘stochastic differential equation’. \nA model the estimates the noise in a noisy x can be though of as predicting the gradient. And ‘undoing’ the corruption process can be thought of as solving an (ordinary) differential equation. We usually can’t solve ODEs in one step, but we can iteratively approximate a solution. And this is one way people view sampling methods for diffusion models: as custom ODE solvers. Indeed, you can use off-the-shelf DE solving tools to sample diffusion models if you formulate things the right way. \nThat said, you can also go a long way towards understanding diffusion models without worrying too much about the mathy details around differential equations. If you want to dive deeper, this video introduces some of these ideas alongisde the notation you’ll need to understand this a little better. But if you’d prefer to save yourself the trouble, forget DEs for now and let’s instead continue to explore diffusion models together."
  },
  {
    "objectID": "dm1.html#the-model-1",
    "href": "dm1.html#the-model-1",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "The model",
    "text": "The model\nThe Unet used here is a little fancier than our mini one above.\n\nfrom diffusers import UNet2DModel\n\n\n# Create a model\nmodel = UNet2DModel(\n    sample_size=image_size,  # the target image resolution\n    in_channels=3,  # the number of input channels, 3 for RGB images\n    out_channels=3,  # the number of output channels\n    layers_per_block=2,  # how many ResNet layers to use per UNet block\n    block_out_channels=(64, 128, 256),  # <<< smaller than their eg\n    down_block_types=( \n        \"DownBlock2D\",  # a regular ResNet downsampling block\n        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n        \"AttnDownBlock2D\",\n    ), \n    up_block_types=(\n        \"AttnUpBlock2D\", \n        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n        \"UpBlock2D\",   # a regular ResNet upsampling block\n      ),\n)\nmodel.to(device);\n\n\n# model # Uncomment to view the model\n\nTODO: talk about resnets in some bonus notebook\nTODO talk about attention referencing transformers lesson\nTODO look at the code to see timestep embedding\n\n# Make sure the shapes work by pushing our noisy batch through\nmodel_output = model(noisy_xb, timestep=timesteps).sample\nprint('Output shape:', model_output.shape)\n\nOutput shape: torch.Size([8, 3, 32, 32])\n\n\n\n# Plot the outputs as they currently stand:\nfig, axs = plt.subplots(1, 1, figsize=(16, 3))\naxs.imshow(torchvision.utils.make_grid(model_output[:8]).clip(-1, 1).detach().cpu().permute(1, 2, 0)*0.5 + 0.5);\n\n\n\n\n\nsum([p.numel() for p in model.parameters()]) # That's a lot of parameters!\n\n16056451"
  },
  {
    "objectID": "dm1.html#the-training-loop",
    "href": "dm1.html#the-training-loop",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "The training loop",
    "text": "The training loop\nAs before we will: - Grab some data - Noise by random amounts (using noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) where timesteps are random ints between 0 and noise_scheduler.num_train_timesteps) - Have the model predict the noise residual (rather than the denoised image) based on the noisy inputs and the timesteps (the latter as conditioning). - Cmpute a loss (MSE), backward, update with an optimizer.\n\nimport torch.nn.functional as F\n\n\n# Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n\nlosses = []\n\nfor epoch in range(25):\n    for step, batch in enumerate(train_dataloader):\n        clean_images = batch['images'].to(device)\n        # Sample noise to add to the images\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n        bs = clean_images.shape[0]\n\n        # Sample a random timestep for each image\n        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n\n        # Add noise to the clean images according to the noise magnitude at each timestep\n        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n        \n        # Get the model prediction\n        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n        \n        # Calculate the loss\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n        losses.append(loss.item())\n\n        # Update the model parameters with the optimizer\n        optimizer.step()\n        optimizer.zero_grad()\n\n    if (epoch+1)%5 == 0: print(epoch, sum(losses[-len(train_dataloader):])/len(train_dataloader))\n\n4 0.08375396009068936\n9 0.060123563394881785\n14 0.054015768808312714\n19 0.0513919978402555\n24 0.039610146603081375\n\n\n\nplt.plot(losses)"
  },
  {
    "objectID": "dm1.html#sampling-1",
    "href": "dm1.html#sampling-1",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "Sampling",
    "text": "Sampling\n\nfrom diffusers import DDIMScheduler\n\n\n# Doesn't have to be the same as scheduler we trained with\nsampling_scheduler = DDIMScheduler(num_train_timesteps=1000)\nsampling_scheduler.set_timesteps(40)\n\n# Sample with this new scheduler\nsample = torch.randn(1, 3, 32, 32).to(device)\nfor i, t in enumerate(sampling_scheduler.timesteps):\n    \n  # Get model pred\n    with torch.no_grad():\n        residual = model(sample, t).sample\n\n    # Update sample with step\n    sample = sampling_scheduler.step(residual, t, sample).prev_sample\n\n    # 3. optionally look at image\n    if (i + 1) % 10 == 0:\n        # Soon we can do:\n        # pred_original_sample = scheduler.step(residual, t, sample).pred_original_sample \n        # But for now we steal code from step:\n        # Look at predicted output image if this is the noise (see sampler code for formula)\n        alpha_prod_t = sampling_scheduler.alphas_cumprod[t]\n        beta_prod_t = 1 - alpha_prod_t\n        pred_original_sample = (sample - beta_prod_t ** (0.5) * residual) / alpha_prod_t ** (0.5)\n\n        preview_im = torchvision.utils.make_grid(torch.cat([sample, pred_original_sample], dim=0)).unsqueeze(0)\n        display_sample(preview_im, f'prev_sample and pred_original_sample at step: {i} (timestep {t})', size=(128, 64))\n\n'prev_sample and pred_original_sample at step: 9 (timestep 750)'\n\n\n\n\n\n'prev_sample and pred_original_sample at step: 19 (timestep 500)'\n\n\n\n\n\n'prev_sample and pred_original_sample at step: 29 (timestep 250)'\n\n\n\n\n\n'prev_sample and pred_original_sample at step: 39 (timestep 0)'\n\n\n\n\n\nViewing a grid of more samples:\n\n\n\n\n\nAs you can see, this model isn’t perfect… \nYou can try changing the number of layers and the number of channels per layer, as well as training for much longer. You can also try different datasets, image sizes etc. I’ve made a script (scripts/train_unconditional_diffusion_model.py) to make experimenting easier, and you can see some different runs in thie W&B project. The project is open - you can edit the script or try it with different command-line options to log your own runs for us to compare. Collectively, we should be able to figure out some sensible defaults for unet size, training parameters etc! I’ll write up anything fun we discover in this report (which also has some extra info and tips on where to start).\nTraining from scratch (especially at higher resolution) can be very time-consuming, and so in the next lesson we’ll show how you can start from an existing diffusion model and re-train it on new data, cutting down the time required for a good model. And then we’ll see what we can do with these unconditional models to steer them as we wish.\n\n## PS: If you run the script with --save_model and specify --output_dir it will save a pipeline\n## which you can load like so:\n# from diffusers import DDPMPipeline\n# pipeline = DDPMPipeline.from_pretrained('my_output_dir')\n# pipeline().images[0] # Sample a single image (DDPM sampler)\n## We'll cover saving models to the hub in the next lesson."
  },
  {
    "objectID": "dm1.html#our-toy-example-to-the-max",
    "href": "dm1.html#our-toy-example-to-the-max",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "Our ‘Toy’ Example TO THE MAX",
    "text": "Our ‘Toy’ Example TO THE MAX\nYou may be wondering: that toy example, were all those choices terrible? What would this ‘naive’ corruption method and sampling approach look like if you trained a decent unet model to ‘decorrupt’ the image? Turns out you can get that approach working fairly well with a few tweaks. The UNet TODO write up experiments here."
  },
  {
    "objectID": "dm1.html#conclusions",
    "href": "dm1.html#conclusions",
    "title": "Lesson 12 - Introduction to Diffusion Models",
    "section": "Conclusions",
    "text": "Conclusions\nWe’ve met diffusion models and tried a few toy demos. Hopefully you’ve got a handle on the key pieces and what they do. In the next lesson we’ll cover a number of improvements.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "clip.html",
    "href": "clip.html",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "",
    "text": "CLIP: - concepts - embedding text and images - measuring similarity - Use as a loss (imstack demo) - Need for transformations - export some functions for later - CLOOB\nCLIP (Contrastive Language-Image Pre-training) is a method created by OpenAI for training models capable of aligning image and text representations. Images and text are drastically different modalities, but CLIP manages to map both to a shared space, allowing for all kinds of neat tricks.\n\n\n\nclip diagram\n\n\nDuring training, CLIP takes in image-caption pairs. The images are fed through an image encoder model (based on a resnet or ViT backbone) and transformed into an embedding vector (I in the diagram above). A second model (typically a transformer model of some sort) takes the text and transforms it into an embedding vector (T in the diagram).\nCrucially, both these embeddings are the same shape, allowing for direct comparison. Given a batch of image-caption pairs, CLIP tries to maximize the similarity of image and text embeddings that go together (the blue diagonal in the diagram) while minimizing the similarity between non-related pairs.\nIn this lesson we’re going to explore some of the use-cases for a model like this, using an open CLIP implementation called OpenCLIP. If you’d like to read some more background on OpenCLIP and see some benchmarks I ran with some of the pretrained models, check out this report I wrote on the subject."
  },
  {
    "objectID": "clip.html#loading-clip-models-with-openclip",
    "href": "clip.html#loading-clip-models-with-openclip",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Loading CLIP models with OpenCLIP",
    "text": "Loading CLIP models with OpenCLIP\nYou can use pretrained CLIP models in a few different ways. There is OpenAI’s github repository or the HuggingFace implementations, but I like the OpenCLIP project which includes both OpenAI versions and a number of new models trained in the open on public datasets like LAION.\nLet’s import the library and see what models are available:\n\nimport open_clip\n\nprint(open_clip.list_pretrained())\n\n[('RN50', 'openai'), ('RN50', 'yfcc15m'), ('RN50', 'cc12m'), ('RN50-quickgelu', 'openai'), ('RN50-quickgelu', 'yfcc15m'), ('RN50-quickgelu', 'cc12m'), ('RN101', 'openai'), ('RN101', 'yfcc15m'), ('RN101-quickgelu', 'openai'), ('RN101-quickgelu', 'yfcc15m'), ('RN50x4', 'openai'), ('RN50x16', 'openai'), ('RN50x64', 'openai'), ('ViT-B-32', 'openai'), ('ViT-B-32', 'laion400m_e31'), ('ViT-B-32', 'laion400m_e32'), ('ViT-B-32', 'laion2b_e16'), ('ViT-B-32', 'laion2b_s34b_b79k'), ('ViT-B-32-quickgelu', 'openai'), ('ViT-B-32-quickgelu', 'laion400m_e31'), ('ViT-B-32-quickgelu', 'laion400m_e32'), ('ViT-B-16', 'openai'), ('ViT-B-16', 'laion400m_e31'), ('ViT-B-16', 'laion400m_e32'), ('ViT-B-16-plus-240', 'laion400m_e31'), ('ViT-B-16-plus-240', 'laion400m_e32'), ('ViT-L-14', 'openai'), ('ViT-L-14', 'laion400m_e31'), ('ViT-L-14', 'laion400m_e32'), ('ViT-L-14', 'laion2b_s32b_b82k'), ('ViT-L-14-336', 'openai'), ('ViT-H-14', 'laion2b_s32b_b79k'), ('ViT-g-14', 'laion2b_s12b_b42k'), ('roberta-ViT-B-32', 'laion2b_s12b_b32k'), ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k')]\n\n\nFollowing the demo code on GitHub, here’s how we load a model:\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n\n100%|████████████████████████████████████████| 605M/605M [00:05<00:00, 109MiB/s]\n\n\n\nAside: Model Naming\nWhat does ‘ViT-B/32’ mean? This uses a Vision Transformer (ViT) as the image encoder, with an image patch size of 32 (larger patch size means fewer patches per image at a given size, and thus faster/smaller models) and the ‘Base’ size model. I believe the size order is ‘Base’ (B), Large (L), Huge (H) and ginormous (g) but don’t quote me on that! Generally the larger models with the smaller patch sizes will need more memory and compute in exchenge for better performance. Most models were trained on 224px input images but there are variants trained on larger image sizes - again they will be more computationally expensive to run. ‘RN50’ style names mean the vision encoder is based on a Resnet50 architecture instead of a vision transformer.\nGenerally I like to stick with the smaller models, but you can experiment with swapping in larger variants or using multiple models and combining the results."
  },
  {
    "objectID": "clip.html#preprocessing",
    "href": "clip.html#preprocessing",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe want to prepare our image inputs to match the data used for training. In most cases, this involves resizing to 224px square images and normalizing the image data:\n\npreprocess # View the preprocessing function loaded with the model above\n\nCompose(\n    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n    CenterCrop(size=(224, 224))\n    <function _convert_to_rgb>\n    ToTensor()\n    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n)"
  },
  {
    "objectID": "clip.html#image---features",
    "href": "clip.html#image---features",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Image -> Features",
    "text": "Image -> Features\nWe can load in an image and pre-process it with this function, and then use model.encode_image to turn it into a CLIP embedding:\n\ninput_image = pil_from_url('https://images.pexels.com/photos/185032/pexels-photo-185032.jpeg?').resize((600, 400))\ninput_image\n\n\n\n\n\nimage = preprocess(input_image).unsqueeze(0)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    \nprint('image.shape:', image.shape, 'image_features.shape:', image_features.shape)\n\nimage.shape: torch.Size([1, 3, 224, 224]) image_features.shape: torch.Size([1, 512])"
  },
  {
    "objectID": "clip.html#text---features",
    "href": "clip.html#text---features",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Text -> Features",
    "text": "Text -> Features\nFor text inputs, we first tokenize them (something you’ll learn more about in lesson 9) and then encode them with the encode_text method. Note that the resulting embeddings have the same number of dimensions as the image embedding above (512 in this case):\n\ntext = open_clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nprint('Tokenized text shape:', text.shape)\nprint('text_features.shape', text_features.shape)\n\nTokenized text shape: torch.Size([3, 77])\ntext_features.shape torch.Size([3, 512])"
  },
  {
    "objectID": "clip.html#computing-similarity",
    "href": "clip.html#computing-similarity",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Computing Similarity",
    "text": "Computing Similarity\nOnce we have representations of both images and text in the same space, we can compare them! Here’s how we might compare the three texts above with the seal image:\n\n# Computing similarities\ndef probability_scores(image_features, text_features):\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    text_probs = (image_features @ text_features.T).softmax(dim=-1)\n    return text_probs\n\ntorch.set_printoptions(precision=3)\nprint(\"Label probs:\", probability_scores(image_features, text_features))  # Apparently the closest guess is 'a dog' - do you agree?\n\nLabel probs: tensor([[0.330, 0.342, 0.328]])\n\n\nApparently the seal looks like a dog (the middle probability is higher). What is going on here?\nThis operation is very similar to the Cosine Similarity we saw in lesson 4. Specifically, the following two calulations are identical:\n\n(image_features/image_features.norm(dim=-1, keepdim=True))@(text_features/text_features.norm(dim=-1, keepdim=True)).T\n\ntensor([[0.143, 0.180, 0.138]])\n\n\n\ntorch.nn.functional.cosine_similarity(image_features, text_features)\n\ntensor([0.143, 0.180, 0.138])\n\n\nSo we’re calculating the cosine similarity between the image and each text, giving the values above. The final step in the classification demo is to run this through a softmax operation which will scale the values so that they sum to 1, allowing us to interpret them as probabilities:\n\ntorch.nn.functional.cosine_similarity(image_features, text_features).softmax(dim=-1)\n\ntensor([0.330, 0.342, 0.328])"
  },
  {
    "objectID": "clip.html#image-search",
    "href": "clip.html#image-search",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Image Search",
    "text": "Image Search\nWe can take advantage of this similarity measure for image search too! If you want to see an impressive large-scale version in action, check out the CLIP retrieval tool which lets you search LAION. For our demo let’s just search the images folder of this course:\n\n# Finding image files\nimport glob\nimage_files = glob.glob('images/*.png') + glob.glob('images/*.jpeg')\nlen(image_files)\n\n12\n\n\n\n# Claculating their CLIP embeddings\nimage_features = []\nfor fn in image_files:\n    im = load_image_pil(fn)\n    image_features.append(model.encode_image(preprocess(im).unsqueeze(0)))\nimage_features = torch.cat(image_features)\nimage_features.shape\n\ntorch.Size([12, 512])\n\n\n\n# Embedding a query text and finding the image with the highest similarity\ntext = open_clip.tokenize([\"a frog\"])\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\nsimilarities = torch.nn.functional.cosine_similarity(text_features, image_features)\nprint('Similarities:', similarities.detach().numpy())\nprint('Argmax:', similarities.argmax()) # TODO explain\nprint('Best image match:')\nload_image_pil(image_files[similarities.argmax()]).resize((128, 128))\n\nSimilarities: [0.12317517 0.07264645 0.07863379 0.11897033 0.0889878  0.07535253\n 0.07106286 0.11585622 0.12133323 0.12665415 0.27785698 0.13541897]\nArgmax: tensor(10)\nBest image match:\n\n\n\n\n\nPretty nifty! Now, what if instead of searching images we want to GENERATE them?"
  },
  {
    "objectID": "clip.html#using-clip-as-a-loss-function",
    "href": "clip.html#using-clip-as-a-loss-function",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Using CLIP as a loss function",
    "text": "Using CLIP as a loss function\nLet’s try optimizing the pixels of an image to mazimise the similarity of the image with a text prompt:\n\ndevice = 'cuda'\nmodel.to(device)\n\ntext = open_clip.tokenize([\"a picture of a frog\"]).to(device)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nim = torch.rand(1, 3, 224, 224).to(device)\nstart_im = im.clone() # Keep a copy of the initial noise image\nim.requires_grad = True\n\nnormalize = torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n\nopt = torch.optim.Adam([im], lr=1e-2)\nlosses = []\n\nfor i in tqdm(range(250)):\n    image_features = model.encode_image(normalize(im))\n    sim = torch.nn.functional.cosine_similarity(text_features, image_features)\n    loss = 1-sim # So lower is better\n    loss.backward()\n    losses.append(loss.item())\n    opt.step()\n    opt.zero_grad()\n    \ntensor_to_pil(im)\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\ntensor_to_pil(torch.abs(start_im - im)*2) # Viewing the difference between the start image and the final result\n\n\n\n\nWoah - what’s going on here?\n\nSome values in our image are > 1, which is not ideal\nThe CLIP model we’re using here is absed on a Vision Trnasformer, which works with image patches. You can see the patches in the difference image above - since each patch is seeing the same pixels all the time, those pixels can go crazy getting just the right output for that patch.\nThat learning rate might be a little high - you can try it lower or play with optmiizing for more iterations.\n\nSo, how do we fix all of this? Here’s a few improvements we can make:\nImprovements: - Changing our loss to something called the Great Circle Distance Squared - Applying some transforms to the image before feeding it to CLIP, such that the model sees a slightly different version of the image each time - Forcing the values to lie in the expected range for an image with im.clip(0, 1).\n\ntext = open_clip.tokenize([\"a picture of a frog\"]).to(device)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nim = torch.rand(1, 3, 300, 300).to(device) # Larger image\nstart_im = im.clone() # Keep a copy of the initial noise image\nim.requires_grad = True\n\n# Our new and improved loss function\ndef clip_loss(image_features, text_features):\n    input_normed = torch.nn.functional.normalize(image_features.unsqueeze(1), dim=2)\n    embed_normed =torch.nn.functional.normalize(text_features.unsqueeze(0), dim=2)\n    dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2) # Squared Great Circle Distance\n    return dists.mean()\n\ntfms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop(224), # Random CROP each time\n    torchvision.transforms.RandomAffine(5),\n    # torchvision.transforms.ColorJitter(), # You can experiment with different ones here\n    # torchvision.transforms.GaussianBlur(5), \n    torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n])\n\nopt = torch.optim.Adam([im], lr=5e-3)\nlosses = []\n\nfor i in tqdm(range(350)):\n    image_features = model.encode_image(tfms(im))\n    loss = clip_loss(image_features, text_features)\n    loss.backward()\n    losses.append(loss.item())\n    opt.step()\n    opt.zero_grad()\n    \ntensor_to_pil(im)\n\n\n\n\n\n\n\nBetter but still not amazing! TODO talk about more possible improvementsfrom tglcourse.generation_utils import SirenGenerator, CLIPLossToTargets, optimise\n\nfrom tglcourse.generation_utils import ImStackGenerator, CLIPLossToTargets, optimise\n\n# Parameters of the imstack tweaked\ngen = ImStackGenerator(size=300,base_size=8,n_layers=3,scale=3,layer_decay = 0.3).to(device)\n\n# A custom optimiser\nopt = torch.optim.AdamW(gen.parameters(), lr=0.05, \n                       weight_decay=1e-4) # Weight decay for less extreme values\n\n# Loss function\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A watercolor painting of a frog on a lillypad'],\n                                 n_cuts=64) # More cuts for smoother loss signal\n\n# Tweak number of steps and use our custom optimiser\noptimise(gen, loss_functions=[clip_loss_fn],\n         optimizer=opt, n_steps=60)\n\n\n\n\n\n\n\n\n\n\n\n\nTODO video looking at Remi’s multi-perceptor notebook.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Generative models are cool [no citation needed].\nIt is important that this cool new tech be accessible. Part of that is open models and code, but that isn’t much use if no-one but a select few can understand how they work. The goal of this course is to get you up to speed with the latest advancements in generative modelling, to the point where you can understand and contribute to the work being done in this area."
  },
  {
    "objectID": "getting_started.html#how-to-navigate-this-course",
    "href": "getting_started.html#how-to-navigate-this-course",
    "title": "Getting Started",
    "section": "How To Navigate This Course",
    "text": "How To Navigate This Course\n\n\n\n\n\nThis course is split up into four main components:  - ‘Lessons’ has the core material that makes up the bulk of the course content. You can work through all of these in order, or skip through to topics that interest you.  - ‘Bonus Material’ covers additional topics that compliment the main lessons. Setting up cloud machines, or tracking experiments - things that will make your experience working with deep learning richer and more productive. This also includes some important extra topics such as ethics in generative modelling. - ‘Projects’ outlines some recommended exercises for you to do as you proceed through the course material. These will also be used to assess participation if we decide to figure out some sort of certification (TBD).  - ‘Discussions’ features podcast-style interviews with interesting people working in this field, from artists to deep learning researchers. \nThis whole course is written as notebooks, so you can open each lesson in colab using [TODO open in colab button] to start running the code, or see [TODO make setting up in paperspace] to see how you can run the course in the cloud.\nThe recommended way to work through the course is to start at Lesson 1. If you already have some PyTorch skills you’ll be able to skim the first few lessons quite rapidly, but I still recommend taking a look and trying the exercises at the end of each notebook. Specific bonus materials and projects will get referenced from within the lessons once you get to the stage where they are relevant. Of course, you are also welcome to ignore this route and skip around wherever your interest takes you."
  },
  {
    "objectID": "getting_started.html#the-library",
    "href": "getting_started.html#the-library",
    "title": "Getting Started",
    "section": "The Library",
    "text": "The Library\nAs we progress through the lessons we’ll begin to build a library of useful functions and classes that will come in handy for various generative modelling tasks. Thanks to the magic of NBDEV these are automatically exported into the tglcourse library, which you can use in your own projects. An overview is available here.\nInstalling is as simple as pip install tglcourse. This is also the easiest way to get all of the requirements set up if you’re trying to run the lesson notebooks locally."
  },
  {
    "objectID": "getting_started.html#live-lessons-discussion-groups",
    "href": "getting_started.html#live-lessons-discussion-groups",
    "title": "Getting Started",
    "section": "Live Lessons + Discussion Groups",
    "text": "Live Lessons + Discussion Groups\nThe course is designed to be explored at your own pace, but we will be running a sort of ‘study group’ over on Discord with weekly discussions around some of the lessons. Final details are still TBC, but I’ll probably do one or more live run-throughs of the lesson notebooks and then make those recordings available alongside the lesson notebooks for those who prefer a longer-form video to keep them company as they explore the code."
  },
  {
    "objectID": "getting_started.html#contributing",
    "href": "getting_started.html#contributing",
    "title": "Getting Started",
    "section": "Contributing",
    "text": "Contributing\nThis entire course/library is built using NBDev. Every page is a notebook, and the docs, library code and webpage are all created from these source notebooks. The hope is that these will stay up-to-date with continual improvements and bug-fixes added.\nIf you spot a bug or have a request for an improvement, you can open an issue on GitHub to let me know, or send a message via Discord.\nI’d also love to hear about any guests you think would be good to have on for the discussion series, or any extra topics you’d like to see covered.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "finetuning_pretrained_models.html",
    "href": "finetuning_pretrained_models.html",
    "title": "Fine-Tuning Pretrained Networks for Image Classification",
    "section": "",
    "text": "import torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "finetuning_pretrained_models.html#replicating-some-of-this-in-pytorch",
    "href": "finetuning_pretrained_models.html#replicating-some-of-this-in-pytorch",
    "title": "Fine-Tuning Pretrained Networks for Image Classification",
    "section": "Replicating (some of) this in PyTorch",
    "text": "Replicating (some of) this in PyTorch\nWhat would this look like in raw PyTorch? Let’s clear out some GPU memory and then try to replicate the result above:\n\nimport gc\ndel learn\ngc.collect()\ntorch.cuda.empty_cache()\n\nThe first component is the dataloaders. In addition to loading the images, resizing and normalizing the data, reading in the labels and so on, the dataloaders also do something called ‘data augmentation’, where each image is randomly transformed using a user-specified list of possible transforms (we use the pre-defined aug_transforms above). This means that every time we get a batch of data we’ll see the model inputs (images that have been scaled and transformed with some augmentation) and the labels:\n\nxb, yb = dls.one_batch()\nxb.shape, yb\n\n(torch.Size([64, 3, 224, 224]),\n TensorCategory([15,  1, 23, 20,  7,  7, 26, 30, 27, 22, 27,  2, 15,  0,  2, 31,\n                 24, 10,  2,  4, 33, 22, 28,  0, 27, 16, 34, 19,  7, 16,  6, 18,\n                  2,  3, 27, 26, 30, 28, 13, 18, 28, 16, 31, 19, 21, 19, 14,  3,\n                 13, 15, 27,  2,  2, 24,  0, 27, 32, 13, 34, 19, 23,  5, 25, 10],\n                device='cuda:0'))\n\n\nIf you want to see the string versions of the labels, you can look them up in dls.vocab:\n\nlen(dls.vocab), dls.vocab\n\n(37,\n ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier'])\n\n\nWe’ll just use the dataloaders here rather than re-creating them ourselves. The more intersting bit is: how do we turn a model trained on 1000 imagenet classes into one capable of classifying 37 dog breeds?\nFirst, we load the pretrained model:\n\nmodel = resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n\nThen we replace the final fully connected layer (model.fc, although the naming will vary depending on your choice of model/architecture) with our own new layer. This gives us a modified version of the model with 37 outputs:\n\nmodel.fc = nn.Linear(512, 37)\nmodel.to(xb.device)\nwith torch.no_grad():\n    preds = model(xb)\npreds.shape\n\ntorch.Size([64, 37])\n\n\n\n# model # Uncomment to view the full model\n\nWe can train this with cross entropy loss (see Lesson 3):\n\nloss_fn = CrossEntropyLossFlat()\nloss_fn(preds, yb)\n\nTensorBase(3.8790, device='cuda:0')\n\n\nImportantly, the final layer we added has been initialized with random parameters, so we should probably train those a bit first, without modifying the rest of the model too drastically. After all, it has already learnt lots of useful features and we wouldn’t want to mess it up while this random final layer was not making good use of them. To do this, we freeze all the rest of the model parameters (setting requires_grad to False) and train for one epoch:\n\n[p.shape for p in model.fc.parameters()]\n\n[torch.Size([37, 512]), torch.Size([37])]\n\n\n\nlosses = []\n\n# Freeze all layers (requires_grad=False)\nfor p in model.parameters():\n    p.requires_grad = False\n\n# Unfreeze final layer\nfor p in model.fc.parameters():\n    p.requires_grad= True\n\n# Optimizer on just these unfrozen parameters\nopt = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n\n# Train for one epoch\nfor xb, yb in dls.train:\n    preds = model(xb)\n    loss = loss_fn(preds, yb)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\nPlotting the losses:\n\nplt.plot(losses)\n\n\n\n\nNow we can unfreeze the rest of the parameters and train a bit longer, using a lower learning rate:\n\n# Unfreeze the rest of the model parameters\nfor p in model.parameters():\n    p.requires_grad = True\n\n# New optimizer and train a bit more at a lower learning rate\nopt = torch.optim.Adam(model.parameters(), lr=1e-4)\nfor xb, yb in dls.train:\n    preds = model(xb)\n    loss = loss_fn(preds, yb)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\nPlotting the losses, showing this second epoch in yellow:\nYou could train for multiple epochs, explore using some sort of learning rate schedule and otherwise tweak this training procedure to try to improve performance further. But before you do, let’s see how we might calculate accuracy on the validation set:\nNot bad! It is pretty convenient to have all of these features (automatic evaluation of metrics, data augmentation, learning rate schedules, tracking model freezing/unfreezing…) built into fastai, but they aren’t magic - we can totally do these ourselves too!\nTODO script doing this all from scratch in PyTorch\nTODO talk about how having logging is nicer than waiting for a training loop to finish before you can see stats\nTODO Add a bit more explanation"
  },
  {
    "objectID": "gan1.html",
    "href": "gan1.html",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "",
    "text": "# TODO video intro"
  },
  {
    "objectID": "gan1.html#gans---theory",
    "href": "gan1.html#gans---theory",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "GANs - Theory",
    "text": "GANs - Theory\nThe motivation for GANs was the desire for a generative model that could generate plausible new images (or any kind of data for that matter) that look like they could have come from some domain or training set. In the previous lesson we saw how Variational Auto-Encoders learn how to encode an input into some latent representation and then decode it back into an output, and how the decoder part of an auto-encoder can be used to generate new images.\nGANs achieve a similar result but tend to peform far better than auto-encoder based systems at generating new, novel outputs.\nThey do this by training two separate sub-networks, a generator network and a discriminator network. They are trained in parallel. The goal of the discriminator is to tell whether a given image is real (aka from the training data) or fake (aka generated by the generator model). The generator tries to create plausible outputs to fool the discriminator.\n\n# TODO images\n\nA typical GAN training loop might look something like the following:\nfor batch in data:\n\ngenerate a batch of fake images\nfeed both the fake images and the batch of training images into the discriminator\ncalculate the discriminator loss and update the discriminator weights to improve it’s accuracy\nupdate the weights of the generator to better fool the discriminator (i.e. increase the discriminator loss)\n\nThis core idea is very simple, and yet GAN training can fail in some unexpected ways. For example, a GAN that perfectly memorizes the training data can fool the discriminator perfectly, but might not be able to generate any new data. Even worse: it can get away with memorizing only a subset of the training data, a situation called ‘mode collapse’.\nBecause of these quirks, GAN training is viewed by many as some sort of dark art! But in this lesson we’re going to face it bravely, exploring a basic GAN implementation and seeing for ourselves what training looks like. In the next lesson, we’ll introduce some additional ideas and a few of the tricks people use to get better results with GANs. And in the bonus notebook [coming some time] we’ll look at an idea called NO-GAN.\n“Wait, I just want to train a good GAN right now!” you say? Fair enough - in that case skip this simple demo and jump straight to something like LightWeightGAN which has ready-to-use training scripts, or look around for one of the many StyleGAN tutorials."
  },
  {
    "objectID": "gan1.html#dc-gan-from-scratch",
    "href": "gan1.html#dc-gan-from-scratch",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "DC-GAN From Scratch",
    "text": "DC-GAN From Scratch\nMuch of the code here comes from the PyTorch docs [TODO link]\n\nxb = next(iter(train_dataloader))['images'].to(device)[:8]\n\n\ndataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n\nimage_size = 32\nbatch_size = 32\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((image_size, image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\ndataset.set_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
  },
  {
    "objectID": "gan1.html#a-few-improvements",
    "href": "gan1.html#a-few-improvements",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "A few improvements",
    "text": "A few improvements\n\nAdd logging to training loop\nAdd a few things like pixelshuffle/blur\nLeave space to tweak loss functions and such\nExport a bunch of components for next notebook"
  },
  {
    "objectID": "gan1.html#playing-with-an-existing-gan",
    "href": "gan1.html#playing-with-an-existing-gan",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "Playing with an existing GAN",
    "text": "Playing with an existing GAN\n\n# Load a lightweight-GAN? Or a model trained with the above code (wrapped in a script) for a little longer?\n\n\n# latent walk\n\n\n# Guide with a loss (demo one from generators and losses notebook\n\n\n# Link to butterflyGAN demo on HF spaces\n\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "representations.html#what-do-networks-learn",
    "href": "representations.html#what-do-networks-learn",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "What Do Networks Learn",
    "text": "What Do Networks Learn\n\n\n\n\n\nIn this lesson we’re going to look at some uses of large pretrained neural networks, and try to understand why they work as well as they do. To keep things organised, the content has been split between this notebook and two bonus notebooks.\n\nExtracting Represenations from Pretrained Models\nVGG16 was a fairly popular CNN that improved upon the famous AlexNet architecture. It has since been supeceeded by more complex networks but it still works great for our purposes. The architecture is something like the following (source):\n\nWe can load it like so (check the imports section at the start of this notebook for the relevant imports):\n\nvgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n\n\n# vgg # Uncomment to show the sumary\n\n\n# Random 'images'\nx = torch.randn(8, 3, 128, 128) \n\n# Normalize with the same stats used during model training\ndef normalize(x):\n    mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n    std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n    x = (x-mean) / std\n    return x\nx = normalize(x)\n\n# Pass through the model\nvgg(x).shape\n\ntorch.Size([8, 1000])\n\n\n\ndef get_penultimate_features(x):\n    x = vgg.features(x)\n    x = vgg.avgpool(x)\n    x = torch.flatten(x, 1)\n    for l in vgg.classifier[:-3]:\n        x = l(x)\n    return x\n\nfeatures = get_penultimate_features(x)\nfeatures.shape\n\ntorch.Size([8, 4096])\n\n\nFor each image we now have a feature vector containing 4096 values.\nWhat can we do with these? Let’s use them for image search!\n\nfrog_image = pil_from_url(\"https://images.pexels.com/photos/70083/frog-macro-amphibian-green-70083.jpeg?auto=compress&cs=tinysrgb&w=1600\", size=(128, 128))\nfrog_image\n\n\n\n\n\n# Get features from the frog image we're searching with\nfrog_features = get_penultimate_features(normalize(pil_to_tensor(frog_image)))\n\n# Get featurs for all images in the images/ folder\nimage_files = glob.glob('images/*')\npil_images = [load_image_pil(im, size=(128, 128)) for im in image_files]\nimages = [pil_to_tensor(pil_image) for pil_image in pil_images]\nx = torch.cat(images, dim=0)\nimage_features = get_penultimate_features(normalize(x))\n\n# Calculate the similarities\ncosine_sim =  torch.nn.CosineSimilarity()\nsimilarities = cosine_sim(frog_features, image_features)\nprint('Similarities:', similarities)\nbest_match_idx = torch.argmax(similarities).item()\nprint('Best match:', image_files[best_match_idx])\npil_images[best_match_idx]\n\nSimilarities: tensor([0.3742, 0.4047, 0.3873, 0.6293, 0.5818, 0.5095, 0.3890, 0.5207, 0.4447,\n        0.5527, 0.6004, 0.5090, 0.6598], grad_fn=<SumBackward1>)\nBest match: images/frog.png\n\n\n\n\n\nSome variant of this is used for most reverse-image-search implementations, and can be an extremely useful way to quickly ‘search’ a database of images, especially if you pre-compute the features for each image as it is added. Choosing how the network used for feature extraction is trained can determine what kinds of features are used - for example, a face recognition network will have features useful for finding similar faces, and can be used to find pictures of a specific person in a dataset."
  },
  {
    "objectID": "representations.html#transfer-learning",
    "href": "representations.html#transfer-learning",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\n\n\nAs we said in the introduction, these learned ‘features’ capture a lot of useful information. When training an image classification network (for example) it is often beneficial to begin with a pretrained network rather than starting from scratch for this reason. Look at the bonus notebook to see an example of this in action, implemented in two different ways.\nClassifying Imagenet images is a pretty good pretraining task, and models trained on Imagenet tend to transfer well to other tasks. But this is not the only game in town! For specific applications, finding a pre-training task with lots of relevant data can help provide a model that will be a better starting point for downstream tasks. One example: training a model on satellite imagery on the task of predicting night-time light emmisions results in a model that has learn features associated with buildings and economic activity, which in turn can be fine-tuned on the task of estimating financial wellbeing from satellite imagery, as in this great paper from 2016. More generally, something like CLIP (which we’ll see in lesson 5) tends to learn extremely useful representations as a result of beig trained to associate images with captions across a large dataset."
  },
  {
    "objectID": "representations.html#style-transfer",
    "href": "representations.html#style-transfer",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "Style Transfer",
    "text": "Style Transfer\nRemember how we said that early layers in a network tend to learn simpler features like colour and texture, while later layers capture more complex shapes? We’re going to take advantage of this to do a little bit of magic. We’ll take two images, a content image and a style image. Then we’ll feed both through a network and record the activations at various layers. Finally, we will try to optimize the pixels of a new image (or anything that produces an image) such that: \n\nThe content features (from later layers of the network) roughly match those from the content image \nThe types of style features (from early layers) are similar to those from the style image \n\nComparing the content features is relatively simple - something like MSE or MAE will suffice. But for the style features, we don’t want features in the same places, we only care that the same types of feature appear. The bonus notebook has three different takes on ways to measure this, and I hope to add more explanation in a dedicated section soon.\n\nstyle_image = pil_from_url(\"https://i.pinimg.com/originals/c3/b4/38/c3b438401bab3e91b487cd30309224f7.gif\", size=(512, 512)).convert('RGB')\nstyle_image.resize((128, 128)) # Small for preview\n\n\n\n\n\ncontent_image = load_image_pil('images/frog.png', size=(512, 512))\ncontent_image.resize((128, 128)) # Small for preview\n\n\n\n\n\nfrom tglcourse.generation_utils import PixelGenerator, VincentStyleLossToTarget, ContentLossToTarget, optimise\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nstyle_loss_fn = VincentStyleLossToTarget(pil_to_tensor(style_image).to(device), size=512, style_layers = [3, 6, 9, 11])\ncontent_loss_fn = ContentLossToTarget(pil_to_tensor(content_image).to(device))\n\n# The pixels we'll optimise\ngen = PixelGenerator(512, init_image=content_image).to(device)\n\n# The optimizer - feel free to try different ones here\nopt = torch.optim.AdamW(gen.parameters(), lr=0.05, weight_decay=1e-6)\n\noptimise(gen, loss_functions=[style_loss_fn, content_loss_fn], optimizer=opt, n_steps=60)\n\n\n\n\n\n\n\n\n\n\n\n\ntensor_to_pil(gen())\n\n\n\n\nPretty neat hey! More background in the lesson live-stream and the bonus notebook ‘Fun with Generators and Losses’."
  },
  {
    "objectID": "representations.html#a-dose-of-ethics-i-like-your-style",
    "href": "representations.html#a-dose-of-ethics-i-like-your-style",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "A Dose of Ethics: I Like Your Style",
    "text": "A Dose of Ethics: I Like Your Style\nStyle transfer is extremely fun. It is tempting to grab some random pics from Google Image Search and start playing, but please pause for a second and imagine yourself in the shoes of the artist or photographer whose work you’re about to download and modify. Perhaps you’d be happy to see your work used creatively, perhaps you’d be outraged. The image might have a licence that specifically restricts re-use or modification, and by using it you might be ‘ripping off’ a characteristic composition or style from someone who’d prefer you didn’t! Better to avoid any potential pitfalls and stick with your own images or works that are explicity in the public domain (or at least licenced permissively) - especially if you’re considering making money from the results.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "library.html",
    "href": "library.html",
    "title": "The Library",
    "section": "",
    "text": "This course is written in Jupyter notebooks. If you view the raw notebooks rather than the rendered website, you’ll see lots of extra syntax sprinkled around - thigs like ‘#|hide’ or ‘#|export’ at the top of a code cell. These are NBDev directives. Many are for visual formatting, but some (like ‘#|export’) tell the processing system to export parts of the code into separate files where they form components of the tglcourse library. This means that any functions or classes defined in the notebooks and exported in this way can be used at a later date. In this notebook we’ll demonstrate some of these functions and their potential uses - if you’re curious about each you can always go back to the lesson where it is defined for more background info.\nIf you haven’t already, make sure you install the library and its requirements with pip install tglcourse\nThis is a WIP and needs some love.\n\nfrom tglcourse.utils import *\n\n\nim = pil_from_url('https://johnowhitaker.github.io/tglcourse/index_files/figure-html/cell-2-output-1.png')\nim.resize((128, 128))\n\n\n\n\n\n# TODO generation example (fix device requirements for OTStyleLossToTarget and co)\n\n\n# TODO get list of all the things exported and what notebooks they come from\n\n\nimport glob\nfor f in glob.glob('tglcourse/*.py'):\n    if f.split('/')[1][0]!= '_':\n        with open(f, 'r') as ff:\n            print(f)\n            lines = ff.readlines()[:5] \n            print('From:', lines[0].split('../')[1].strip())\n            print('__all__:', lines[3].split('=')[1].strip(), '\\n')\n\ntglcourse/data_utils.py\nFrom: 61_Datasets.ipynb.\n__all__: ['to_tensor', 'mnist_transform', 'get_mnist_dl'] \n\ntglcourse/generation_utils.py\nFrom: 62_Generators_and_Losses.ipynb.\n__all__: ['PixelGenerator', 'MSELossToTarget', 'ImStackGenerator', 'calc_vgg_features', 'ContentLossToTarget', \n\ntglcourse/lesson12.py\nFrom: 12_DM1.ipynb.\n__all__: ['BasicConvNet', 'PreNormResidual', 'FeedForward', 'MLPMixer', 'BasicUNet', 'NoiseConditionedUNet'] \n\ntglcourse/utils.py\nFrom: 01_PyTorch_Basics.ipynb.\n__all__: ['load_image_pil', 'pil_from_url', 'pil_to_tensor', 'tensor_to_pil']"
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Creating Scripts",
    "section": "",
    "text": "Notebooks are great for exploration. And with NBDev they’re pretty amazing for development work too! But sometimes you just want a nice command-line script so that you can run your code with some thing like:\npython my_script.py --input cat.png --width 128 \nIn this quick notebook I’ll show you my new favourite way of doing this, using fastcore.script. The example from their docs shows how simple this can be. We write the following to a file:\n\nfrom fastcore.script import *\n@call_parse\ndef main(msg:str,     # The message\n         upper:bool): # Convert to uppercase?\n    \"Print `msg`, optionally converting to uppercase\"\n    print(msg.upper() if upper else msg)\n\nOverwriting scripts/example_script.py\n\n\nNow we can run this like so:\n\n!python scripts/example_script.py --upper 'Hello World'\n\nHELLO WORLD\n\n\nAnd those type hints and comments in the function definition above? They become part of the help text:\n\n!python scripts/example_script.py -h\n\nusage: example_script.py [-h] [--upper] msg\n\nPrint `msg`, optionally converting to uppercase\n\npositional arguments:\n  msg         The message\n\noptions:\n  -h, --help  show this help message and exit\n  --upper     Convert to uppercase? (default: False)\n\n\nPretty nifty right!\nCheck out the scripts/ directory for a bunch of examples which I made using this technique to go along with the course."
  },
  {
    "objectID": "bonus_material_intro.html",
    "href": "bonus_material_intro.html",
    "title": "Bonus Material",
    "section": "",
    "text": "This section contains all the useful bits that didn’t quite fit in the core course curriculum. It’s a little sparse right now, but post launch there are a host of extra topics planned :)"
  },
  {
    "objectID": "sequence_3.html",
    "href": "sequence_3.html",
    "title": "Lesson 11: Everything Is A Sequence",
    "section": "",
    "text": "Page stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm2.html",
    "href": "dm2.html",
    "title": "Lesson 13: Going Deeper with Diffusion Models",
    "section": "",
    "text": "Load a diffusers pipeline and generate an image\nSampling loop with a new sampler (plus DDIM explanation?)\nFine-tune on a new dataset\nGuide with a loss or two (start simple, then CLIP)\nCondition on a class\nClassifier-free guidance\nLatent Diffusion Model\nProgressive Distillation\n\n\nfrom diffusers import DDPMPipeline\n\n\nimage_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\");\nimage_pipe.to(\"cuda\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimages = image_pipe().images # Forward method does all the magic (slowly)\nprint(len(images))\nimages[0]\n\n\n\n\n1\n\n\n\n\n\n\nimage_pipe.scheduler\n\nDDPMScheduler {\n  \"_class_name\": \"DDPMScheduler\",\n  \"_diffusers_version\": \"0.4.2\",\n  \"beta_end\": 0.02,\n  \"beta_schedule\": \"linear\",\n  \"beta_start\": 0.0001,\n  \"clip_sample\": true,\n  \"num_train_timesteps\": 1000,\n  \"trained_betas\": null,\n  \"variance_type\": \"fixed_small\"\n}\n\n\n\nimage_pipe.unet.config\n\nFrozenDict([('sample_size', 256),\n            ('in_channels', 3),\n            ('out_channels', 3),\n            ('center_input_sample', False),\n            ('time_embedding_type', 'positional'),\n            ('freq_shift', 1),\n            ('flip_sin_to_cos', False),\n            ('down_block_types',\n             ['DownBlock2D',\n              'DownBlock2D',\n              'DownBlock2D',\n              'DownBlock2D',\n              'AttnDownBlock2D',\n              'DownBlock2D']),\n            ('up_block_types',\n             ['UpBlock2D',\n              'AttnUpBlock2D',\n              'UpBlock2D',\n              'UpBlock2D',\n              'UpBlock2D',\n              'UpBlock2D']),\n            ('block_out_channels', [128, 128, 256, 256, 512, 512]),\n            ('layers_per_block', 2),\n            ('mid_block_scale_factor', 1),\n            ('downsample_padding', 0),\n            ('act_fn', 'silu'),\n            ('attention_head_dim', None),\n            ('norm_num_groups', 32),\n            ('norm_eps', 1e-06),\n            ('_class_name', 'UNet2DModel'),\n            ('_diffusers_version', '0.4.2'),\n            ('_name_or_path',\n             '/root/.cache/huggingface/diffusers/models--google--ddpm-celebahq-256/snapshots/cd5c944777ea2668051904ead6cc120739b86c4d')])\n\n\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\ndataset[0]['image']\n\nUsing custom data configuration huggan--smithsonian_butterflies_subset-7665b1021a37404c\nReusing dataset parquet (/root/.cache/huggingface/datasets/huggan___parquet/huggan--smithsonian_butterflies_subset-7665b1021a37404c/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n\n\n\n\n\n\nimport torch\nfrom torchvision import transforms\n\nimage_size = 256\nbatch_size = 4\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((image_size, image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\ndataset.set_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nParameter 'transform'=<function transform> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n\n# Training loop\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\ndevice = 'cuda'\noptimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=2e-5)\n\nlosses = []\n\nfor epoch in range(5):\n    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n        clean_images = batch['images'].to(device)\n        # Sample noise to add to the images\n        noise = torch.randn(clean_images.shape).to(clean_images.device)\n        bs = clean_images.shape[0]\n\n        # Sample a random timestep for each image\n        timesteps = torch.randint(0, image_pipe.scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n\n        # Add noise to the clean images according to the noise magnitude at each timestep\n        # (this is the forward diffusion process)\n        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n        \n        # Get the model prediction for the noise\n        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n        \n        # Compare the prediction with the actual noise:\n        loss = F.mse_loss(noise_pred, noise) # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)\n        \n        # Store for later plotting\n        losses.append(loss.item())\n\n        # Update the model parameters with the optimizer based on this loss\n        loss.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n    print(epoch, sum(losses[-len(train_dataloader):])/len(train_dataloader))\n\n\n\n\n0 0.016021831180201843\n\n\n\n\n\n1 0.012031297225388699\n\n\n\n\n\n2 0.012823557708179579\n\n\n\n\n\n3 0.013202842021593823\n\n\n\n\n\n4 0.011400782576529309\n\n\n\n# Plot the losses\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\n# Generate and view an image\nimages = image_pipe().images # Forward method does all the magic (slowly)\nprint(len(images))\nimages[0]\n\n\n\n\n1\n\n\n\n\n\n\nfrom diffusers import DDIMScheduler\nimport torchvision\n\n# Create new scheduler and set num inference steps\nscheduler = DDIMScheduler.from_config(\"google/ddpm-celebahq-256\")\nscheduler.set_timesteps(num_inference_steps=40)\n\nx = torch.randn(8, 3, 256, 256).to(device) # Batch of 8\n\nfor i, t in tqdm(enumerate(scheduler.timesteps)):\n    model_input = scheduler.scale_model_input(x, t)\n    with torch.no_grad():\n        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n    x = scheduler.step(noise_pred, t, x).prev_sample\n\ngrid = torchvision.utils.make_grid(x, nrow=4)\nplt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5);\n\n\n\n\n\ndef blue_loss(images):\n    # How far are the blue channel values to 0.9:\n    error = torch.abs(images[:,2] - 0.9).mean() # [:,2] -> all images in batch, only the blue channel\n    return error\n\n\nfrom diffusers import DDIMScheduler\nimport torchvision\n\n# Create new scheduler and set num inference steps\nscheduler = DDIMScheduler.from_config(\"google/ddpm-celebahq-256\")\nscheduler.set_timesteps(num_inference_steps=40)\n\nx = torch.randn(1, 3, 256, 256).to(device)\n\nfor i, t in tqdm(enumerate(scheduler.timesteps)):\n\n    model_input = scheduler.scale_model_input(x, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n\n\n    x = x.detach().requires_grad_()\n\n    # Get the predicted x0:\n    x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n\n    # Calculate loss\n    loss = blue_loss(x0) * 5\n    if i%10==0:\n        print(i, 'loss:', loss.item())\n\n    # Get gradient\n    cond_grad = -torch.autograd.grad(loss, x)[0]\n\n    # Modify the latents based on this gradient\n    x = x.detach() + cond_grad # * sigma**2 # TODO should scale by something here for this to be 'correct'\n    \n    # Now step with scheduler\n    x = scheduler.step(noise_pred, t, x).prev_sample\n    \nplt.imshow(x[0].permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5)\n\nThe config attributes {'variance_type': 'fixed_small'} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n\n\n\n\n\n0 loss: 3.9597525596618652\n10 loss: 1.1187255382537842\n20 loss: 0.914357602596283\n30 loss: 0.8674078583717346\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nfrom PIL import Image\nimport numpy as np\nim = grid.permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5\nImage.fromarray(np.array(im*155).astype(np.uint8)).save('butterflies_grid.png')\n\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Each lesson will have some small exercises for you to try out some of the ideas, but I’ve also designed three larger projects for you to show off your skills. At some point TBD I’d like to figure out getting certificates for those who complete these, but have yet to figure out the details. This page will host the project descriptions and info on submitting yours once that’s all figured out."
  },
  {
    "objectID": "projects.html#project-1-the-golden-generation",
    "href": "projects.html#project-1-the-golden-generation",
    "title": "Projects",
    "section": "Project 1: The Golden Generation",
    "text": "Project 1: The Golden Generation\nOnce you’re done with lessons 1-5, check out the bonus notebook ‘Fun with Generators and Losses’ [TODO link]. Your goal is to come up with a custom pipeline for generating images. A user should be able to provide minimal input (perhaps an input image, or a piece of text, or both) and get back something special. You can acheive this in a number of ways: - Pick an existing generator alongside one or more loss functions and set them up with good default settings - Create a new generator following the template in that notebook - Create your own custom loss function - Chain together a sequence of steps where the output of one step feeds into the next.\nPackage the resulting pipeline into a gradio interface (see the bonus notebook ‘TODO Gradio NB’ for a guide) and share it as a huggingface space.\n\n\n\nwaterface demo\n\n\nFor inspiration, check out my ‘waterface’ demo that first processes an input image to look like a sketch then optimizes an imstack of the result towards a CLIP prompt: https://huggingface.co/spaces/johnowhitaker/waterface"
  },
  {
    "objectID": "projects.html#project-2-we-gan-do-it",
    "href": "projects.html#project-2-we-gan-do-it",
    "title": "Projects",
    "section": "Project 2: WE GAN DO IT!",
    "text": "Project 2: WE GAN DO IT!\nTrain a GAN on your own data - synthetic or curated. Try different architectures and training tricks, until you get a result you’re happy with. Document your experiments and results with a nice report. See TODO W&B tutorial for a guide on logging experiments.\nTODO example report, sweeps example"
  },
  {
    "objectID": "projects.html#project-3-looking-finetuned",
    "href": "projects.html#project-3-looking-finetuned",
    "title": "Projects",
    "section": "Project 3: Looking Fine(tuned)",
    "text": "Project 3: Looking Fine(tuned)\nFine-tune a diffusion model of your choice using your own dataset. Share it with the world - set up a custom pipeline, share training details in a report, make the model weights available and maybe set up a cool colab notebook for people to try your new model, complete with fancy features like CLIP guidance or image-to-image."
  },
  {
    "objectID": "projects.html#final-project",
    "href": "projects.html#final-project",
    "title": "Projects",
    "section": "Final Project",
    "text": "Final Project\nMake something amazing! You can adapt one of the projects above or forge your own path off the beated track. Whatever you choose, the goal of this final project is to take it to the next level :)\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "generative_1.html",
    "href": "generative_1.html",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "",
    "text": "noise -> data\nAEs (https://colab.research.google.com/drive/1Rk8cXMdad9ASIVI6avPyp5bl0qmeOu6S#scrollTo=jLKvlxgB7qXc)\nVAEs"
  },
  {
    "objectID": "generative_1.html#auto-encoders-learned-representations-and-vaes",
    "href": "generative_1.html#auto-encoders-learned-representations-and-vaes",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "Auto-Encoders, Learned Representations and VAEs",
    "text": "Auto-Encoders, Learned Representations and VAEs\nWe’ve seen that pre-trained networks can learn representations that are useful for classification. And that we can use some of these learned representations to create a measure of structural or stylistic similarity between images, which we can re-purpose for style transfer.\nHow does this idea of learning useful representations tie into generating new images?\nIn this section we’ll meet something called an autoencoder.\n\n\n\nae\n\n\nThe idea here is to make a neural network with two parts: and encoder and a decoder. The encoder will take an image and process it until, at some point, we reach a ‘bottleneck’ layer where only a handful of numbers are passed to the next layer. We call this small set of numbers h or z* or ‘the latent representation of the image’. The task of the network during training is to learn how to produce a useful representation so that the decoder can re-create the input image with minimal loss.\nBy setting things up in this way, we’re forcing the network to learn enough about the data to create a useful representation AND a decoder network that can create nice outputs despite only seeing this very compressed representation of the input image.\nTake a moment to appreciate this: as with many things in deep learning, we don’t do anything too special ourselves - we just set an objective and let the network learn how to do things!\n*some people are picky about notation, but we’ll be fairly flexible and just try to clarify what something is rather than relying too heavily on one poor letter for meaning.\n\nCreating a Convolutional AutoEncoder\nHere we build on the simple networks we made in lesson 3 to create our network. Skim the code and see if you can understand how the encoder and decoder work. Note that ConvTranspose2d is effectively the reverse of a convolution layer - check the docs for details.\n\nclass ConvAE(nn.Module):\n  def __init__(self, hdim=20):\n    super(ConvAE, self).__init__()\n    # Encoder layers\n    self.enc_conv1 = nn.Conv2d(1, 32, 3)\n    self.enc_conv2 = nn.Conv2d(32, 32, 3)\n    self.enc_fc1 = nn.Linear(32*24*24, 128)\n    self.enc_fc2 = nn.Linear(128, hdim)\n\n    # Decoder layers\n    self.dec_fc1 = nn.Linear(hdim, 128)\n    self.dec_fc2 = nn.Linear(128, 32*24*24)\n    self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=(32, 24, 24))\n    self.dec_conv1 = nn.ConvTranspose2d(32, 32, 3)\n    self.dec_conv2 = nn.ConvTranspose2d(32, 1, 3)\n\n  def encode(self, x):\n    x = self.enc_conv1(x)\n    x = F.relu(x)\n    x = self.enc_conv2(x)\n    x = F.relu(x)\n    x = torch.flatten(x, 1)\n    x = self.enc_fc1(x)\n    x = F.relu(x)\n    x = self.enc_fc2(x)\n    return x\n\n  def decode(self, x):\n    x = self.dec_fc1(x)\n    x = F.relu(x)\n    x = self.dec_fc2(x)\n    x = F.relu(x)\n    x = self.dec_unflatten(x)\n    x = self.dec_conv1(x)\n    x = F.relu(x)\n    x = self.dec_conv2(x)\n    return x\n\n  def forward(self, x):\n    return self.decode(self.encode(x))\n\n\n# mnist = datasets.MNIST('./mnist/',\n#                        train=True,\n#                        transform=transforms.ToTensor(),\n#                        download=False) # TODO use whole dataset\n\n\n# Create a network as defined above and pass an image through it\nae = ConvAE()\nencoded = ae.encode(mnist[0][0].unsqueeze(0))\nprint(encoded.shape)\ndecoded = ae.decode(encoded)\nprint(decoded.shape)\nplt.imshow(decoded.detach().squeeze(), cmap='gray')\n\ntorch.Size([1, 20])\ntorch.Size([1, 1, 28, 28])\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\nTime to train it. I stole most of this code from the NMA content on this topic, which is where I’d suggest you start if you want to go deeper.\n\ndef train_autoencoder(autoencoder, dataset, device, epochs=20, batch_size=250,\n                      seed=0):\n  autoencoder.to(device)\n  optim = torch.optim.Adam(autoencoder.parameters(),\n                           lr=1e-3,\n                           weight_decay=1e-5)\n  loss_fn = nn.MSELoss()\n  g_seed = torch.Generator()\n  loader = DataLoader(dataset,\n                      batch_size=batch_size,\n                      shuffle=True,\n                      pin_memory=True,\n                      num_workers=2,\n                      generator=g_seed)\n\n  mse_loss = torch.zeros(epochs * len(dataset) // batch_size, device=device)\n  i = 0\n  for epoch in tqdm(range(epochs)):\n    for im_batch, _ in loader:\n      im_batch = im_batch.to(device)\n      optim.zero_grad()\n      reconstruction = autoencoder(im_batch)\n      # write the loss calculation\n      loss = loss_fn(reconstruction.view(batch_size, -1),\n                     target=im_batch.view(batch_size, -1))\n      loss.backward()\n      optim.step()\n\n      mse_loss[i] = loss.detach()\n      i += 1\n    \n      break\n    break # TODO remove \n\ntrain_autoencoder(ae, mnist, device)\n\n\n\n\nAfter training, we no longer get the noisy nothingness we saw as the output of the network. Instead, the images look impressively close to the originals! You could try lowering the size of the hidden dimension and seeing how far you can push it.\n\nmnist_val = datasets.MNIST('./mnist/',\n                           train=False,\n                           transform=transforms.ToTensor(),\n                           download=False)\n\n\n# Plot the reconstructed versions of some random samples from the validation set\nfig, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(4):\n  im, label = mnist_val[random.randint(0, len(mnist_val))]\n  axs[0][i].imshow(im.squeeze(), cmap='gray')\n  axs[0][i].set_title(label)\n  axs[1][i].imshow(ae(im.unsqueeze(0).to(device)).squeeze().detach().cpu(), cmap='gray')\n  axs[1][i].set_title(str(label) + ' - reconstructed')\n\n\n\n\nAutoencoders are great, and have many uses (data compression, for example). But they aren’t so great at producing new images that look like the training data. Ideally, we’d pick a random point in the latent space (i.e. a set of random numbers to be the h) and run it through the decoder to get something that looks like a real image. Instead, if we try this we see mostly strange alien shapes:\n\n# Generate new images from random zs\nfig, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n  im = ae.decode(torch.randn(1, 20).to(device))\n  axs[i//4][i%4].imshow(im.detach().cpu().squeeze(), cmap='gray')\n\n\n\n\nThis is because the autoencoder learns to map different image classes to drastically different parts of the latent space - there is no incentive for it to learn a ‘neat’ representation, only that it learns one that let’s it solve the problem as we framed it.\nTo ‘fix’ this, we need to add an additional component to the loss which stops it from keeping all the latent representations wildly separated. The details are technical and you’re brain is probably feeling a little full, so we won’t actually try this ;) Just know that once we add this extra magical bit of code, we get something called a VARIATIONAL auto-encoder (‘VAE’) which enerates new images that look much closer to the training data.\nTODO VAE\nFor a deeper dive into all of this, check out https://deeplearning.neuromatch.io/tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial1.html\n\n# TODO consolidate datasets into bonus notebook that provides imports\n\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm3.html",
    "href": "dm3.html",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "",
    "text": "Stable Diffusion is a powerful text-to-image model. There are various websites and tools to make using it as easy as possible. It is also integrated into the Huggingface diffusers library where generating images can be as simple as:\nIn this notebook we’re going to dig into the code behind these easy-to-use interfaces, to see what is going on under the hood. We’ll begin by re-creating the functionality above as a scary chunk of code, and then one by one we’ll inspect the different components and figure out what they do. By the end of this notebook that same sampling loop should feel like something you can tweak and modify as you like.\nNB: A version of this notebook was used as ‘Lesson 9A’ of the fastai course 2022 part 2, alongside a video run-through which is here: https://youtu.be/844LY0vYQhc"
  },
  {
    "objectID": "dm3.html#setup-imports",
    "href": "dm3.html#setup-imports",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Setup & Imports",
    "text": "Setup & Imports\nYou’ll need to log into huggingface and accept the terms of the licence for this model - see the model card for details.\n\n# !pip install -q --upgrade transformers diffusers ftfy\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\n\nimport torch\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers import logging\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\nfrom tqdm.auto import tqdm\nfrom torch import autocast\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport numpy\nfrom torchvision import transforms as tfms\n\n# For video display:\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n# Supress some unnecessary warnings when loading the CLIPTextModel\nlogging.set_verbosity_error()\n\n# Set device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "dm3.html#loading-the-models",
    "href": "dm3.html#loading-the-models",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Loading the models",
    "text": "Loading the models\nThis code (and that in the next section) comes from the Huggingface example notebook.\nThis will download and set up the relevant models and components we’ll be using. Let’s just run this for now and move on to the next section to check that it all works before diving deeper.\nIf you’ve loaded a pipeline, you can also access these components using pipe.unet, pipe.vae and so on.\n\n# Load the autoencoder model which will be used to decode the latents into image space. \nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n\n# Load the tokenizer and text encoder to tokenize and encode the text. \ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# The UNet model for generating the latents.\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n# To the GPU we go!\nvae = vae.to(torch_device)\ntext_encoder = text_encoder.to(torch_device)\nunet = unet.to(torch_device);"
  },
  {
    "objectID": "dm3.html#a-diffusion-loop",
    "href": "dm3.html#a-diffusion-loop",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "A diffusion loop",
    "text": "A diffusion loop\nIf all you want is to make a picture with some text, you could ignore this notebook and use one of the existing tools (such as DreamStudio) or use the simplified pipeline from huggingface, as documented here.\nWhat we want to do in this notebook is dig a little deeper into how this works, so we’ll start by checking that the example code runs. Again, this is adapted from the HF notebook and looks very similar to what you’ll find if you inspect the __call__() method of the stable diffusion pipeline.\n\n# Some settings\nprompt = [\"A watercolor painting of an otter\"]\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 30            # Number of denoising steps\nguidance_scale = 7.5                # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\n# Prep text \ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler\nscheduler.set_timesteps(num_inference_steps)\n\n# Prep latents\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n\n# Loop\nwith autocast(\"cuda\"):\n    for i, t in tqdm(enumerate(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        # Scale the latents (preconditioning):\n        # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\n# Display\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\npil_images[0]\n\n\n\n\n\n\n\nIt’s working, but that’s quite a bit of code! Let’s look at the components one by one."
  },
  {
    "objectID": "dm3.html#the-autoencoder-ae",
    "href": "dm3.html#the-autoencoder-ae",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "The Autoencoder (AE)",
    "text": "The Autoencoder (AE)\nThe AE can ‘encode’ an image into some sort of latent representation, and decode this back into an image. I’ve wrapped the code for this into a couple of functions here so we can see what this looks like in action:\n\ndef pil_to_latent(input_im):\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n    return 0.18215 * latent.latent_dist.sample()\n\ndef latents_to_pil(latents):\n    # bath of latents -> list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\nWe’ll use a pic from the web here, but you can load your own instead by uploading it and editing the filename in the next cell.\n\n# Download a demo Image\n!curl --output macaw.jpg 'https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 62145  100 62145    0     0  70379      0 --:--:-- --:--:-- --:--:-- 70299\n\n\n\n# Load the image with PIL\ninput_image = Image.open('macaw.jpg').resize((512, 512))\ninput_image\n\n\n\n\nEncoding this into the latent space of the AE with the function defined above looks like this:\n\n# Encode to the latent space\nencoded = pil_to_latent(input_image)\nencoded.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n# Let's visualize the four channels of this latent representation:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(encoded[0][c].cpu())\n\n\n\n\nThis 4x64x64 tensor captures lots of information about the image, hopefully enough that when we feed it through the decoder we get back something very close to our input image:\n\n# Decode this latent representation back into an image\ndecoded = latents_to_pil(encoded)[0]\ndecoded\n\n\n\n\nYou’ll see some small differences if you squint! Forcus on the eye if you can’t see anything obvious. This is pretty impressive - that 4x64x64 latent seems to hold a lot more information that a 64px image…\nThis autoencoder has been trained to squish down an image to a smaller representation and then re-create the image back from this compressed version again.\nIn this particular case, it does so by a factor of ~8 (512 / 8 = 64), so each 8x8 pixel patch gets compressed down to four numbers (the four channels in the AE output). You can find AEs with a higher compression ratio (eg f16 like some popular VQGAN models) but at some point they begin to introduce artifacts that we don’t want.\nWhy do we even use an autoencoder? We can do diffusion in pixel space - where the model gets all the image data as inputs and produces an output prediction of the same shape. But this means processing a LOT of data, and make high-resolution generation very computationally expensive. Some solutions to this involve doing diffusion at low resolution (64px for eg) and then training a separate model to upscale repeatedly (as with D2/Imagen). But latent diffusion instead does the diffusion process in this ‘latent space’, using the compressed representations from our AE rather than raw images. These representations are information rich, and can be small enough to handle manageably on consumer hardware. Once we’ve generated a new ‘image’ as a latent representation, the autoencoder can take those final latent outputs and turn them into actual pixels."
  },
  {
    "objectID": "dm3.html#loop-starting-from-noised-version-of-input-aka-image2image",
    "href": "dm3.html#loop-starting-from-noised-version-of-input-aka-image2image",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Loop starting from noised version of input (AKA image2image)",
    "text": "Loop starting from noised version of input (AKA image2image)\nLet’s see what happens when we use our image as a starting point, adding some noise and then doing the final few denoising steps in the loop with a new prompt.\nWe’ll use a similar loop to the first demo, but we’ll skip the first start_step steps.\nTo noise our image we’ll use code like that shown above, using the scheduler to noise it to a level equivalent to step 10 (start_step).\n\n# Settings (same as before except for the new prompt)\nprompt = [\"A colorful dancer, nat geo photo\"]\nheight = 512                        # default height of Stable Diffusion\nwidth = 512                         # default width of Stable Diffusion\nnum_inference_steps = 50            # Number of denoising steps\nguidance_scale = 8                  # Scale for classifier-free guidance\ngenerator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\nbatch_size = 1\n\n# Prep text (same as before)\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler (setting the number of inference steps)\nscheduler.set_timesteps(num_inference_steps)\n\n# Prep latents (noising appropriately for start_step)\nstart_step = 10\nstart_sigma = scheduler.sigmas[start_step]\nnoise = torch.randn_like(encoded)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\nlatents = latents.to(torch_device).float()\n\n# Loop\nfor i, t in tqdm(enumerate(scheduler.timesteps)):\n    if i > start_step: # << This is the only modification to the loop we do\n        \n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\nlatents_to_pil(latents)[0]\n\n\n\n\n\n\n\nYou can see that some colours and structure from the image are kept, but we now have a new picture! The more noise you add and the more steps you do, the further away it gets from the input image.\nThis is how the popular img2img pipeline works. Again, if this is your end goal there are tools to make this easy!\nBut you can see that under the hood this is the same as the generation loop just skipping the first few steps and starting from a noised image rather than pure noise.\nExplore changing how many steps are skipped and see how this affects the amount the image changes from the input."
  },
  {
    "objectID": "dm3.html#exploring-the-text---embedding-pipeline",
    "href": "dm3.html#exploring-the-text---embedding-pipeline",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Exploring the text -> embedding pipeline",
    "text": "Exploring the text -> embedding pipeline\nWe use a text encoder model to turn our text into a set of ‘embeddings’ which are fed to the diffusion model as conditioning. Let’s follow a piece of text through this process and see how it works.\n\n# Our text prompt\nprompt = 'A picture of a puppy'\n\nWe begin with tokenization:\n\n# Turn the text into a sequnce of tokens:\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input['input_ids'][0] # View the tokens\n\ntensor([49406,   320,  1674,   539,   320,  6829, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n        49407, 49407, 49407, 49407, 49407, 49407, 49407])\n\n\n\n# See the individual tokens\nfor t in text_input['input_ids'][0][:8]: # We'll just look at the first 7 to save you from a wall of '<|endoftext|>'\n    print(t, tokenizer.decoder.get(int(t)))\n\ntensor(49406) <|startoftext|>\ntensor(320) a</w>\ntensor(1674) picture</w>\ntensor(539) of</w>\ntensor(320) a</w>\ntensor(6829) puppy</w>\ntensor(49407) <|endoftext|>\ntensor(49407) <|endoftext|>\n\n\nWe can jump straight to the final (output) embeddings like so:\n\n# Grab the output embeddings\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nprint('Shape:', output_embeddings.shape)\noutput_embeddings\n\nShape: torch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n\n\nWe pass our tokens through the text_encoder and we magically get some numbers we can feed to the model.\nHow are these generated? The tokens are transformed into a set of input embeddings, which are then fed through the transformer model to get the final output embeddings.\nTo get these input embeddings, there are actually two steps - as revealed by inspecting text_encoder.text_model.embeddings:\n\ntext_encoder.text_model.embeddings\n\nCLIPTextEmbeddings(\n  (token_embedding): Embedding(49408, 768)\n  (position_embedding): Embedding(77, 768)\n)\n\n\n\nToken embeddings\nThe token is fed to the token_embedding to transform it into a vector. The function name get_input_embeddings here is misleading since these token embeddings need to be combined with the position embeddings before they are actually used as inputs to the model! Anyway, let’s look at just the token embedding part first\nWe can look at the embedding layer:\n\n# Access the embedding layer\ntoken_emb_layer = text_encoder.text_model.embeddings.token_embedding\ntoken_emb_layer # Vocab size 49408, emb_dim 768\n\nEmbedding(49408, 768)\n\n\nAnd embed a token like so:\n\n# Embed a token - in this case the one for 'puppy'\nembedding = token_emb_layer(torch.tensor(6829, device=torch_device))\nembedding.shape # 768-dim representation\n\ntorch.Size([768])\n\n\nThis single token has been mapped to a 768-dimensional vector - the token embedding.\nWe can do the same with all of the tokens in the prompt to get all the token embeddings:\n\ntoken_embeddings = token_emb_layer(text_input.input_ids.to(torch_device))\nprint(token_embeddings.shape) # batch size 1, 77 tokens, 768 values for each\ntoken_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 0.0011,  0.0032,  0.0003,  ..., -0.0018,  0.0003,  0.0019],\n         [ 0.0013, -0.0011, -0.0126,  ..., -0.0124,  0.0120,  0.0080],\n         [ 0.0235, -0.0118,  0.0110,  ...,  0.0049,  0.0078,  0.0160],\n         ...,\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052],\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052],\n         [ 0.0012,  0.0077, -0.0011,  ..., -0.0015,  0.0009,  0.0052]]],\n       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n\n\n\n\nPositional Embeddings\nPositional embeddings tell the model where in a sequence a token is. Much like the token embedding, this is a set of (optionally learnable) parameters. But now instead of dealing with ~50k tokens we just need one for each position (77 total):\n\npos_emb_layer = text_encoder.text_model.embeddings.position_embedding\npos_emb_layer\n\nEmbedding(77, 768)\n\n\nWe can get the positional embedding for each position:\n\nposition_ids = text_encoder.text_model.embeddings.position_ids[:, :77]\nposition_embeddings = pos_emb_layer(position_ids)\nprint(position_embeddings.shape)\nposition_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 0.0016,  0.0020,  0.0002,  ..., -0.0013,  0.0008,  0.0015],\n         [ 0.0042,  0.0029,  0.0002,  ...,  0.0010,  0.0015, -0.0012],\n         [ 0.0018,  0.0007, -0.0012,  ..., -0.0029, -0.0009,  0.0026],\n         ...,\n         [ 0.0216,  0.0055, -0.0101,  ..., -0.0065, -0.0029,  0.0037],\n         [ 0.0188,  0.0073, -0.0077,  ..., -0.0025, -0.0009,  0.0057],\n         [ 0.0330,  0.0281,  0.0289,  ...,  0.0160,  0.0102, -0.0310]]],\n       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n\n\n\n\nCombining token and position embeddings\nTime to combine the two. How do we do this? Just add them! Other approaches are possible but for this model this is how it is done.\nCombining them in this way gives us the final input embeddings ready to feed through the transformer model:\n\n# And combining them we get the final input embeddings\ninput_embeddings = token_embeddings + position_embeddings\nprint(input_embeddings.shape)\ninput_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[ 2.6770e-03,  5.2133e-03,  4.9323e-04,  ..., -3.1321e-03,\n           1.0659e-03,  3.4316e-03],\n         [ 5.5371e-03,  1.7510e-03, -1.2381e-02,  ..., -1.1410e-02,\n           1.3508e-02,  6.8378e-03],\n         [ 2.5356e-02, -1.1019e-02,  9.7663e-03,  ...,  1.9460e-03,\n           6.8375e-03,  1.8573e-02],\n         ...,\n         [ 2.2781e-02,  1.3262e-02, -1.1241e-02,  ..., -8.0054e-03,\n          -2.0560e-03,  8.9366e-03],\n         [ 2.0026e-02,  1.5015e-02, -8.7638e-03,  ..., -4.0313e-03,\n           1.8487e-05,  1.0885e-02],\n         [ 3.4206e-02,  3.5826e-02,  2.7768e-02,  ...,  1.4465e-02,\n           1.1110e-02, -2.5745e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n\n\nWe can check that these are the same as the result we’d get from text_encoder.text_model.embeddings:\n\n# The following combines all the above steps (but doesn't let us fiddle with them!)\ntext_encoder.text_model.embeddings(text_input.input_ids.to(torch_device))\n\ntensor([[[ 2.6770e-03,  5.2133e-03,  4.9323e-04,  ..., -3.1321e-03,\n           1.0659e-03,  3.4316e-03],\n         [ 5.5371e-03,  1.7510e-03, -1.2381e-02,  ..., -1.1410e-02,\n           1.3508e-02,  6.8378e-03],\n         [ 2.5356e-02, -1.1019e-02,  9.7663e-03,  ...,  1.9460e-03,\n           6.8375e-03,  1.8573e-02],\n         ...,\n         [ 2.2781e-02,  1.3262e-02, -1.1241e-02,  ..., -8.0054e-03,\n          -2.0560e-03,  8.9366e-03],\n         [ 2.0026e-02,  1.5015e-02, -8.7638e-03,  ..., -4.0313e-03,\n           1.8487e-05,  1.0885e-02],\n         [ 3.4206e-02,  3.5826e-02,  2.7768e-02,  ...,  1.4465e-02,\n           1.1110e-02, -2.5745e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\n\nFeeding these through the transformer model\n A set of embeddings pass through some transformer blocks, from the excellent ‘illustrated gpt-2’ blog post\nWe want to mess with theese input embeddings (specifically the token embeddings) before we send them through the rest of the model, but first we should check that we know how to do that. I read the code of the text_encoders forward method, and based on that the code for the forward method of the text_model that the text_encoder wraps. To inspect it yourself, type ??text_encoder.text_model.forward and you’ll get the function info and source code - a useful debugging trick!\nAnyway, based on that we can copy in the bits we need to get the so-called ‘last hidden state’ and thus generate our final embeddings:\n\ninput_embeddings.type()\n\n'torch.cuda.FloatTensor'\n\n\n\ndef get_output_embeds(input_embeddings):\n    # CLIP's text model uses causal mask, so we prepare it here:\n    bsz, seq_len = input_embeddings.shape[:2]\n    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(bsz, seq_len, dtype=input_embeddings.dtype)\n\n    # Getting the output embeddings involves calling the model with passing output_hidden_states=True \n    # so that it doesn't just return the pooled final predictions:\n    encoder_outputs = text_encoder.text_model.encoder(\n        inputs_embeds=input_embeddings,\n        attention_mask=None, # We aren't using an attention mask so that can be None\n        causal_attention_mask=causal_attention_mask.to(torch_device),\n        output_attentions=None,\n        output_hidden_states=True, # We want the output embs not the final output\n        return_dict=None,\n    )\n\n    # We're interested in the output hidden state only\n    output = encoder_outputs[0]\n\n    # There is a final layer norm we need to pass these through\n    output = text_encoder.text_model.final_layer_norm(output)\n\n    # And now they're ready!\n    return output\n\nout_embs_test = get_output_embeds(input_embeddings) # Feed through the model with our new function\nprint(out_embs_test.shape) # Check the output shape\nout_embs_test # Inspect the output\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.0221, -0.0053, -0.0089,  ..., -0.7303, -1.3830, -0.3011],\n         [-0.0062, -0.0246,  0.0065,  ..., -0.7326, -1.3745, -0.2953],\n         [-0.0536,  0.0269,  0.0444,  ..., -0.7159, -1.3634, -0.3075]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n\n\nNote that these match the output_embeddings we saw near the start - we’ve figured out how to split up that one step (“get the text embeddings”) into multiple sub-steps ready for us to modify.\nNow that we have this process in place, we can replace the input embedding of a token with a new one of our choice - which in our final use-case will be something we learn. To demonstrate the concept though, let’s replace the input embedding for ‘puppy’ in the prompt we’ve been playing with with the embedding for token 2368, get a new set of output embeddings based on this, and use these to generate an image to see what we get:\n\nprompt = 'A picture of a puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\n# The new embedding. In this case just the input embedding of token 2368...\nreplacement_token_embedding = text_encoder.get_input_embeddings()(torch.tensor(2368, device=torch_device))\n\n# Insert this into the token embeddings\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)\n\nprint(modified_output_embeddings.shape)\nmodified_output_embeddings\n\ntorch.Size([1, 77, 768])\n\n\ntensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n         [ 0.0290, -1.3258,  0.3085,  ..., -0.5257,  0.9768,  0.6652],\n         [ 0.6942,  0.3538,  1.0991,  ..., -1.5716, -1.2643, -0.0121],\n         ...,\n         [-0.6034, -0.5322,  0.0629,  ..., -0.3964,  0.0877, -0.9558],\n         [-0.5936, -0.5407,  0.0731,  ..., -0.3876,  0.0906, -0.9436],\n         [-0.6393, -0.4703,  0.1103,  ..., -0.3904,  0.1351, -0.9726]]],\n       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n\n\nThe first few are the same, the last aren’t. Everything at and after the position of the token we’re replacing will be affected.\nIf all went well, we should see something other than a puppy when we use these to generate an image. And sure enough, we do!\n\n#Generating an image with these modified embeddings\n\ndef generate_with_embs(text_embeddings):\n    height = 512                        # default height of Stable Diffusion\n    width = 512                         # default width of Stable Diffusion\n    num_inference_steps = 30            # Number of denoising steps\n    guidance_scale = 7.5                # Scale for classifier-free guidance\n    generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n    batch_size = 1\n\n    max_length = text_input.input_ids.shape[-1]\n    uncond_input = tokenizer(\n      [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n    # Prep Scheduler\n    scheduler.set_timesteps(num_inference_steps)\n\n    # Prep latents\n    latents = torch.randn(\n    (batch_size, unet.in_channels, height // 8, width // 8),\n    generator=generator,\n    )\n    latents = latents.to(torch_device)\n    latents = latents * scheduler.init_noise_sigma\n\n    # Loop\n    for i, t in tqdm(enumerate(scheduler.timesteps)):\n        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n        latent_model_input = torch.cat([latents] * 2)\n        sigma = scheduler.sigmas[i]\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        with torch.no_grad():\n            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n        # perform guidance\n        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n        # compute the previous noisy sample x_t -> x_t-1\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    return latents_to_pil(latents)[0]\n\n\ngenerate_with_embs(modified_output_embeddings)\n\n\n\n\n\n\n\nSuprise! Now you know what token 2368 means ;)\n\n\nTextual Inversion\nOK, so we can slip in a modified token embedding, and use this to generate an image. We used the token embedding for ‘cat’ in the above example, but what if instead could ‘learn’ a new token embedding for a specific concept? This is the idea behind ‘Textual Inversion’, in which a few example images are used to create a new token embedding:\n Diagram from the textual inversion blog post - note it doesn’t show the positional embeddings step for simplicity\nWe won’t cover how this training works, but we can try loading one of these new ‘concepts’ from the community-created SD concepts library and see how it fits in with our example above. I’ll use https://huggingface.co/sd-concepts-library/birb-style since it was the first one I made :) Download the learned_embeds.bin file from there and upload the file to wherever this notebook is before running this next cell:\n\nbirb_embed = torch.load('learned_embeds.bin')\nbirb_embed.keys(), birb_embed['<birb-style>'].shape\n\n(dict_keys(['<birb-style>']), torch.Size([768]))\n\n\nWe get a dictionary with a key (the special placeholder I used, ) and the corresponding token embedding. As in the previous example, let’s replace the ‘puppy’ token embedding with this and see what happens:\n\nprompt = 'A mouse in the style of puppy'\n\n# Tokenize\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ninput_ids = text_input.input_ids.to(torch_device)\n\n# Get token embeddings\ntoken_embeddings = token_emb_layer(input_ids)\n\n# The new embedding - our special birb word\nreplacement_token_embedding = birb_embed['<birb-style>'].to(torch_device)\n\n# Insert this into the token embeddings\ntoken_embeddings[0, torch.where(input_ids[0]==6829)] = replacement_token_embedding.to(torch_device)\n\n# Combine with pos embs\ninput_embeddings = token_embeddings + position_embeddings\n\n#  Feed through to get final output embs\nmodified_output_embeddings = get_output_embeds(input_embeddings)\n\n# And generate an image with this:\ngenerate_with_embs(modified_output_embeddings)\n\n\n\n\n\n\n\nThe token for ‘puppy’ was replaced with one that captures a particular style of painting, but it could just as easily represent a specific object or class of objects.\nAgain, there is a nice inference notebook from hf to make it easy to use the different concepts, that properly handles using the names in prompts (“A <cat-toy> in the style of <birb-style>”) without worrying about all this manual stuff. The goal of this notebook is to pull back the curtain a bit so you know what is going on behind the scenes :)"
  },
  {
    "objectID": "dm3.html#messing-with-embeddings",
    "href": "dm3.html#messing-with-embeddings",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Messing with Embeddings",
    "text": "Messing with Embeddings\nBesides just replacing the token embedding of a single word, there are various other tricks we can try. For example, what if we create a ‘chimera’ by averaging the embeddings of two different prompts?\n\n# Embed two prompts\ntext_input1 = tokenizer([\"A mouse\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\ntext_input2 = tokenizer([\"A leopard\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings1 = text_encoder(text_input1.input_ids.to(torch_device))[0]\n    text_embeddings2 = text_encoder(text_input2.input_ids.to(torch_device))[0]\n\n# Mix them together\nmix_factor = 0.35\nmixed_embeddings = (text_embeddings1*mix_factor + \\\n                   text_embeddings2*(1-mix_factor))\n\n# Generate!\ngenerate_with_embs(mixed_embeddings)"
  },
  {
    "objectID": "dm3.html#the-unet-and-cfg",
    "href": "dm3.html#the-unet-and-cfg",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "The UNET and CFG",
    "text": "The UNET and CFG\nNow it’s time we looked at the actual diffusion model. This is typically a Unet that takes in the noisy latents (x) and predicts the noise. We use a conditional model that also takes in the timestep (t) and our text embedding (aka encoder_hidden_states) as conditioning. Feeding all of these into the model looks like this: noise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\nWe can try it out and see what the output looks like:\n\n# Prep Scheduler\nscheduler.set_timesteps(num_inference_steps)\n\n# What is our timestep\nt = scheduler.timesteps[0]\nsigma = scheduler.sigmas[0]\n\n# A noisy latent\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma\n\n# Text embedding\ntext_input = tokenizer(['A macaw'], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n\n# Run this through the unet to predict the noise residual\nwith torch.no_grad():\n    noise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\nlatents.shape, noise_pred.shape # We get preds in the same shape as the input\n\n(torch.Size([1, 4, 64, 64]), torch.Size([1, 4, 64, 64]))\n\n\nGiven a set of noisy latents, the model predicts the noise component. We can remove this noise from the noisy latents to see what the output image looks like (latents_x0 = latents - sigma * noise_pred). And we can add most of the noise back to this predicted output to get the (slightly less noisy hopefully) input for the next diffusion step. To visualize this let’s generate another image, saving both the predicted output (x0) and the next step (xt-1) after every step:\n\nprompt = 'Oil painting of an otter in a top hat'\nheight = 512                      \nwidth = 512                        \nnum_inference_steps = 50\nguidance_scale = 8     \ngenerator = torch.manual_seed(32)\nbatch_size = 1\n\n# Make a folder to store results\n!rm -rf steps/\n!mkdir -p steps/\n\n# Prep text \ntext_input = tokenizer([prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(\n    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n)\nwith torch.no_grad():\n    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n# Prep Scheduler\nscheduler.set_timesteps(num_inference_steps)\n\n# Prep latents\nlatents = torch.randn(\n  (batch_size, unet.in_channels, height // 8, width // 8),\n  generator=generator,\n)\nlatents = latents.to(torch_device)\nlatents = latents * scheduler.init_noise_sigma\n\n# Loop\nfor i, t in tqdm(enumerate(scheduler.timesteps)):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n    sigma = scheduler.sigmas[i]\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n    \n    # Get the predicted x0:\n    # latents_x0 = latents - sigma * noise_pred # Calculating ourselves\n    latents_x0 = scheduler.step(noise_pred, t, latents).pred_original_sample # Using the scheduler (Diffusers 0.4 and above)\n\n    # compute the previous noisy sample x_t -> x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    # To PIL Images\n    im_t0 = latents_to_pil(latents_x0)[0]\n    im_next = latents_to_pil(latents)[0]\n\n    # Combine the two images and save for later viewing\n    im = Image.new('RGB', (1024, 512))\n    im.paste(im_next, (0, 0))\n    im.paste(im_t0, (512, 0))\n    im.save(f'steps/{i:04}.jpeg')\n\n\n\n\n\n# Make and show the progress video (change width to 1024 for full res)\n!ffmpeg -v 1 -y -f image2 -framerate 12 -i steps/%04d.jpeg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p out.mp4\nmp4 = open('out.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=600 controls>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)\n\n\n\n      \n\n\n\nThe version on the right shows the predicted ‘final output’ (x0) at each step, and this is what is usually used for progress videos etc. The version on the left is the ‘next step’. I found it interesteing to compare the two - watching the progress videos only you’d think drastic changes are happening expecially at early stages, but since the changes made per-step are relatively small the actual process is much more gradual.\n\nClassifier Free Guidance\nBy default, the model doesn’t often do what we ask. If we want it to follow the prompt better, we use a hack called CFG. There’s a good explanation in this video (AI coffee break GLIDE).\nIn the code, this comes down to us doing:\nnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\nThis works suprisingly well :) Explore changing the guidance_scale in the code above and see how this affects the results. How high can you push it before the results get worse?"
  },
  {
    "objectID": "dm3.html#sampling",
    "href": "dm3.html#sampling",
    "title": "Lesson 14 - Stable Diffusion Deep Dive",
    "section": "Sampling",
    "text": "Sampling\nThere is still some complexity hidden from us inside latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]. How exactly does the sampler go from the current noisy latents to a slightly less noisy version? Why don’t we just use the model in a single step? Are there other ways to view this?\nThe model tries to predict the noise in an image. For low noise values, we assume it does a pretty good job. For higher noise levels, it has a hard task! So instead of producing a perfect image, the results tend to look like a blurry mess - see the start of the video above for a visual! So, samplers use the model predictions to move a small amount towards the model prediction (removing some of the noise) and then get another prediction based on this marginally-less-rubbish input, and hope that this iteratively improves the result.\nDifferent samplers do this in different ways. You can try to inspect the code for the default LMS sampler with:\n\n# ??scheduler.step\n\nIt keeps a buffer of past predictions (order=4) and uses a combination of these to better inform the current update. Explain more or link???"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics in Generative Modelling",
    "section": "",
    "text": "Hopefully find some people for good discussions here :)"
  },
  {
    "objectID": "generators_and_losses.html",
    "href": "generators_and_losses.html",
    "title": "Fun with Generators and Losses",
    "section": "",
    "text": "This content is adapted from a talk I gave to the MIT ‘Computer Visions’ class. I’ll link the recording if that get’s released, but in the meantime here is a different variant I recorded that includes a notebook run-through which should explain most of the code here.\nWe spoke about how deep learning relies heavily on one key idea: optimization:\nWe’re going to apply this mindset to the task of creating imagery in creative ways. To this end, we’ll explore a number of different ‘generators’ (each of which create an image from some set of parameters) and a number of ‘losses’ (which try to measure how ‘good’ the generated images are by some measure). And then we’ll play with combining different generators and losses to achieve different outputs.\nThese will be introduced one by one, but I’ve tried to make them as interchangeable as possible so that you can swap in or combine any of these building blocks for any of the demos. And at the end there’s a template for you to build your own final custom image generation tool and some hints for ideas to explore."
  },
  {
    "objectID": "generators_and_losses.html#our-first-generator-raw-pixels",
    "href": "generators_and_losses.html#our-first-generator-raw-pixels",
    "title": "Fun with Generators and Losses",
    "section": "Our First Generator: Raw Pixels",
    "text": "Our First Generator: Raw Pixels\nWhat if we just optimize some pixels directly? An image is now represented by a number of parameters (the raw RGB values). This should be a good test case, and a chance to think about how we want to frame our Generators going forward.\nWe need access to the parameters we can tweak, and a way to get the output.\nThe best way I know of is to lean on the machinery PyTorch has for neural networks by inheriting from the nn.Module class. nn.Parameter() makes a tensor that automatically has gradient tracking set up, and all parameters created this way can be accessed with the parameters() function of our generator - which saves us needing to write that ourselves.\nWe specify how we’d like to produce an output by defining the forward method. This lets us use our new object as a function - when we run im = gen() we’re actually saying im = gen.forward().\nThis might seem slightly overkill for this first example, but let’s just check it out and see how it works:\n\ngen = PixelGenerator(128)\nim = gen() # Get the output of the generator (the image)\nprint(f'Output shape: {im.shape}')\ntensor_to_pil(im) # View it\n\nOutput shape: torch.Size([1, 3, 128, 128])\n\n\n\n\n\nInspecting the parameters:\n\n[p.shape for p in gen.parameters()]\n\n[torch.Size([1, 3, 128, 128])]\n\n\nThere we go. Hopefully this will become useful in a second."
  },
  {
    "objectID": "generators_and_losses.html#our-first-loss-mean-squared-error",
    "href": "generators_and_losses.html#our-first-loss-mean-squared-error",
    "title": "Fun with Generators and Losses",
    "section": "Our First Loss: Mean Squared Error",
    "text": "Our First Loss: Mean Squared Error\nWe’ll take the difference between an image and a target and square it.\n\n# Make a target image\ntarget_image = torch.zeros(1, 3, 128, 128)\ntarget_image[:,1] += 1 # Set the green channel to all ones\ntensor_to_pil(target_image) # View it\n\n\n\n\n\n# Create a loss function with this as the target\nmse_loss = MSELossToTarget(target_image, size=128)\n\n\n# Calculate the loss between this and the output of a generator\ngen = PixelGenerator(128)\nim = gen()\nmse_loss(im) # We get a single measure with a way to trace the gradients backward\n\ntensor(0.3330, grad_fn=<MeanBackward0>)\n\n\nQ: Does that number make sense? What would the theoretical prediction be?"
  },
  {
    "objectID": "generators_and_losses.html#optimization",
    "href": "generators_and_losses.html#optimization",
    "title": "Fun with Generators and Losses",
    "section": "Optimization",
    "text": "Optimization\nWe want to tweak the parameters of our generator to make the loss (derived from the output) lower. Here’s how we might do this in PyTorch:\n\n# Set a target - here a green image as in the previous example\ntarget_image = torch.zeros(1, 3, 128, 128)\ntarget_image[:,1] += 1\n\n# Make a loss function based on this target\nmse_loss = MSELossToTarget(target_image, size=128)\n\n# Set up our generator\ngen = PixelGenerator(128)\n\n# Set up an optimizer on the generators parameters\noptimizer = torch.optim.Adam(gen.parameters(), lr=1e-2)\n\n\n# get the generator output\nim = gen()\n\n# find the loss\nloss = mse_loss(im)\n\n# Reset any stored gradients\noptimizer.zero_grad()\n\n# Calculate the gradients\nloss.backward()\n\n# Print the loss\nprint(loss.item()) \n\n# Update the generator parameters to reduce this loss\noptimizer.step()\n\n0.335597962141037\n\n\nRe-run the above cell a number of times, and use the following cell to see the current output:\n\ntensor_to_pil(gen()) # Generate and view an image\n\n\n\n\nIt gets greener over time - and the loss goes down. Hooray! Let’s define some new generators and loss functions and then make a clean version of this optimization code that runs in a loop so we don’t need to keep re-running a cell!"
  },
  {
    "objectID": "generators_and_losses.html#next-generator-imstack",
    "href": "generators_and_losses.html#next-generator-imstack",
    "title": "Fun with Generators and Losses",
    "section": "Next Generator: ImStack",
    "text": "Next Generator: ImStack\nImStack is a library I made to represent images as a ‘stack’ of tensors of different sizes. The intuition here is that the lowest level can incorporate the large shapes and higher layers can capture fine details. When optimizing it can be useful to have a few parameters that have a large effect on the output - this can allow a cleaner gradient signal than if each pixel is independant.\nThis ‘generator’ just wraps an imstack. Note that it is the same as the pixel_generator except that we have a few extra possible arguments when creating one - for example we can initialise it with an input image (which we’ll try soon).\n\ngen = ImStackGenerator(size=128, n_layers=4, base_size=16)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 128, 128])\nParameter shapes: [torch.Size([3, 16, 16]), torch.Size([3, 32, 32]), torch.Size([3, 64, 64]), torch.Size([3, 128, 128])]\n\n\n\n\n\nBreaking down the layers in the stack:\n\ngen.imstack.plot_layers()\n\n\n\n\nYou can explore tweaking the base_size, n_layers and scale parameters to see how they affect the look of the initial (random) output and the total number of parameters."
  },
  {
    "objectID": "generators_and_losses.html#style-transfer",
    "href": "generators_and_losses.html#style-transfer",
    "title": "Fun with Generators and Losses",
    "section": "Style Transfer",
    "text": "Style Transfer\nNow we’re going to try a classic application of pretrained models for artistic purposes: style transfer.\n\nExtracting features from a pretrained model\nPytorch has definitions of many common model architectures, and ways for loading pre-trained versions of them. In this case, we go for a small, older architecture called VGG16 trained on Imagenet (a dataset with >1M across 1k classes):\n\n# Load a pretrained model:\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\nvgg16.eval()\nvgg16 = vgg16.features\n\n\nim = torch.rand(1, 3, 128, 128).to(device) # A random image for demo\nfeats = calc_vgg_features(vgg16, im) # The activations of the specified layers\n[f.shape for f in feats] # See the shapes of the returned features\n\n[torch.Size([1, 3, 16384]),\n torch.Size([1, 64, 16384]),\n torch.Size([1, 128, 4096]),\n torch.Size([1, 256, 1024]),\n torch.Size([1, 512, 256]),\n torch.Size([1, 512, 64])]\n\n\nYou can see that from an input image we’ve got a bunch of features, one for each specified layer. We will use these for the style and content losses.\n\n\nContent Loss/Perceptual Loss\nRemember our picture of a CNN: \nWe spoke about how early layers tend to capture edges and textures, while later layers aggregate these smaller features into more complex ones.\nWe can exploit this to try and focus on the broad ‘content’ of an image in a way that is robust to small changes to texture or color. To achieve this, we’ll look only at activations from some deeper layers, in this case specified by content_layers = [14, 19]. You can print the network description and pick a few - see how changing them affects things!\n\n# print(vgg16)\n\n\ncontent_loss = ContentLossToTarget(im)\ncontent_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(2.4038, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\n# TODO: handle VGG16 when in other notebooks!\n\nWe won’t do a demo with just this loss, but feel free to experiment with it after you’ve seen a few of the upcoming demos. What happens when you start from random noise and optimise with just content loss to a target image - does it perfectly re-produce the target? What about intermediate stages - what kinds of feature appear first?\n\n\nStyle Loss (OT version)\nIn a similar way, we want to capture style features. We mentioned that these will be better described by earlier layers, but there is a hitch: we want the styles of a target image, but not necessarily in the same places (otherwise we’d just get the whole picture!). So we need some way to remove the spatial component and just focus on the relative mix of colours, textures etc.\nThere are a few approaches. Most tutorials will use a gram-matrix based approach (which works fine) but I recently heard of a potentially better approach using ideas of optimal transport via this great video. We’ll implement both and you can compare the two for yourself :)\nBoth give super large loss figures by default, so I’ve included a scale_factor argument to tame the values a little.\n\n# Create and test a version of this loss\nstyle_loss = OTStyleLossToTarget(im)\nstyle_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(10.4530, device='cuda:0', grad_fn=<MulBackward0>)\n\n\n\n# Testing...\nstyle_loss = GramStyleLossToTarget(im, vgg16=vgg16)\nstyle_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(10.6278, device='cuda:0', grad_fn=<MulBackward0>)\n\n\n\n# Create and test a version of this loss\nstyle_loss = VincentStyleLossToTarget(im, vgg16=vgg16)\nstyle_loss(torch.rand(1, 3, 128, 128).to(device))\n\ntensor([[5.7033]], device='cuda:0', grad_fn=<MulBackward0>)"
  },
  {
    "objectID": "generators_and_losses.html#new-generator-siren",
    "href": "generators_and_losses.html#new-generator-siren",
    "title": "Fun with Generators and Losses",
    "section": "New Generator: SIREN",
    "text": "New Generator: SIREN\nSIREN represents an image in an interesting way, using a bunch of sinusiodal functions in a network. Anyone with some signals processing background can probably guess why this seems interesting.\nWe’ll wrap a library that does all the hard work for us, but just for curiosity’s sake we can at least look at the building blocks, starting with the activation function:\n\ngen = SirenGenerator(size=128)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 128, 128])\nParameter shapes: [torch.Size([64, 2]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([3, 64]), torch.Size([3])]\n\n\n\n\n\n\n# TODO copy in and explain the rest of the code from https://colab.research.google.com/drive/1bsboh2GCxUwdzSmSg9AeCEaKKfW-hd74#scrollTo=wh8JQDX4izqN?\n\nWhat is neat here is that the output of the network is a function of x and y coords - we can evalluate this function at any resolution! No nasty pixels here. We can also control the number of parameters by chanigng the number and size of the layers. For example, here are two versions and the corresponding total number of parameters:\n\n# The default\ngen = SirenGenerator()\nprint('Number of parameters in default net:', sum([p.numel() for p in gen.parameters()]))\n\n# A smaller version\ngen = SirenGenerator(dim_hidden=16, num_layers=3)\nprint('Number of parameters in mini version:', sum([p.numel() for p in gen.parameters()]))\n\nNumber of parameters in default net: 17027\nNumber of parameters in mini version: 643\n\n\nSince these networks can be quite small, and run once per pixel at whatever size you want to generate, they are perfect for running as compute shaders. For eg, I trained a SIREN network with CLIP and turned it into a shader here: https://www.shadertoy.com/view/flGSDD (animating some of the parameters for a cool effect)."
  },
  {
    "objectID": "generators_and_losses.html#final-generator-bokeh",
    "href": "generators_and_losses.html#final-generator-bokeh",
    "title": "Fun with Generators and Losses",
    "section": "Final Generator: Bokeh!",
    "text": "Final Generator: Bokeh!\nI’m going to show one final generator here as a demo of how you can get more creative with things like this. I wanted to make images with a small number of shapes, and while playing around got the idea of summing gaussians to get blurry blobs of different colours.\nWhat are the parameters? The location, color, intensity and size of eah blob.\nHow do we render this in a way that is differentiable? It’s a little tricky, but to make it easier I did something to make any deep learning researcher cringe: I wrote a for loop. As in, for each dot: ... We don’t like things like this because GPUs are good at doing things in parallel! But hacky as it is, it works! You don’t have to do everything perfectly ;)\nThis code isn’t itself very interesting or worth copying, but hopefully it does highlight the more general idea: hack things together and have fun!\n\n# Create one with 100 blobs\nd = DotGenerator(100)\n\ngen = DotGenerator(size=256)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 256, 256])\nParameter shapes: [torch.Size([2, 100]), torch.Size([100]), torch.Size([100]), torch.Size([3, 100]), torch.Size([100])]\n\n\n\n\n\nYou’ll need to tweak parameters to keep the image looking nice with larger sizes or different numbers of dots, but at least this does roughly what we wanted. Inpecting the parameters you’ll see we have a few for each dot (100 dots here):\nhttps://teia.art/sparkles_jw has examples of some animations made with this same idea…"
  },
  {
    "objectID": "generators_and_losses.html#final-loss-clip",
    "href": "generators_and_losses.html#final-loss-clip",
    "title": "Fun with Generators and Losses",
    "section": "Final Loss: CLIP",
    "text": "Final Loss: CLIP\nOK, the final loss function is going to feel like a super-power. What if we want to just describe what we want in text?\nEnter CLIP. Remember: CLIP maps images and text to the same space, so we can compare them. We’ll load a CLIP model and test this for ourselves for a few mini demos before turning this into another loss function we can use to guide generation.\nText and image similarity (show use for one-shot classification and search)\nText or image (or multiple) as prompts\n\n#@title load a clip model\n\n# A nice small model (B='base') - good for quick tests and smaller download:\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n\n# A medium one (L='large'):\n# clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n\n# A massive one (H='huge') that needs lots of RAM but might generate better images?:\n# model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n\n\n# print(preprocess)\npreprocess = T.Compose([\n    T.Resize(size=224, max_size=None, antialias=None),\n    T.CenterCrop(size=(224, 224)),\n    T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n])\nclip_model.to(device)\n\n# We don't want to train CLIP at all so setting requires_grad=False everywhere\n# Probably unnecessary but rather safe than sorry :)\nclip_model.eval();\nfor p in clip_model.parameters():\n  p.requires_grad = False\n\n\n# One shot classification demo\n# Load an image\ncat_im = pil_to_tensor(Image.open('cat.jpeg')).to(device)\n\n# Encode the image with CLIP\nimage_embed = clip_model.encode_image(preprocess(cat_im))\nprint('Image embed shape:', image_embed.shape)\n\n# Encode some labels with CLIP\ntokenized_text = open_clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\ntarget_embeds = clip_model.encode_text(tokenized_text)\nprint('Texts embed shape:',target_embeds.shape) # One for each label\n\n# Find the similarity to each \ntorch.nn.CosineSimilarity()(image_embed, target_embeds)\n\nImage embed shape: torch.Size([1, 512])\nTexts embed shape: torch.Size([3, 512])\n\n\ntensor([0.1011, 0.1522, 0.2708], device='cuda:0')\n\n\nWe see a higher similarity for the label ‘a cat’ vs ‘a dog’ and ‘a diagram’ is the lowest’\nWe can flip this around to do image search. Given a load of images, we embed a text query and find the image that is the best match. This could be a fun exercise to try ;)\n\nUsing it as a loss\nWe can look at the similarity between the CLIP embedding of a generated image and one or more CLIP embeddings of images or text we’re feeding in as targets.\nLet’s look at this in action:\n\n# Create a generator and get an output im\ngen = SirenGenerator(size=128).to(device)\ngen.to(device)\nim = gen()\n\n# Embed this with CLIP\nwith torch.no_grad():\n  image_embed = clip_model.encode_image(preprocess(im))\nprint(image_embed.shape)\n\n# Embed some target texts\nwith torch.no_grad():\n  tokenized_text = open_clip.tokenize([\"a blue cat\", \"A cat picture\"]).to(device)\n  target_embeds = clip_model.encode_text(tokenized_text)\nprint(target_embeds.shape)\n\n# I wrote clip_loss_embeddings to take an image embed and multiple target embeds,\n# and return the average loss across the different targets:\nclip_loss_embeddings(image_embed, target_embeds)\n\ntorch.Size([1, 512])\ntorch.Size([2, 512])\n\n\ntensor(1.0452, device='cuda:0')\n\n\nMaking our neat loss class It helps to make multiple variations of the generated image so CLIP doesn’t see the exact same thing each time - hence the make_cutouts bit here. More cutouts => cleaner loss signal but more memory usage. You can explore this or just go with the defaults.\n\n# Testing...\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A cat'], image_prompts=[im], clip_model=clip_model)\nclip_loss_fn(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(0.5813, device='cuda:0')"
  },
  {
    "objectID": "generators_and_losses.html#ideas",
    "href": "generators_and_losses.html#ideas",
    "title": "Fun with Generators and Losses",
    "section": "Ideas",
    "text": "Ideas\nCan you get cool-looking images with a really small SIREN network? Can you get a nice-looking style transfer demo working? Can you implement a loss function that forces a specific palette of colours? Or a color gradient across the image? Can you chain these, first optimising towards one target then another (hint: just re-use a generator and call optimise again with different loss functions!). For eg SIREN to an image (via MSE) then to a CLIP prompt to tweak it. Advanced: Can you add new generators? One based on ‘Deep Image Priors’ perhaps, or using something like VQGAN and optimizing the latents which are then decoded by the VQGAN decoder to get an output image. Can you think of a loss that would avoid noisy images? Hint: look up ‘TV Loss’ How about a loss that penalizes over-saturated colours? How about a generator that can only make black and white images? How would you make a generator that created ‘seamless’ images to use in combination with CLIP? Advanced: What about using PyTorch3D’s differentiable rendering to optimise the vertices of a shape or the texture of a 3d model to match a style or CLIP prompt?\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "",
    "text": "If you’d prefer a much longer video with more explanation, here is a fairly rough live notebook walkthrough."
  },
  {
    "objectID": "optimization.html#gradient-descent",
    "href": "optimization.html#gradient-descent",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\n\n\n\nLets consider the following situation. Say we have: - A function (could be a neural net, could be the equation for a straight line…) that takes some input(s) and produces some output(s) based on a set of parameters - let’s call them w.  - Some measure of how well this function performs. Maybe this is how poorly the function describes some pattern in your data, or how well a network does in a classification task. Let’s call this measure the loss, where the goal is to make this as small as possible.\nThe question is often ‘how do we find a set of parameters that gives the best possible result?’. There are a few ways we could try to solve this. The most basic might be: - Try all possible values for all parameters - Randomly guess and keep the best\nClearly both of these have some major flaws, and when we’re dealing with thousands or millions of parameters there is no way you could try all possible combinations. So, we need a smarter approach.\n\nThe Gradient Descent Algorithm\nWhat if we could start from some set of parameters, and then see how to modify them slightly such that we get an improvement? Ideally, for each parameter we’d like to know what happens to the loss when we tweak that parameter slightly up or down. Formally, we’d like to know the gradient of the loss with respect to that parameter. You can think of the gradient as telling us which direction to move to get the biggest increase (or decrease if we go in the opposite direction).\nIF we can find these gradients, then a sensible method for finding a good set of parameters to solve a given problem would be 1. Start with some random parameters 2. Find the gradient of the loss with respect to each parameter 3. Update each parameter such that you move some small amount in the direction of steepest descent 4. Go back to step 2, finding the gradients based on the new parameter values and repeat all this a bunch of times.\nThis is the gradient descent algorithm in a nutshell :) Let’s do an example, where we’ll create some data that roughtly follows a trend and try to approximate that trend with a straight line, which will be specified by two parameters.\n\n\nCreating an Example Problem\nHere’s our ‘training’ data, with a single input (x) and a single target (y):\n\n# Creating some data:\nx = torch.rand(20)\ny = 3*x + 0.2 + torch.randn(20)*0.3 # y = ax + b + noise\nplt.scatter(x, y) # It's always helpful to visualize what's going on wherever possible.\n\n<matplotlib.collections.PathCollection>\n\n\n\n\n\n\n\n2.3 Defining our loss\nWe can describe a line as a function y = ax + b where a and b are our parameters. Take a look at the two lines shown here:\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3.6))\naxs[0].scatter(x, y)\naxs[0].plot(x, 0.1*x + 2.6, label='y1 = 0.1x + 2.6', c='orange')\naxs[0].legend()\naxs[1].scatter(x, y)\naxs[1].plot(x, 2*x + 0.5, label='y2 = 2*x + 0.5',  c='orange')\naxs[1].legend();\n\n\n\n\nClearly one does a better job than the other at describing the trend in this data. But how do we quantify this? There are several measures used in this sort of case, with a popular one being the ‘Root Mean Squared Error’. It sounds intimidating, but all we do is take the errors (how far each point is from the line), square them, sum the squares and then take the square root of that. More points further from the line -> higher errors (squaring takes care of any minus signs that would otherwise cause issues with points above/below the line) -> a higher final value for the RMSE. So lower is better. This is our loss function.\nHere’s one way to implement this in code (there are also built-in functions for this and many other ways you could write it):\n\ndef rmse(y, y_pred):\n    return torch.mean((y-y_pred)**2)**0.5 # See how many ways you can write this\n\nUsing this loss function, we can quantify how well those lines match the data:\n\ny1 = 0.1*x + 2.6\ny2 = 2*x + 0.5\nprint('RMSE for y_hat1 = 0.1x + 2.6:', rmse(y, y1))\nprint('RMSE for y_hat2 = 2*x + 0.5:', rmse(y, y2))\n\nRMSE for y_hat1 = 0.1x + 2.6: tensor(1.3639)\nRMSE for y_hat2 = 2*x + 0.5: tensor(0.3955)\n\n\nThe second line has a lower loss, and is therefore a better fit. Tweak the parameters and see if you can do even better.\nTHINK: What is your thought process as you try this? Are you doing something like the gradient descent described earlier?\n\n\n2.4 Calculating Gradients\nSo, how do we find the gradients we keep talking about? If you’re good at differentiation, you can look at a function and figure them out analytically. But this quickly breaks down when the function is complex or involves many steps. Fortunately, PyTorch does something called automatic differentiation, where it can keep track of every operation that happens to a tensor. It builds something called a computational graph, and when you want to calculate the gradients with respect to some final result you can simply call .backward() and PyTorch will trace the path back through this graph filling in the gradients at each step. We won’t go too deep into this, but here’s a simple example:\n\n# Some operations to demonstrate autograd\na = torch.tensor(5.7)\na.requires_grad = True # This is important - by default PyTorch won't track gradients\n\nb = 5*a + 2\n\nc = torch.sin(2*b+0.1)\n\nc\n\ntensor(-0.9871, grad_fn=<SinBackward0>)\n\n\nNotice that grad_fn bit there? Because c depends on something (b) that depends on a tensor that requires_grad (a), PyTorch keeps track of the function needed to calculate the gradients. We could then see the gradient of c with respect to a with:\n\nc.backward() # Tell pytorch to propagate the gradients backwards down the chain of operations\na.grad # See the resulting gradient\n\ntensor(-1.6036)\n\n\nThe derivative of c with respect to a is (10*cos(10*(a+0.41)) - plugging in a=5.7 we see that this does indeed give the answer dc/da = -1.603. This is quite magical - we can chain complex functions together and as long as eveything is differentiable we can rely on PyTorch to be able to work backwards and give us all the gradients we need.\n\n\nGradient Descent on our Toy Example\nLet’s get back to that example we were playing with, trying to find the parameters for a line that best describes the trend in our data.\nWe create our parameters w (initialized to 2 random floats) and tell pytorch to keep track of gradients.\nThen, in a loop, we repeatedly find the loss, find the gradients (loss.backward()) and update the parameters accordingly. We could do this ourselves but PyTorch provides an optimizer that handles the update for us - torch.optim.SGD. The learning rate lr determines how small of a step we take at each iteration.\nOnce the loop has finished running, we plot the losses and see that we are indeed getting better and better over time.\n\nw = torch.rand(2) # Our parameters\nw.requires_grad = True # Explain\n\noptimizer = torch.optim.SGD([w], lr=0.2) # Research: What does SGD stand for?\n\nlosses = [] # Keep track of our losses (RMSE values)\nws = [] # Keep track of the values we predicted\n\nfor i in range(100):\n\n    # Reset everything related to gradient calculations\n    optimizer.zero_grad()\n\n    # Get our outputs\n    y_hat = w[0]*x + w[1] \n\n    # Calculate our loss\n    loss = rmse(y, y_hat)\n\n    # Store the loss and a copy of the weights for later\n    losses.append(loss.detach().item())\n    ws.append(w.clone().detach().numpy())\n\n    # Print out updates ever few iterations\n    if i % 20 == 0:\n        print('loss at step', i, ':', loss)\n\n    # Backpropagate the loss and use it to update the parameters\n    loss.backward() # This does all the gradient calculations\n    optimizer.step() # The optimizer does the update. \n\n\nplt.plot(losses)\nplt.title('Loss over time')\n\nloss at step 0 : tensor(1.0562, grad_fn=<PowBackward0>)\nloss at step 20 : tensor(0.5021, grad_fn=<PowBackward0>)\nloss at step 40 : tensor(0.3565, grad_fn=<PowBackward0>)\nloss at step 60 : tensor(0.2702, grad_fn=<PowBackward0>)\nloss at step 80 : tensor(0.2418, grad_fn=<PowBackward0>)\n\n\nText(0.5, 1.0, 'Loss over time')\n\n\n\n\n\nOur random parameters have been updated 100 times and are now close to as good as they can possibly get:\n\nw # View the learned parameters\n\ntensor([3.0881, 0.0810], requires_grad=True)\n\n\n\nw.grad # We can see the gradients of the loss with respect to w (now small since we're close to optimum)\n\ntensor([-0.0208,  0.0105])\n\n\n\n# Plot predictions with these parameters\nplt.scatter(x, y, label='training data')\ny_hat = w[0]*x + w[1]\nplt.plot(x, y_hat.detach(), c='red', label='y_hat (with learned parameters)')\nplt.legend();\n\n\n\n\nSince we only have two parameters, we can make a plot that shows the loss for every combination of values within some range. We’ll plot the values of the parameters during the optimization loop above as points, and you can see how they slowly ‘move’ towards a point with lower loss:\nInstead of optimizer.step(), we could do w -= w.grad * 0.2 where 0.2 is the learning rate and the minus sign is because we want to move in the direction that reduces loss (so opposite to the steepest gradient).\nEXERCISE:: Try this and confirm for yourself that this works. (You’ll need with torch.no_grad(): w -= w.grad * 0.2 or PyTorch will complain - try it without first for a glimpse at an error you’ll likely meet a few more times in life ;)\nTHINK: Does this make sense? Are there any issues? What happens when the gradients are small? What happens when our step size (learning rate) is too high?\nTHINK: What kinds of problems can we solve with this tool? Can you think of examples? What are the limitations?"
  },
  {
    "objectID": "optimization.html#optimization-methods",
    "href": "optimization.html#optimization-methods",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Optimization Methods",
    "text": "Optimization Methods\nGradient Descent (or Stochastic Gradient Descent, which is just GD on batches of data rather than the full dataset) is just one optimization method. There are many improvements that can be made. If you’re interested, here is a great rundown of the many alternatives that are used today: https://ruder.io/optimizing-gradient-descent/\nOne useful idea that is bundled with optimizers in PyTorch is that of regularization. It’s a large topic, but in essence regularization is concerned with smoothing things out and simplifying models or parameter sets by avoiding any values that are too extreme.\nAt some point I hope to add another notebook for exploring this, for now just remember that there are lots of different choices of optimizer available in PyTorch and they can be fun to experiment with and compare."
  },
  {
    "objectID": "optimization.html#demo-time",
    "href": "optimization.html#demo-time",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Demo Time!",
    "text": "Demo Time!\nWe’ve solved the toy problem above with optimization. What else can we do with this? Well, for one thing this same approach is used to train pretty much all neural networks in use today! We’ll look at how that works in a future lesson. For now, you might enjoy checking out the bonus notebook ‘Fun With Generators and Losses’ where we look at a number of ‘generators’ (functions with parameters we can optimize that produce and image) and a number of ‘losses’ which we can use to get different effects. For example, here’s how we’d optimize a network based on sinusoidal functions to match a text description (leaning on some magical algorithms like CLIP which we haven’t covered yet):\n\nfrom tglcourse.generation_utils import SirenGenerator, CLIPLossToTargets, optimise\nimport torch\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ngen = SirenGenerator().to(device)\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A painting of a jellyfish by a coral reef'], n_cuts=32, device=device)\noptimizer = torch.optim.Adam(gen.parameters(), lr=0.01)\noptimise(gen, [clip_loss_fn], optimizer=optimizer, n_steps=50)\n\nIf you dig into the code for that optimise function you’ll see that it is essentially just the same as the optimization loop we made earlier in this notebook!"
  },
  {
    "objectID": "optimization.html#where-next",
    "href": "optimization.html#where-next",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Where next",
    "text": "Where next\nThe bonus notebook ‘Fun with Generators and Losses’ and the accompanying video are fairly complimentary to this notebook. The video especially shows a very high-level view of optimization and how this single idea underlies a TON of deep learning applications. Lots of the code in the notebook will look unfamiliar since we haven’t covered many of the building blocks in this course yet - give it a skim now and then re-read it again after, say, lesson 5.\n\n\n\n\n\nTwo other ideas for how to keep busy until the next lesson:  - Check out The spelled-out intro to neural networks and backpropagation: building micrograd where Andrej Karpathy trains a neural network from scratch, including implementing the gradient calculations and things which we’ve been offloading to PyTorch. Great if you like to see things built from the bottom up! - Go back through this notebook and try to tweak as many things as you can. Re-implement the loss calculation without peeking at the RMSE function, change the starting values of the parameters we’re optimizing, try a different optimizer (see the torch documentation) or play with changing the learning rate. Can you find a way to get a good solution with fewer steps? Does it ever go wrong and give crazy results or errors? - Look for code online for training a neural network in PyTorch. Can you spot the optimization loop? More generally, can you find examples online of deep learning tasks and identify the three ingredients shown above in each case?"
  },
  {
    "objectID": "optimization.html#a-dose-of-ethics-objectives-matter",
    "href": "optimization.html#a-dose-of-ethics-objectives-matter",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "A Dose Of Ethics: Objectives Matter",
    "text": "A Dose Of Ethics: Objectives Matter\nLet’s say you want to predict recidivism rates (how likely a person is to commit a crime once released). What should your loss function be? Perhaps you can look at past cases, see who was re-arrested and train a model to maximise accuracy when predicting this. After all, arrests are a good way to measure crimes, right? But what if some neighbourhoods have more police presence, or the officers are more likely to arrest some subset of the population? If the measure is biased, the algorithm will be biased, and a biased algorithm can mean unfair outcomes. This is a big problem for risk prediction software and is one example of the kinds of issues that can arise when you pick the wrong metric to optimize.\nFrom content recommendation systems pushing outrage-inducing clickbait to maximise ‘engagement’, to chatbots spewing hate-speech, failures to consider the consequences of optimizing towards a single metric abound. We’re going to have a lot of fun with optimization, but remember: with great fun… comes great responsibility.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets and General Data Utilities",
    "section": "",
    "text": "For many tasks in machine learning, the actual model training is the easy bit! Many data scientists spend most of their time sourcing and exploring data, and getting it into the right format ready for modelling. Lucky for us, most of the data we’ll use for demos during this course has already been collected and organised for us, and to make things even more convenient during the lessons themselves we’re going to lay some additional groundwork here in this notebook.\nMotivate dataloaders Batching Advantage of pre-fetching the next batch Mention monitoring GPU usage and watching for CPU bottlenecks in the dataloaders Dive into pytorch dataloaders HF Hub and datasets library\nhttps://huggingface.co/docs/datasets/quickstart\nhttps://huggingface.co/docs/datasets/stream\nDATA UTILS\n\nTODO redo this and integrate into notebooks\n\ndataloader = get_cifar10_dl(batch_size=128, split='train')\nbatch = next(iter(dataloader))\nbatch['image'].shape, batch['label']\n\nReusing dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-16b9e105e7ead8c5.arrow\nParameter 'transform'=<function cifar10_transform> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n(torch.Size([128, 3, 32, 32]),\n tensor([1, 2, 6, 7, 9, 4, 7, 6, 4, 2, 2, 0, 4, 8, 4, 2, 5, 7, 2, 9, 9, 8, 8, 1,\n         4, 3, 7, 3, 5, 6, 9, 3, 6, 4, 3, 4, 7, 9, 3, 3, 0, 6, 4, 3, 5, 1, 9, 6,\n         2, 2, 1, 0, 6, 7, 4, 3, 1, 4, 4, 2, 2, 5, 4, 5, 7, 0, 3, 0, 8, 4, 5, 7,\n         9, 0, 9, 9, 9, 4, 8, 3, 3, 6, 5, 5, 3, 2, 8, 1, 4, 3, 4, 2, 7, 8, 2, 0,\n         9, 6, 8, 7, 4, 3, 2, 0, 2, 0, 3, 2, 4, 9, 2, 5, 9, 6, 0, 6, 0, 7, 2, 2,\n         1, 7, 5, 9, 6, 8, 6, 4]))"
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Lesson 10 - Transformers",
    "section": "",
    "text": "# https://youtube.com/watch?v=gDNRnjcoMOY\n\nIllustrated transformer: https://jalammar.github.io/illustrated-transformer/\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "discussions.html",
    "href": "discussions.html",
    "title": "Discussions",
    "section": "",
    "text": "Most of this course is me (Jonathan Whitaker) talking into my webcam. I do my best to cover the different topics we explore in this course, but you’re ultimately getting a single viewpoint on all of this - not ideal! There are thousands of people with unique backgrounds and perspectives working in this space, and this discussions series is an attempt to bring in some of those different voices and ideas. If there is someone you think should be on this list, please reach out and let me know! I’m still figuring out the recording workflow, so some early interviews might have slightly sub-optimal audio - I’ll add transcripts ASAP."
  },
  {
    "objectID": "discussions.html#apolinário-passos---ml-art-engineer-huggingface",
    "href": "discussions.html#apolinário-passos---ml-art-engineer-huggingface",
    "title": "Discussions",
    "section": "Apolinário Passos - ML Art Engineer @ Huggingface",
    "text": "Apolinário Passos - ML Art Engineer @ Huggingface\nApolinário is one of the best folks to follow to keep track of what is happening in this space. His ‘MindsEye’ tool and more recently the ‘Majesty Diffusion’ notebook have helped to make some of the latest AI art techniques easier to use for non-technical users. These days he works at HuggingFace in the intersection between the technical side and the artistic community. In this conversation we discuss his journey and work, how to stay on top of new releases and how to get involved.\n\n\n\n        \n        \n\n\nLinks:  + Apolinário’s website: multimodal.art/  + His @multimodalart Twitter account"
  },
  {
    "objectID": "discussions.html#enzymezoo---artist-and-developer-deforum",
    "href": "discussions.html#enzymezoo---artist-and-developer-deforum",
    "title": "Discussions",
    "section": "@EnzymeZoo - Artist and Developer (Deforum)",
    "text": "@EnzymeZoo - Artist and Developer (Deforum)\n@EnzymeZoo is an artist and developer. In this discussion we chat about the community development work that goes into creating a tool like the Deforum notebook. We explore how that project came together, how you can get involved and @EnzymeZoos general thoughts on AI art and creativity.\n\n\n\n        \n        \n\n\nLinks:  + Deforum Discord (user discord, but from there you can find the developer group as well)  + Jax diffusion notebook  + Disco Diffusion notebook  + List of tools by @pharmapsychotic which has many more notebooks and guides: https://pharmapsychotic.com/tools.html  + An artist making very cool animations with Deforum that @EnzymeZoo shared: @Infinite Vibes  + More creations using deforum on Twitter \nAfter we stopped recording @EnzymeZoo also asked that I mention/thank Stability AI. The release of Stable Diffusion was the catalyst for Deforum springing into existence, and they have since started supporting some of the developers behind this and other notebooks."
  },
  {
    "objectID": "discussions.html#teodora-szasz",
    "href": "discussions.html#teodora-szasz",
    "title": "Discussions",
    "section": "Teodora Szasz",
    "text": "Teodora Szasz\nTeodora works at the University of Chicago helping researchers from different departments incorporate deep learning into their work. She also runs workshops on topics like AI Art and creative coding! In this discussion we chat about her background and research, then zero in on her research into representation in media and how generative models might help create stories with characters that more people can identify with.\n\n\n\n        \n        \n\n\nLinks:  + A video on the childrens book project  + Teodora’s Twitter @TeodoraSzasz  + https://github.com/joojs/fairface"
  },
  {
    "objectID": "discussions.html#jason-antic",
    "href": "discussions.html#jason-antic",
    "title": "Discussions",
    "section": "Jason Antic",
    "text": "Jason Antic\nJason is the creator of the incredible ‘DeOldify’ application for restoring historical photos, and one of the best experimentalists I’ve met. In this conversation we chat a little bit about his journey then dive into the nuts and bolts of what research actually looks like. Audio only since I lost power as recording started and didn’t notice the video recording freezing.\n\n\n\n        \n        \n\n\nLinks:  + Deoldify integrated into MyHeritage: https://www.myheritage.com/incolor  + A (slightly outdated) blog post with some technical details  + The fast.ai course that we both credit as a large part of our paths into deep learning: https://course.fast.ai/"
  },
  {
    "objectID": "discussions.html#hamel-husain",
    "href": "discussions.html#hamel-husain",
    "title": "Discussions",
    "section": "Hamel Husain",
    "text": "Hamel Husain\nHamel is a Machine Learning Engineer/Data Scientist passionate about making tools. He is one of the most helpful and interesting people I’ve met. He turned the usual interview process on its head and decided to interview me too, so you get two for the price of one in this episode!\n\n\n\n        \n        \n\n\nLinks: + Hamel’s personal site: hamel.dev  + NBDev\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Generative Landscape",
    "section": "",
    "text": "This is still a work in progress, but most of the important bits should be done in time for a launch on Friday, Novermer 25! In the meantime, you can check out my previous course, AIAIART or join the Discord where I’ll be streaming final lesson checks and updates. \nIf you want to support this effort, I now have a Patreon: https://www.patreon.com/johnowhitaker \nThe material will show in github pages at https://johnowhitaker.github.io/tglcourse/ (and the http://thegenerativelandscape.com will redirect there now too). Hooray for the magic of nbdev. \nJoin our Discord to discuss the course, join study groups or chat about all things generative. That’s also the place to go for notifications of live lesson walkthroughs and course updates.\n\n\n\n\n\nCheck out the Getting Started page for an overview of the course and more information on things like study groups.\nCheck out the Library page for information on the tglcourse library that accompanies the course.\n\nThe Plan\nThe idea is to have a core curriculum building up to an understanding of key generative modelling techniques, split into three rough sections. The first 5 lessons will cover basics of building NNs and crafting loss functions. Lessons 6-10 will introduce generative modelling, GANs and working with sequences using transformers. Lessons 11-15 will be a deep dive into diffusion models, and lesson 16 will wrap up and give sugestions for new directions to explore.\nAlongside this will be a number of ‘bonus’ notebooks that don’t need to be completed but which augment the core content. Managing datasets, experiment tracking, sharing demos and so on. Some will augment specific lessons, some will add functionality to the library and some will just be standalone topics I think are cool. The latter category is likely to continue to grow even after the course launches :)\nThere will be three suggested projects (after lessons 5, 11 and 14) to mark milestones in the course, and the final lesson will also encourage you to do a larger project at the end. Once we launch there’ll be a place to everyone’s projects and some prizes for the best.\nThis table has a rough status on the main lessons.\n\n\n\n\n\n\n\n\nLesson\nDescription\nStatus\n\n\n\n\nLesson 1: PyTorch Basics\nIntro to PT, tensor manipulation, images as tensors, LeastAverageImage exercise\nDone\n\n\nLesson 2: Optimization\nIntro to GD, optimization examples exercise\nRough Draft Done\n\n\nLesson 3: Building NNs\nnn.Module, building blocks, CNNs\nRough Draft Done\n\n\nLesson 4: Learning Representations\nWhat do networks learn, style transfer\nWIP\n\n\nLesson 5: CLIP\nDemo use as a loss function, video deep dive into VQGAN notebooks\nRough Draft Done\n\n\nLesson 6: Generative Modelling\nVAE part, latent walks, PCA\nWIP\n\n\nLesson 7: GANs 1\nIntro to GANs, DC GAN, Conditioning\nWIP\n\n\nLesson 8: GANs 2\nGAN training tricks, NOGAN, using modern GANs, VQGAN\nNot Started\n\n\nLesson 9: Sequence Modelling Intro\nidea, language modelling concept, transformer demo\nWIP\n\n\nLesson 10: Transformers\nIntro to transformes, attention, comparing to lstm, reading minGPT\nNot Started\n\n\nLesson 11: Everythign is a sequence\nShow whistlegen, protein, VQGAN, parti…\nNot Started\n\n\nLesson 12: DM 1\nIntro to diffusion models, toy example, comparison to DDPM\nRough Draft Done\n\n\nLesson 13: DM2\nConditioning, CFG, guiding, sampling, better training\nWIP\n\n\nLesson 14: DM3\nSD deep dive\nRough Draft Done\n\n\nLesson 15: DM4\nOther modalities - ideally demo class-conditioned audio generation. Might not be done by the time the course launches but would be nice to have.\nWIP\n\n\nLesson 16: Going Further\nFinding your niche, exploring less common areas\nWIP\n\n\n\n\n\nFAQs\nSome course-related questions that have tricked in:  + ‘Any prerequisites?’: If you’re comfortable with a bit of Python and using Jupyter Notebooks you should be ready to take this course. No prior deep learning knowledge is assumed, and although we will dive fairly deep fairly quickly I’ve tried to link lots of resources wherever possible.  + ‘How long is the course?’: There are 15 core lessons plus a number of bonus notebooks. You can take them at whatever pace you find enjoyable, or join in with a study group on Discord to work through one a week. + ‘Does the HuggingFace Diffusion Model Class supercede this?’: I’ve teamed up with HF to help build their diffusion model class, sharing a lot of material between it and this course. Both will have unique things to add - I’d recommend signing up for theirs even if you’re working through ‘The Generative Landscape’ as well, since there will be extra projects and fun community activities to get involved in with that one too.  + ‘Can genereative landscape be real?’: Yes, if you subscribe to David Chalmers ‘simulation realism’ ;) + ‘Will it include stable diffusion?’: Yes, see lesson 14 + ‘Will we learn how to adapt these method to 3d?’: At some point I’d love to add more 3D related content - stay tuned for bonus notebooks once the craziness of the course launch calms down. + ‘I hope this includes generating text!’: It does! Lessons 9 - 11 deal with modelling sequences, and should set you up with everything you need to make models which can spew out AI-generated gibberish all day long. +‘My question isn’t in the FAQs?’: OK, so I made this one up. But if you have a burning question, send it to me and I’ll add it here.\nCI status: \nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "interfaces_with_gradio.html",
    "href": "interfaces_with_gradio.html",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "",
    "text": "In this notebook we’ll see how Gradio let’s you quickly turn a function into a beautiful-looking web application that you can share with others to demo your latest model or pipeline.\nI’ve kept this fairly simple, and tried to show the process I’d follow if I was making this from scratch rather than just the final result."
  },
  {
    "objectID": "interfaces_with_gradio.html#what-should-our-demo-do",
    "href": "interfaces_with_gradio.html#what-should-our-demo-do",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "What Should Our Demo Do?",
    "text": "What Should Our Demo Do?\nAs a first step, we need to decide what our demo should do! Here I load an image and experiment with fitting a small Siren network to it as a way to create an interesting blurred effect:\n\nfrom tglcourse.generation_utils import MSELossToTarget, SirenGenerator, optimise\nfrom tglcourse.utils import *\n\nLoading the image:\n\nim = load_image_pil('images/frog.png').resize((256, 256))\nim\n\n\n\n\nCreating a SirenGenerator (see the ‘Fun with Generators and Losses’ bonus notebook):\n\ngen = SirenGenerator(dim_hidden=32, num_layers=3)\n\nAt the moment the output of the generator doesn’t look like much:\n\ntensor_to_pil(gen())\n\n\n\n\nCreate a loss function that compares an input to a target image, in this case our frog:\n\nloss = MSELossToTarget(pil_to_tensor(im), size=256)\nloss(gen())\n\ntensor(0.1525, grad_fn=<MeanBackward0>)\n\n\nOptimise the parameters of the generator based on this loss:\n\noptimise(gen, [loss], n_steps=30)\n\n\n\n\n\n\n\n\n\n\n\nAnd get the result back as a PIL image:\n\ntensor_to_pil(gen())\n\n\n\n\nOK, so far so good - let’s try to serve this process up in an easy-to-use interface."
  },
  {
    "objectID": "interfaces_with_gradio.html#wrapping-our-code-in-a-function",
    "href": "interfaces_with_gradio.html#wrapping-our-code-in-a-function",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Wrapping our code in a function",
    "text": "Wrapping our code in a function\nThe first step to turning this into a nice web app is wrapping the above in a nice function:\n\nfrom IPython.utils import io\n\n\ndef sirenify(image, size=256):\n    gen = SirenGenerator()\n    loss = MSELossToTarget(pil_to_tensor(image), size=int(size))\n    with io.capture_output() as captured: # Hide output to keep things clean - remove for debugging\n        optimise(gen, [loss], n_steps=30)\n    return tensor_to_pil(gen())\n\n\nsirenify(im)\n\n\n\n\nYou can see it takes several inputs, each of which we’ll be able to set using the gradio interface. In this case we want to return an image, but you can return multiple different outputs (maybe an image, a caption and a confidence score)."
  },
  {
    "objectID": "interfaces_with_gradio.html#making-the-gradio-interface",
    "href": "interfaces_with_gradio.html#making-the-gradio-interface",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Making the Gradio Interface",
    "text": "Making the Gradio Interface\nWith the defined, we can create the interface like so:\n\nimport gradio as gr\n\n\niface = gr.Interface(fn=sirenify, inputs=[gr.Image(type='pil'), gr.Number(value=256)], outputs=gr.Image())\niface.launch(share=True)\n\nPretty simple! You should be able to upload an image and click ‘Submit’, and if all goes well you’ll soon see the result in the output tab on the right. For example:"
  },
  {
    "objectID": "interfaces_with_gradio.html#sharing-with-the-world",
    "href": "interfaces_with_gradio.html#sharing-with-the-world",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Sharing with the World",
    "text": "Sharing with the World\nIn the example above we used share=True and got a public URL. You can send this to someone for a quick demo, but the link will expire in 72 hours and the demo is running on your machine. If you want to share your app somewhere more permanent, the easiest option is HuggingFace Spaces.\nGetting an app running on there is as simple as packaging up your code into a script and adding requirements to a ‘requirements.txt’ file. The Gradio docs include some examples and there are plenty of tutorials out there if you get stuck. However, my favourite resource for seeing how to do this is reading the source code for existing spaces! For any space on the HuggingFace Hub! For example, check out the stable diffusion space, click on ‘Files and Versions’ and open app.py to see how they handle a more complex layout with custom styling and a bunch of integrated examples.\nThis excellent guide shows how you can take an experiment such as our demo above and export the relevant code straight from a notebook into an app.py file using NBDev. Yet another place where NBDev just seems like magic; I think this is how I will build all my spaces going forward!"
  },
  {
    "objectID": "wrapup.html",
    "href": "wrapup.html",
    "title": "Lesson 16: The Next Generation",
    "section": "",
    "text": "Congratulations! You made it to the end!\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "building_nns.html",
    "href": "building_nns.html",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "",
    "text": "In Lesson 2 we said that if we have (1) a function defined by some parameters and (2) a loss function or some measure of how well it performs then we can optimize the parameters using something like gradient descent to try and improve performance.\nWhat does that ‘function’ look like? In many cases, the answer is some sort of neural network. Artificial Neural Networks (ANNs) have been around for many years [citation needed] but it is only in the psat decade or so that we’ve really figured out ways to train them well. The reason they’re so great is that they are pretty good general function approximators. In this notebook we’ll start with a very simple network approximating a fairly simple function, and then we’ll move on to solving a classic ML problem: classifying hand-written digits.\nIn the bonus notebook [TODO finish, link] and tomorrows lesson [TODO finish, link] we will build on these foundations to see how we can use modernt pre-trained networks to solve much more complex problems than those presented here.\nFor this notebook the code examples move fairly quickly - the lesson run-through will try to break them down more so check that out if you’re having difficulty."
  },
  {
    "objectID": "building_nns.html#a-simple-neural-network",
    "href": "building_nns.html#a-simple-neural-network",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "A Simple Neural Network",
    "text": "A Simple Neural Network\n\n\n\n\n\nLet’s start by making a smallish network to solve a small (contrived) problem. We’ll generate some data, and as in the previous lesson we’d like our network to learn the relationship between our input data (x) and output data (y).\n\n# Generating some data (inputs and targets)\nn_samples = 64\ninputs = torch.linspace(-1.0, 1.0, n_samples).reshape(n_samples, 1)\nnoise = torch.randn(n_samples, 1) / 5\ntargets = torch.sin(3.14 * inputs) + noise\nplt.figure(figsize=(8, 5))\nplt.scatter(inputs, targets, c='c')\nplt.xlabel('x (inputs)')\nplt.ylabel('y (targets)')\nplt.show()\n\n\n\n\nHere’s how we can make a neural network, leaning on PyTorch’s handy functions:\n\n## A Wide neural network with a single hidden layer\nclass WideNet(nn.Module):\n\n    def __init__(self, n_cells=512): # Initialize our network\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(1, n_cells), # One input, n_cells outputs\n            nn.Tanh(), # Our non-linearity - there are many on offer!\n            nn.Linear(n_cells, 1), # n_cells inputs, one output\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nWe’re inheriting from the nn.Module class, which gives us some bonus features. For example, instead of directly calling the forward method (which passes the data through the different layers) we can just create our network and call it like a function:\n\nwn = WideNet()\nprint('Input shape:', inputs.shape)\nout = wn(inputs) # This passes our data in as the input to the forward method defined above\nprint('Output shape:', out.shape)\nprint('PyTorch sumary of wn:')\nwn\n\nInput shape: torch.Size([64, 1])\nOutput shape: torch.Size([64, 1])\nPyTorch sumary of wn:\n\n\nWideNet(\n  (layers): Sequential(\n    (0): Linear(in_features=1, out_features=512, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=512, out_features=1, bias=True)\n  )\n)\n\n\nThis network includes some layers that have learnable parameters. We can access all of these via wn.parameters() - in this case we get four sets of parameters - the weights and biases for each of the two linear layers. Feel free to experiment with the network definition and see how this changes:\n\n[p.shape for p in wn.parameters()]\n\n[torch.Size([512, 1]),\n torch.Size([512]),\n torch.Size([1, 512]),\n torch.Size([1])]\n\n\nTime for our training loop - compare this with the optimization loop we in the previous lesson (spoiler: they’re the same!). We’re optimizing the parameters of our neural network - all the weights and biases in the different layers.\n\n# Create our network\nwide_net = WideNet()\n\n# Create a mse loss function\nloss_function = nn.MSELoss()\n\n# Stochstic Gradient Descent optimizer\noptimizer = torch.optim.Adam(wide_net.parameters(), lr=1e-3)\n\n# The training loop\nlosses = []  # keeping recods of loss\nfor i in range(500): # 500 'epochs' of training\n    optimizer.zero_grad()  # set gradients to 0\n    predictions = wide_net(inputs)  # Compute model prediction (output)\n    loss = loss_function(predictions, targets)  # Compute the loss\n    loss.backward()  # Compute gradients (backward pass)\n    optimizer.step()  # update parameters (optimizer takes a step)\n\n    # Storing our loss for later viewing\n    losses.append(loss.item())\n\n# Plot the losses over time\nplt.plot(losses) # Plot the losses over time\n\n\n\n\nTHINK/DISCUSS: Go line-by-line through the code above - does it make sense?\nNotice we don’t have to set requires_grad manually on any of the parameters, since the learnable parameters in each layer are grouped automatically by wide_net.parameters() (inspect that and see what it contains).\n\nplt.scatter(inputs, targets)\nplt.plot(inputs, wide_net(inputs).detach(), c='red', label='Predictions')\n# Create a new, untrained widenet and plot those predictions for comparison\nnew_wn = WideNet()\nplt.plot(inputs, new_wn(inputs).detach(), c='yellow', label='Predictions (untrained network)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Exercise: Create a neural network with two hidden layers and train it on this same task..."
  },
  {
    "objectID": "building_nns.html#a-lightning-overview-of-convnets",
    "href": "building_nns.html#a-lightning-overview-of-convnets",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "A Lightning Overview of ConvNets",
    "text": "A Lightning Overview of ConvNets\nSo-called ‘dense’ networks are useful in some cases, but we need to be mindful of the number of parameters required to solve certain types of problems. For example, consider the case of image recognition - our inputs consist of thousands of individual pixel values. A dense network that could take in 500px images and then run them through many hidden layers ends up with millions or billions of parameters, which can make training tricky.\nIn addition, each pixel feeds into a different part of the network. When we look at how the vision system works in the brain, or just think about what we’d want in a computer vision system, we’ll start to hit requirements that might not be easy to satisfy with a simple MLP network. Fortunately, we have some tricks to imporve things! Here’s another video that takes us through a key idea in deep learning for images: CNNs\nHere’s another video from the (free and I CC-licenced) neuromatch course that gives a little more background:\n\nhtml = ipd.display(ipd.IFrame(src=\"https://www.youtube.com/embed/AXO-iflKa58\", width=\"560\", height=\"315\"))\nhtml\n\n\n        \n        \n\n\nThe following interactive website is a great way to get an intuition for both how convolution works. You can see each learned filter and the corresponding output. It also shows a second key idea: pooling. By ‘downsampling’ the outputs of successinve convolution layers we end up with fewer and fewer activations, each representing more and more of the input image. Play around a bit until you’re sort of happy with the basic concepts (see the video for more discussion and explanations) and then move on to the next section, where we’ll build our own very similar network.\n\n# html = ipd.display(ipd.HTML('<iframe width=\"1200\" height=\"600\" src=\"https://adamharley.com/nn_vis/cnn/3d.html\" title=\"CNN Visualization\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'))\n# html\n\nFor more on what convolution is, how the different hyperparameters (padding, stride etc) do and a general overview of CNNs, see https://poloclub.github.io/cnn-explainer/"
  },
  {
    "objectID": "building_nns.html#the-dataset",
    "href": "building_nns.html#the-dataset",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "The Dataset",
    "text": "The Dataset\nWe’ll use the classic MNIST dataset (as shown in the video and examples above). But these same ideas apply to more complex image recognition\n\n#@title Loading the data\nmnist_dl_train = get_mnist_dl(batch_size=128, split='train')\nmnist_dl_test = get_mnist_dl(batch_size=128, split='test')\n\n# Get one batch of data\nbatch = next(iter(mnist_dl_train))\n\ndata_shape = (1, 28, 28)\n\n# Plot a few examples\nfig, axs = plt.subplots(1, 4, figsize=(12, 4))\nfor i in range(4):\n    im, label = batch['image'][i], batch['label'][i]\n    axs[i].imshow(im.squeeze(), cmap='gray')\n    axs[i].set_title(label)\n\nReusing dataset mnist (/root/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\nReusing dataset mnist (/root/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)"
  },
  {
    "objectID": "building_nns.html#defining-our-network",
    "href": "building_nns.html#defining-our-network",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Defining Our Network",
    "text": "Defining Our Network\nThe convolution operation is handled by the nn.Conv2d layer. Uncomment the next line to view some info about this:\n\n# ?nn.Conv2d\n\nLet’s use nn.Conv2D to convolve this image with some random kernels:\n\nconv_test = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, padding=0)\nimage = batch['image'][0].unsqueeze(0) # unsqueeze the first image to make this a 'batch of size 1'\nprint('Input shape: ', image.shape) # One channel greyscale image\nprint('Output shape: ', conv_test(image).shape) # 12 output channels (from 12 kernels)\n\nInput shape:  torch.Size([1, 1, 28, 28])\nOutput shape:  torch.Size([1, 12, 24, 24])\n\n\nNote the initial shape is slightly smaller - how does padding change this? Does the output shape make sense?\nThis layer has some trainable parameters: the kernels. Let’s check these out:\n\nconv_test.weight.shape # 12 filters, each 1x5x5\n\ntorch.Size([12, 1, 5, 5])\n\n\nTHINK: Does the number of parameters in this layer depend on the input image size?\nHere’s a network that uses these layers (along with nn.MaxPool2d for the downsampling/pooling). We could use nn.Sequential as in the previous example, but I’d like to show another common style here. We define all the layers we’ll need in init but only in the forward() method do we actually specify how that data should flow through the network.\n\n# Network definition\nclass MiniCNN(nn.Module):\n    def __init__(self):\n        super(MiniCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10) # 10 outputs (for 10 digits)\n        self.pool = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x"
  },
  {
    "objectID": "building_nns.html#the-training-loop",
    "href": "building_nns.html#the-training-loop",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "The Training Loop",
    "text": "The Training Loop\n\n\n\ntraining\n\n\nHere’s a training loop that should now be getting quite familiar. A few noteworthy things: - We can’t push all the images through in one go, so within each epoch (i.e. each full psas through the data) we do multiple batches. This is when ‘Gradient Descent’ becomes ‘Stochastic Gradient Descent’ or ‘Mini-batch GD’ depending on who you’re talking to. PyTorch does the batching for us via something called a DataLoader. - We’d like to train on the GPU, so we need to make sure both the model and the data are on the right device with .to(device) (device is defined earlier). - We’re using a loss function that is good for classification tasks: nn.CrossEntropyLoss(). Accuracy has ‘steps’ and so it makes differentiation tricky. By treating the outputs of the network as probabilities we can see how confident it is that something is in a specific class while keeping everything continuous and differentiable. Don’t worry too much about this :)\n\n# Set up model, loss and optimizer\nmodel = MiniCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nlosses = []\n\n# The training loop\nfor batch in tqdm(mnist_dl_train, unit='batch'):\n    data, target = batch['image'], batch['label']\n    data, target = data.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n    # Log the loss\n    losses.append(loss.item()) # .item makes a copy of just the value, detached from any gradient calculations.\n\nplt.plot(losses)\n\n\n\n\n\n\n\nASIDE: If you’re on a machine with a GPU, remove all to(device) in the above code - is it slower on CPU?"
  },
  {
    "objectID": "building_nns.html#evaluation---how-well-does-it-do",
    "href": "building_nns.html#evaluation---how-well-does-it-do",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Evaluation - how well does it do?",
    "text": "Evaluation - how well does it do?\n\n\n\ntesting\n\n\n\n## Testing\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n  # Iterate through test set minibatchs\n  for batch in tqdm(mnist_dl_test):\n    data, labels = batch.values() # TODO fix this in data eg\n    data, labels = data.to(device), labels.to(device) # Move the data to GPU for faster execution.\n    y = model(data) # Forward pass\n    predictions = torch.argmax(y, dim=1) # The model has ten outputs, one for each digit. Here we take the index with the highest output\n    correct += torch.sum((predictions == labels).float())\n    total += labels.shape[0]\n\nprint(f'Test accuracy: {correct/total * 100:.2f}%')\n\n\n\n\nTest accuracy: 88.37%\n\n\n\n# Exercise: See how good you can get! Tweak the architecture, the hyperparameters, the training time, the optimizer... go wild ;)\n# You may want to try doing multiple passes through the dataloader - aka multiple epochs.\n\nPhew! Welcome to deep learning :)\nWe’re learning just enough to move on with the course, but these are some big topics and we’re barely scratching the surface. If you’re interested in more of the theory or implementing some of these ideas from scratch, you might like to check out the content at https://deeplearning.neuromatch.io/ or one of the many deep learning courses on various MOOC platforms. If you’d like a more top-down approach to doing practical deep learning realy well, I can’t recommend the fastai course highly enough.\nThere is also a great from-scratch lesson from Andrej Karpathy going into all the nitty-gritty details of how gradients are calculated and so on: https://www.youtube.com/watch?v=VMj-3S1tku0"
  },
  {
    "objectID": "building_nns.html#loss-functions",
    "href": "building_nns.html#loss-functions",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nIt’s worth talking just a little more here about the concept of loss functions.\nIn our first task, we wanted to predict a continuous output, y. How do we compare the network outputs with the known values in such a way as to force them to be as close as possible? A popular choice is the mean squared error (MSE) or the root mean squared error (RMSE). We used the builtin PyTorch method but we can also implement this ourselves and verify that the result is the same on some dummy data:\n\ntargets = torch.tensor([0.2, 0.7, 0.1])\npredictions = torch.tensor([0.25, 0.6, 0.3])\nprint('Result using nn.MSELoss()(predictions, targets):', nn.MSELoss()(predictions, targets)) # Think: does order matter?\nprint('Result using ((targets - predictions)**2).mean():', ((targets - predictions)**2).mean())\n\nResult using nn.MSELoss()(predictions, targets): tensor(0.0175)\nResult using ((targets - predictions)**2).mean(): tensor(0.0175)\n\n\nMSE loss is sometimes called L2 loss. You could also try the Mean Absolute Error (MAE), aka L1 loss. The choice comes down to what we’d like to penalize. Because the error is squared in MSE, larger errors result in a much larger loss, while small errors incur only a small loss. This is preferable in many cases - we’d prefer to explicitly avoid massive errors!\n\nClassification\nHow would we make a loss function for classification? You could try something like accuracy: for a given batch, loss = number_wrong/number_of_examples, for example. But there’s a problem. Our updates require gradients, and accuracy will not be smooth and differentiable! So, we need to come up with a better loss function.\nOne candidate is MSE! We can encode our labels using one-hot encoding to get 10 values, 0 everywhere except for the column corresponding to the right class label:\n\n# Exercise: Can you use MSE loss for classification?\n\n# You can encode some class labels like so:\nlabels = torch.tensor([0, 1, 7, 3, 9])\none_hot_encoded = F.one_hot(labels, num_classes=10)\none_hot_encoded\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n\nSet up a network with 10 outputs, and use MSE to force it towards predicting low outputs for the wrong labels and high labels for the right ones. This should work, and was how the first digit recognition network was trained!\nIn practice, this is not the way we usually do it. Cross Entropy Loss takes the model outputs and does an operation called SoftMax on them (which scales the outputs to sum to 1) so that they can be interpreted as probabilities. Then it calculates a score based on the log of the probability associated with the correct label. Technically, CE is comparing two distributions (the predicted probability distribution from the model, and the known label distribution) and there’s lots of information theory and confusing terminology you are welcome to read up on. But the core of the matter is this: for labels very close to the true values, the loss will be small. For labels that are far off, the loss is high. And thanks to the maths, we can interpret the softmax of the model outputs as probabilities showing how ‘confident’ the model is.\nHere is an attempt at re-creating CrossEntropyLoss in code:\n\nlabels = torch.tensor([0, 2, 1, 0])\npredictions = torch.tensor([[1.95, -0.6, -0.3],\n                            [-0.25, -0.6, 2.3],\n                            [0.05, 3.6, -0.3],\n                            [3.7, -1.1, -1.2]])\nprint('Result using nn.CrossEntropyLoss()(labels, predictions):', nn.CrossEntropyLoss()(predictions, labels))\n\n# Softmax of raw preds (usually model outputs)\nprobs = nn.Softmax(dim=1)(predictions)\nprint('Softmax output:\\n', probs)\n\n# Get the probabilities corresponding to the correct labels\nrelevant_probs = probs[range(labels.shape[0]), labels]\nprint('Relevant Probabilities:', relevant_probs)\n\n# Calculate \nneg_log = -relevant_probs.log()\nprint('Negative Liklihoods:', neg_log)\nprint('Mean NL on softmax outputs:', neg_log.mean())\n\nResult using nn.CrossEntropyLoss()(labels, predictions): tensor(0.0892)\nSoftmax output:\n tensor([[0.8450, 0.0660, 0.0891],\n        [0.0689, 0.0486, 0.8825],\n        [0.0274, 0.9533, 0.0193],\n        [0.9846, 0.0081, 0.0073]])\nRelevant Probabilities: tensor([0.8450, 0.8825, 0.9533, 0.9846])\nNegative Liklihoods: tensor([0.1685, 0.1250, 0.0478, 0.0156])\nMean NL on softmax outputs: tensor(0.0892)\n\n\n\n# Exercise try a version of widenet on mnist as well - how good can you get it?"
  },
  {
    "objectID": "building_nns.html#pretrained-networks",
    "href": "building_nns.html#pretrained-networks",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Pretrained Networks",
    "text": "Pretrained Networks\nTraining a network from scratch can be time-consuming and require LOTs of data…\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm4.html",
    "href": "dm4.html",
    "title": "Lesson 15: Diffusion for Audio",
    "section": "",
    "text": "Class conditioned birdcalls\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "sequence_1.html",
    "href": "sequence_1.html",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "",
    "text": "Intro - lots of things come as sequences, and text is common!"
  },
  {
    "objectID": "sequence_1.html#embeddings-working-with-tokens",
    "href": "sequence_1.html#embeddings-working-with-tokens",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Embeddings: Working With Tokens",
    "text": "Embeddings: Working With Tokens\nConcept of embeddings\nReference https://deeplearning.neuromatch.io/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial1.html and maybe link Lyle’s ‘Embeddings Rule’ video?\n\nimport torch\nfrom torch import nn\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom transformers import PreTrainedTokenizerFast"
  },
  {
    "objectID": "sequence_1.html#modelling-sequences-language-models",
    "href": "sequence_1.html#modelling-sequences-language-models",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Modelling Sequences: Language Models",
    "text": "Modelling Sequences: Language Models\nExplain the objective"
  },
  {
    "objectID": "sequence_1.html#tokenizing-text",
    "href": "sequence_1.html#tokenizing-text",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Tokenizing Text",
    "text": "Tokenizing Text\nHow do we split up text into tokens? We often talk about ‘words’ being the unit of text, but if we just go with a token for each word that we might encounter you’ll end up with a massive (1M) vocabulary filled with mostly obscure/misspelled words. But on the other hand letters would mean using far more tokens to represent the same sentence.\nOne solution.. explain wordpiece and co\nTokenizers: https://huggingface.co/docs/tokenizers/index\n\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n\n\nencoding = tokenizer.encode('What a nice flooble!')\nprint('Encoding:', encoding)\n\nids = encoding.ids\nprint('ids:', ids)\n\nEncoding: Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\nids: [101, 2054, 1037, 3835, 13109, 9541, 3468, 999, 102]\n\n\n\nfor t in ids:\n    print(f'{t}:{tokenizer.decode([t])}')\n\n101:\n2054:what\n1037:a\n3835:nice\n13109:fl\n9541:##oo\n3468:##ble\n999:!\n102:\n\n\nWe have special tokens for start (101), end (102), symbols like ‘!’ (999) and separate tokens for a string like ‘oo’ or ‘ble’ if they don’t occur at the start of a word. Common words get a token, uncommon ones like flooble are broken down into components. THe full vocabulary size of this tokenizer is about 30,000 tokens:\n\nlen(tokenizer.get_vocab().items()) # The vocab size of this tokenizer\n\n30522"
  },
  {
    "objectID": "sequence_1.html#creating-our-own-tokenizer",
    "href": "sequence_1.html#creating-our-own-tokenizer",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Creating Our Own Tokenizer",
    "text": "Creating Our Own Tokenizer\nExplain\n\ndataset = load_dataset('tglcourse/abc_tunes', split='train').shuffle()\ndataset[0]\n\nUsing custom data configuration tglcourse--abc_tunes-5a89386c12e016f6\nReusing dataset parquet (/root/.cache/huggingface/datasets/tglcourse___parquet/tglcourse--abc_tunes-5a89386c12e016f6/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n\n\n{'Title': \"Pat Hogan's One\",\n 'Time Signature': '2/4',\n 'L': '1/8',\n 'Key': 'Edor',\n 'Tune': 'D>D FA|dc BA|BE EF|GA/G/ FE|D>D FA|dcBA|Be Bc|d2 d2:||:eB eB|eB B>c|dA dA|dA A2|eB eB|eB B>c|dB AF|E2 E2:|'}\n\n\n\nvocab_size = 100 # Explore different vocab sizes\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer =BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\"],\n                    vocab_size=vocab_size) \ntokenizer.train_from_iterator(dataset['Tune'], trainer)\n\n\n\n\n\n\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,\n                                    pad_token=\"[PAD]\",\n                                    unk_token=\"[UNK]\")\n\n\n# TODOSee the docs for pre-tokenizer options: https://huggingface.co/docs/tokenizers/quicktour\n\n\n# ??tokenizer.train\n\nPrinting out the decoded string (using ‘#’ as a separator) we ca see how the tune has turned into discrete tokens, some of thich are individual notes but many of which are combined.\n\nids = tokenizer.encode(dataset[0]['Tune']) # .ids if using the Tokenizer version\nprint(ids)\ndecoded = '#'.join([tokenizer.decode([t]) for t in ids])\nprint(decoded)\n\n[40, 34, 40, 4, 42, 37, 95, 71, 70, 4, 38, 37, 95, 38, 41, 4, 41, 42, 95, 43, 37, 19, 43, 19, 4, 42, 41, 95, 40, 34, 40, 4, 42, 37, 95, 71, 70, 38, 37, 95, 38, 72, 4, 38, 70, 95, 71, 22, 4, 71, 22, 30, 95, 95, 30, 72, 38, 4, 72, 38, 95, 72, 38, 4, 38, 34, 70, 95, 71, 37, 4, 71, 37, 95, 71, 37, 4, 37, 22, 95, 72, 38, 4, 72, 38, 95, 72, 38, 4, 38, 34, 70, 95, 71, 38, 4, 37, 42, 95, 41, 22, 4, 41, 22, 30, 95]\nD#>#D# #F#A#|#d#c# #B#A#|#B#E# #E#F#|#G#A#/#G#/# #F#E#|#D#>#D# #F#A#|#d#c#B#A#|#B#e# #B#c#|#d#2# #d#2#:#|#|#:#e#B# #e#B#|#e#B# #B#>#c#|#d#A# #d#A#|#d#A# #A#2#|#e#B# #e#B#|#e#B# #B#>#c#|#d#B# #A#F#|#E#2# #E#2#:#|\n\n\n\nprint(tokenizer.get_vocab())\n\n{'q': 84, '}': 96, 'í': 107, 'e': 72, '@': 36, 't': 87, 'ó': 109, 'U': 57, 'i': 76, '6': 26, '(': 12, 'b': 69, 'º': 101, '~': 97, 'f': 73, 'v': 89, '\\t': 3, 'Q': 53, 'S': 55, '0': 20, '#': 7, '4': 24, 'Z': 62, 'ä': 104, 'T': 56, 'D': 40, '\\x08': 2, '/': 19, '9': 29, 'G': 43, 'Y': 61, ')': 13, 'A': 37, '.': 18, 'I': 45, 'J': 46, 'á': 103, 'è': 105, 'V': 58, \"'\": 11, '^': 66, 'o': 82, '¬': 100, 'm': 80, '=': 33, 'p': 83, '!': 5, '[PAD]': 1, 'd': 71, 'z': 93, 'Ú': 102, '5': 25, ':': 30, '\"': 6, '+': 15, 'M': 49, '$': 8, '<': 32, '&': 10, 'H': 44, 'N': 50, 'h': 75, 'é': 106, 'ñ': 108, 'F': 42, '*': 14, '\\\\': 64, 'l': 79, '[UNK]': 0, '2': 22, 'L': 48, 'E': 41, 'K': 47, 'a': 68, '\\xa0': 98, 'R': 54, '7': 27, 'k': 78, '3': 23, ',': 16, '[': 63, '%': 9, 'ú': 110, 'n': 81, 'w': 90, 's': 86, 'c': 70, 'P': 52, '_': 67, 'y': 92, 'X': 60, ']': 65, 'r': 85, ';': 31, 'W': 59, '-': 17, 'u': 88, '8': 28, 'x': 91, 'g': 74, '{': 94, '|': 95, 'ª': 99, '>': 34, ' ': 4, '?': 35, '1': 21, 'B': 38, 'C': 39, 'O': 51, 'j': 77}"
  },
  {
    "objectID": "sequence_1.html#an-embedding-layer",
    "href": "sequence_1.html#an-embedding-layer",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "An Embedding Layer",
    "text": "An Embedding Layer\n\nemb_dim = 32\nemb_layer = nn.Embedding(vocab_size, emb_dim)\nemb_layer\n\nEmbedding(100, 32)\n\n\n\nemb_layer(torch.tensor(ids)).shape # Passing our tokens through\n\ntorch.Size([106, 32])"
  },
  {
    "objectID": "sequence_1.html#a-simple-mlp",
    "href": "sequence_1.html#a-simple-mlp",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "A simple MLP",
    "text": "A simple MLP\nSimilar to Karpathy’s makemore demo (TODO link)\nHow do we work with sequences of different lengths? Padding + truncation seem non-ideal…\n\nbatch_size=32\nseq_len=64\nbatch_ids = torch.randint(vocab_size, (batch_size,seq_len))\nbatch_ids.shape\n\ntorch.Size([32, 64])\n\n\n\nemb_layer(batch_ids).shape\n\ntorch.Size([32, 64, 32])\n\n\n\n# A minimal model (output sizes shown\nmodel = nn.Sequential(\n    nn.Embedding(vocab_size, emb_dim), # (batch_size, seq_length, emb_dim)\n    nn.Flatten(), # (batch_size, seq_length*emb_dim)\n    nn.Linear(emb_dim*seq_len, 64), # (batch_size, 64)\n    nn.ReLU(), # (batch_size, 64)\n    nn.Linear(64, 2), # (batch_size, 2)\n    \n)\nmodel(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\nQ: What happens when word position changes? Q: Would this work on different length sequences? Q: think of more Qs"
  },
  {
    "objectID": "sequence_1.html#recurrent-neural-networks-and-lstms",
    "href": "sequence_1.html#recurrent-neural-networks-and-lstms",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Recurrent Neural Networks and LSTMs",
    "text": "Recurrent Neural Networks and LSTMs\nExplain the basic architecture\nhttps://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n\n\n\nan unrolled RNN\n\n\nMaybe demo?\nBrief hand-wave explanation of LSTMs and link ULMFIT and co for the curious\nGreat blog by colah https://colah.github.io/posts/2015-08-Understanding-LSTMs/ Karpathy on RNN effectiveness: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\n# Create the RNN\ninput_size = 10 # Number of features in the input (embedding dim)\nhidden_size = 20 # Number of features in the hidden state h\nnum_layers = 1 # Set to 2 for a 'stacked' RNN with 2 layers\nrnn = nn.RNN(input_size, hidden_size, num_layers) # The model\n\n# Run some dummy data through\n# Create the model with batch_first=True if you'd like the batch dimension to come first\nbatch_size = 8\ninput_length = 5\nx = torch.randn(5, batch_size, input_size)\nh0 = torch.randn(num_layers, batch_size, hidden_size)\noutput, hn = rnn(x, h0)\n\n# Check the output shapes\noutput.shape, hn.shape\n\n(torch.Size([5, 8, 20]), torch.Size([1, 8, 20]))\n\n\n\nclass MyRNNClassifier(nn.Module):\n    def __init__(self, input_size=10, hidden_size=20, num_layers=2):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, 2)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        net_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        averaged_output = net_output.mean(dim=1) # Take the mean of the outputs ('mean pooling')\n        result = self.mlp(averaged_output) # THrough the linear layer or MLP to get 2 outputs (assuming binary classification)\n        return result\n\n\nnet = MyRNNClassifier()\nnet(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\n\nsum([p.numel() for p in net.parameters()])\n\n2522\n\n\n\n[p.shape for p in net.parameters()]\n\n[torch.Size([100, 10]),\n torch.Size([20, 10]),\n torch.Size([20, 20]),\n torch.Size([20]),\n torch.Size([20]),\n torch.Size([20, 20]),\n torch.Size([20, 20]),\n torch.Size([20]),\n torch.Size([20]),\n torch.Size([2, 20]),\n torch.Size([2])]\n\n\n\n# TODO try on some data\n\nLSTMs\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\nsomething something memory\n\nclass MyLSTMClassifier(nn.Module):\n    def __init__(self, input_size=10, hidden_size=20, num_layers=2):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, 2)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        net_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        averaged_output = net_output.mean(dim=1) # Take the mean of the outputs ('mean pooling')\n        result = self.mlp(averaged_output) # THrough the linear layer or MLP to get 2 outputs (assuming binary classification)\n        return result\n\n\nnet = MyLSTMClassifier()\nnet(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\n\nsum([p.numel() for p in net.parameters()])\n\n6962"
  },
  {
    "objectID": "sequence_1.html#using-learned-representations",
    "href": "sequence_1.html#using-learned-representations",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Using Learned Representations",
    "text": "Using Learned Representations\nDo review classification or something using a learned embedding combined with an RNN? Or model tunes and then classify into type/key/mode?\nYeah tunes will be good. LM objective first, then re-training\n\nclass LM(nn.Module):\n    def __init__(self, input_size=32, hidden_size=128, num_layers=2, vocab_size=200):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        rnn_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        return self.mlp(rnn_output)\n\n\nlm = LM()\ntokeized_tune = torch.tensor([tokenizer.encode(dataset[0]['Tune'])])\nx = tokeized_tune[:,:-1] # All but the last token\ny = tokeized_tune[:,1:] # All but the first token (x shifted by 1)\nx.shape, lm(x).shape, y.shape\n\n(torch.Size([1, 105]), torch.Size([1, 105, 200]), torch.Size([1, 105]))\n\n\n\ntorch.tensor(tokenizer(dataset['Tune'][:10], padding=True)['input_ids']).shape\n\ntorch.Size([10, 339])\n\n\n\ndef lm_loss_function(model_pred, y):\n    b, n, vc = model_pred.shape\n    pred = model_pred.reshape(b*n, vc)\n    target = y.flatten()\n    return nn.functional.cross_entropy(pred, target)\n\nlm_loss_function(lm(x), y)\n\ntensor(5.2998, grad_fn=<NllLossBackward0>)\n\n\n\nfrom datasets import Dataset\n\n\ndataset = load_dataset('tglcourse/abc_tunes', split='train').shuffle()\nmax_length = 201\ntokenized_tunes = tokenizer(dataset['Tune'], padding=True, truncation=True, max_length=max_length)['input_ids']\nlm_dataset = Dataset.from_dict({\n    'x':torch.tensor([t[:-1] for t in tokenized_tunes]),\n    'y':torch.tensor([t[1:] for t in tokenized_tunes]),\n})\nlm_dataloader = torch.utils.data.DataLoader(lm_dataset.with_format(\"torch\"), batch_size=256)\n\nUsing custom data configuration tglcourse--abc_tunes-5a89386c12e016f6\nReusing dataset parquet (/root/.cache/huggingface/datasets/tglcourse___parquet/tglcourse--abc_tunes-5a89386c12e016f6/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n\n\n\nbatch = next(iter(lm_dataloader))\nbatch['x'].shape, batch['y'].shape\n\n(torch.Size([256, 200]), torch.Size([256, 200]))\n\n\n\nfrom tqdm.notebook import tqdm\n\n\nlm = LM().cuda()\noptimizer = torch.optim.Adam(lm.parameters(), lr=1e-4)\nlosses = []\nfor epoch in range(10):\n    for batch in tqdm(lm_dataloader):\n        x = batch['x'].cuda()\n        y = batch['y'].cuda()\n        model_preds = lm(x)\n        loss = lm_loss_function(lm(x), y)\n        loss.backward()\n        losses.append(loss.item())\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\nimport numpy as np\ndef generate(model, start_text, n, max_length=200, top_k=None):\n    model.eval()\n    tokenized_start = torch.tensor([tokenizer(start_text)['input_ids']]).cuda()\n    x = tokenized_start\n    for i in range(n):\n        model_pred = model(x)\n        last_word_logits = model_pred[0][0]\n        p = torch.nn.functional.softmax(last_word_logits, dim=0).data\n        \n        # Set p to zero for special tokens we don't want\n        p[:4] = 0\n        \n        # Sample\n        if top_k is None:\n            candidate_tokens = np.arange(len(last_word_logits))\n        else:\n            p, candidate_tokens = p.topk(top_k)\n            candidate_tokens = candidate_tokens.detach().cpu().numpy().squeeze()\n        p = p.detach().cpu().numpy().squeeze()\n        word_index = np.random.choice(candidate_tokens, p=p/p.sum())\n        x = torch.cat([x, torch.tensor([[word_index]]).cuda()], dim=-1)\n        \n    return tokenizer.decode(x.flatten())\ngenerate(lm, 'B|F', 20)\n\n'B | F b Z X, c : Y º B l [ B'\n\n\n\ngenerate(lm, 'B|F', 200,top_k=20)\n\n'B | F 2 e F 3 ( ( a / A c E A E \" \" a d a g c E D :   A c   B F   f G : / d   a c A : f D E ( a \" e 2 D 2 F 2 3 E \" c a f 2 2 A G \" d E / E G c 2 / D a | 2 B D f B B   g e c e d B e g   ( f ( D g 3 G e : c g g g ( : | 2 2 / : a \" F G c 3 f \" F / | f \" c E 3 F a g F | f e G 2 D f E d E f a g G ( 3 c g ( G / E F 3 : ( \" 2 3 2 ( e G G E c / A g : / B e a / /   d : A f c / : E 2 \" f : 3 G | / | d | / F \" F'\n\n\n\nsum([p.numel() for p in lm.parameters()])\n\n247240\n\n\n\n# TODO try on some data (LM first then new classification head)\n\n\n# TODO talk about efficiency of training vs sampling\n# TODO demo different sampling approaches\n\nPage stats: Total Hits:  Page visitors:"
  }
]